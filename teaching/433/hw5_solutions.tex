\documentclass{article}

\usepackage{color}

\begin{document}

\begin{flushleft}
Course No. Stat 433 \\
\today
\end{flushleft}

\begin{center}
{\Large{\bf  Homework 5 Solution}}
\end{center}

\textcolor[rgb]{0.98,0.00,0.00}{Comments from the grader:}
\begin{itemize}

    \item \textcolor[rgb]{0.98,0.00,0.00}{These are only partial solutions.  We selected
    questions which were problematic to most of the class.}
    \item \textcolor[rgb]{0.98,0.00,0.00}{The maximum grade for this homework assignment is 10.}
    \item \textcolor[rgb]{0.98,0.00,0.00}{Your solution should contain explanations and not only
    final answers. Points will be deducted if partial solutions
    are submitted.}
    \item \textcolor[rgb]{0.98,0.00,0.00}{Please save a copy of your work and submit the original.
    Write your name and email on top of the first page.}
    \item \textcolor[rgb]{0.98,0.00,0.00}{if you notice a typo in the solution file or have a problem with the homework
    grading please email: sivana@wharton.upenn.edu
}
\end{itemize}


\begin{flushleft}

\begin{eqnarray*}
\\
\end{eqnarray*}


\textbf{Page 131 Question 4.4}

Let $w_i=P(\textrm{the process visits state 2} | X_0=i)$. The
appropriate transition matrix is:

The appropriate transition matrix is:
\[ P = \left ( \begin{array}{cccc}
 1 & 0 & 0 & 0  \\
 0.1 & 0.2 & 0.5 & 0.2  \\
 0 & 0 & 1 & 0  \\
 0.2 & 0.2 & 0.3 & 0.3  \\
\end{array} \right) \]


The first step analysis yields the following equations:
\begin{eqnarray*}
w_0&=&0\\
w_1&=&0.1w_0 + 0.2w_1+0.5w_2+0.2w_3 \\
w_2&=&1\\
w_3&=&0.2 w_0 +0.2w_1+0.3w_2+0.3w_3 \\
\end{eqnarray*}

Solving these equations yields that $w_1=41/52$. Hence,
$P(\textrm{the process never visits state 2} | X_0=2)=11/52$.

\begin{eqnarray*}
\\
\end{eqnarray*}

\textbf{Page 131 Question 4.6}\\
Let $v_i=E(T|X_0=i)$. Using the first step analysis we have

 \begin{eqnarray*}
v_0&=&1+qv_0+pv_1\\
v_1&=&1+qv_0+pv_2\\
v_2&=&1+qv_0+pv_3\\
v_3&=&1+qv_0+pv_4\\
v_4&=&0
 \end{eqnarray*}

Solving this system of equations leads to
\begin{eqnarray*}
v_0&=&\frac{1+p+p^2+p^3}{1-q-pq-p^2q-p^3q}
\end{eqnarray*}


\begin{eqnarray*}
\\
\end{eqnarray*}

\textbf{Page 132 Question 4.10}\\
Let $T=min\{n \geq0;X_n=1\}$ and $v_i=P(T<\infty|X_0=i)$.

One of the appropriate transition matrix is:
\[ P = \left ( \begin{array}{cccccc}
 \frac{1}{32} & \frac{5}{32}  & \frac{10}{32}  & \frac{10}{32} & \frac{5}{32} & \frac{1}{32}   \\
0 & \frac{1}{16}  & \frac{1}{4}  & \frac{3}{8} & \frac{1}{4} &\frac{1}{16} \\
0 & 0  & \frac{1}{8}  & \frac{3}{8} & \frac{3}{8} &\frac{1}{8} \\
0 & 0  & 0  & \frac{1}{4} & \frac{1}{2} &\frac{1}{4} \\
0 & 0  & 0  & 0 & \frac{1}{2} &\frac{1}{2} \\
0 & 0  & 0  & 0 & 0 & 1\\
\end{array} \right) \]

The first step analysis of the system $v=Pv$ where $u$ is the
vector $v=[v0,\ldots,v5]^{'}$ leads to $v_5= \frac{157}{217}$.


\begin{eqnarray*}
\\
\end{eqnarray*}

\textbf{Page 151 Question 5.5}\\
\begin{enumerate}
    \item Let $X_n$ be a Markov Chain starting at value $X_0=k$. Then
    in order to prove that it is a martingale we need to show
that: (i) $E(|X_n|)< \infty$; (ii) $E(X_{n+1}|
X_0,\ldots,X_n)=X_n$.

(i) We know that $E(|X_n|)=E(X_n)$ since $X_n$ can only have
non-negative values. After some thinking we can arrive at the fact
that $E(X_n)=k$.

(ii)If $X_n >0$ then
\begin{eqnarray*} E(X_{n+1}| X_0,\ldots,X_n)&=& X_n+ 0.5-0.5\\
&=& X_n
\end{eqnarray*}

If $X_n=0$ then we know that $X_n+1=0$ since zero is the
absorption state.
    \item $P(max X_n >N) \leq \frac{E(X_0)}{N}$
\end{enumerate}



\end{flushleft}

\begin{itemize}
\item (Web problem) Actually, I used a different proof this year--so
this problem was harder than I meant it to be.  Further it was even
more confusing that it should have been!

\begin{enumerate}
\item  {\bf Prove that $Y_t$ is a martingale.}

$E(Y_t|{\cal F}_{t-1}) = 1/3 (2 Y_{t-1}) + 2/3 (Y_{t-1}/2) =
Y_{t-1}$.  Since $X_t < X_0 + t$, $Y_t < 2^{X_0 + t}$.  Hence $E(|Y_t|) 
= E(Y_t) \le 2^{X_0 + t} < \infty$.

\item  {\bf Compute $E(Y_t)$ given $Y_0$ = $2^i$.}

In more traditional language, this asks $E(Y_t|Y_0=2^i)$ which for any
martingale will be $2^i$.

\item  {\bf Estimate the probability that $Y_t$ has not been
 absorbed by time t.}

This is the confusing one.  It should have mentioned that $t$ should
be large!  It should hvae said, ``compute an upper bound.''  So here
is the answer to the unasked question.

If we ever saw $n$ ``heads'' in a row, we definitely will be absorbed
reguardless of where we started from.  The chance of this occuring is
$1 - (2/3)^n$.  In order for us not to have been absorbed by time $t$,
we have to have made it through $t/n$ blocks without any of them being
all heads.  The chance of this is $(1 - (2/3)^n)^{t/n}$.  So an upper
bound on the chance of being absorbed is $1 - (1 - (2/3)^n)^{t/n}$. 

\item  {\bf How large does $t$ have to be so that we are sure that the
             probability of not being absorbed is less than $2^{-n}$?}

Picking $t$ equal to $n(3/2)^n$ will make  $(1 - (2/3)^n)^{t/n}$ less
than $1/e$. So making $t$ bigger than $n^2(3/2)^n$ will achieve the
desired accuracy.

\item  {\bf Estimate $P(Y_t = 1|Y_0 = 2^i)$}

From the equation for $E(Y_t = 1|Y_0 = 2^i)$ and the fact that there
is very little chance that $Y_t$ isn't either $1$ or $2^n$ we get the
same formula for gambler ruin as is in the book.  THis clearly answer
the next question also.

        \item {\bf  Does this match the formula given in the book?}

Yup! exactly.
\end{enumerate}

\end{itemize}
\end{document}
