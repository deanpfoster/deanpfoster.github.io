\documentclass[14pt]{extarticle}

\usepackage{hyperref}

\begin{document}
\title{Class: standard errors}

(\href{class_standard_errors.pdf}{pdf version})


\section{Status so far}

The model
\begin{displaymath}
Y_i = \alpha + \beta x_i + \epsilon_i 
\end{displaymath}
where $\epsilon_i$ are iid and
\begin{displaymath}
\epsilon_i \sim N(0,\sigma^2).
\end{displaymath}

\begin{itemize}
\item First we discussed fitting ($\alpha + \beta x_i$)
\item Then we discussed the residuals
\item Now we want to discuss how to estimate the error in $\hat\beta$
\end{itemize}

\section{Why we care}

If the normal linear model holds, we know that $\hat\beta$ has close
to a normal distribution.  In particular,
$\frac{\hat\beta}{SE(\hat\beta)}$ is a t-distribution.  We often want
to know how accurately we know $\beta$ purely for its own sake.  For
example, if our model is $Y = $ sales, and $X = $ advertisments, then
$\beta = $sales/ad.  So if we know that each sale generates \$10
profit, and each add costs \$1 (think web based advertisements) then
we need $\beta > .1$ before we make more money in sales than we spent
in advertisements.  

We can also use $\hat\beta$ to make predictions:
\begin{displaymath}
\hat{Y} = \hat\alpha + \hat\beta x_i
\end{displaymath}
 So before we can now how accurate a forecast is, we need to know how
accurate $\hat\beta$ is.

Either of these require knowing that $\hat\beta$ is a good estimate of
$\beta$ and exactly how good an estimate of it it actually is.  So we
need the standard error for $\hat\beta$.

\section{Standard errors rely on the linear model assumption}

The standard errors generated by R/JMP/statistics all require that the
standar linear model holds.  I.e. the residuals are IID normal.  If
they aren't:
\begin{itemize}
\item predictions are wrong
\item CI are wrong
\item hypothesis tests are wrong
\item accuracy of slopes are unknown
\end{itemize}
We are left with just guessing.

In previous classes, all we did is notice problems.  Now we want to
fix them (if possible).

\section{Hetroskadasticity}

One problem that we can fix is that of hetroskadasticity.  Suppose
that we are regression salary (Y) on runs (X).  Then we might expect
that 
\begin{displaymath}
Y = \alpha + \beta x
\end{displaymath}
will display hetroskadastic errors.  In particular, we might expect
that the errors grow with $x$.  

\paragraph{log-log model}  If we use logs, we can consider the model
\begin{displaymath}
\log(Y) = \alpha + \beta \log(x) + \epsilon
\end{displaymath}
In this model, we now expect the errors to be homoskadastic.  
\begin{eqnarray*}
\log(Y) & = & \alpha + \beta \log(x) + \epsilon\\
e^{\log(Y)} & = & e^{\alpha + \beta \log(x) + \epsilon}\\
Y & = & e^{\alpha} e^{\beta \log(x)} e^{\epsilon}\\
Y & = & e^{\alpha} (e^{\log(x)})^\beta  e^{\epsilon}\\
Y & = & k x^\beta  e^{\epsilon}\\
\end{eqnarray*}
where we inserted a $k$ for $e^{\alpha}$ so the equation looked
prettier. 

The problem with this analysis is that we can no longer address the
question, ``How much is a hit worth?''  Instead, we can say, ``How
many log(dollars) is a log(hit) worth?''  If this sounds resonable to
you, then you are definitely an economists:  A slope between
log(Y) and log(x) is called the elasticity.

\paragraph{Intrinsically linear models}  Cobb / Douglass production
functions are a cool example of a linear model that doesn't look like
a linear model:  log(output) = log(labor) + log(capital) etc...

\paragraph{Weighted least squares:} Alternatively, we can do weighted
least squares.  If we believe that the errors scale with $x$, then we
could write our model precisely as:
\begin{displaymath}
Y = \alpha + \beta x + x \epsilon
\end{displaymath}
So when $x$ is large, the errors are large and when $x$ is small the
errors are small.  This would show up nicely in a plot of $\epsilon^2$
vs $x$.  

We can modify our equation by dividing both sides by $x$:
\begin{displaymath}
Y/x = \alpha/x + \beta +  \epsilon
\end{displaymath}
Now if we do a regression of $Y/x$ on $1/x$ we have a homoskadastic
regression.  This methodology is called weighted least squares.  The
weights in this case are $1/x$.

There are two ways of doing a weighted least squares. 
\begin{itemize}
\item  First, you can ask R to simply use a column of $1/x^2$ as
weights. Sample hetroskadastic data with evil LS line:
<<fig=TRUE>>=
x <- 1:20
y <- 10 + x *(3 +rnorm(20))
plot(x,y)
abline(lm(y ~ x)$coef)
@ 

Now with the weighted line:
<<fig=TRUE>>=
plot(x,y)
abline(lm(y ~ x,weights = 1/x^2)$coef)
@ 

This is nice since the equation it generates will still use $\alpha$
and $\beta$ as we have been using them in the equations already.  
\item Second, you can create the two new variables $Y/x$ and $1/x$ and
do the regression yourself.  This allows you control and will work in
any regression package (i.e. excel).  But you then have to
interpret the slope and intercept carefully.  So it looks like:
<<fig=TRUE>>=
newx = 1/x
newy = y/x
plot(newx,newy)
abline(lm(newy ~ newx)$coef)
@
 
Compare the coeffients: 
<<>>=
summary(lm(y ~ x, weights=1/x^2))
summary(lm(newy ~ newx))
@ 

To check that $1/x$ is the wrong weights:
<<>>=
summary(lm(y ~ x,weights=1/x))
@ 



\end{itemize}


\end{document}
