\documentclass[10pt]{extarticle}
\usepackage{hyperref}
\begin{document}

\title{Practice in R}
\maketitle
(\href{practice.pdf}{pdf version})


\section{Week 2: Doglegs}

\begin{itemize}
\item Let's start by drawing a dog leg:
<<bentstick,fig=TRUE>>=
  x <- seq(1:100)
  large.x <- (x > 50)
  deviation <- (x - 50) * large.x
  y = x + 5 * deviation
  plot(x,y,type="l")
@ 
Now grab some data, and put a dog leg in the middle of it.  Basically
define a new large.x and a new deviation to match your new data.  I'll
do a simulation since that is what I had in the notes.  In that case,
we need to create a $y$ variable:
<<regression>>=
  y = x + 5 * deviation + 30 *rnorm(100)
@ 
The rnorm command generates random normals.  
\item Fitting is via multiple regression
<<fig=TRUE>>=
  plot(x,y)
  summary(lm(y ~ x + deviation))
  fit <- lm(y ~ x + deviation)$fit
  lines(x,fit)
@ 
\item More than one dogleg: Let's add a second dog leg.  Call it
bend2:
<<>>=
bend2 <- (x - 75) * ( x > 75)
@ 
We can now fit on both bends:
<<fig=TRUE>>=
  plot(x,y)
  fitB <- lm(y ~ x + deviation + bend2)$fit
  lines(x,fitB)
@ 
Run the command
<<eval=FALSE>>=
  summary(lm(y ~ x + deviation + bend2))
@ 
and confirm that bend2 doesn't have a significant $t$-statistic.
\item TO make more smooth bends, square the deviation:
<<fig=TRUE>>=
  plot(x,y)
  deviation2 = deviation**2
  fit2 <- lm(y ~ x + deviation2)$fit
  lines(x,fit2)
@ 
\item We can put them together with
<<fig=TRUE>>=
  plot(x,y)
  bend2sq <- bend2 ** 2
  fit2b <- lm(y ~ x + deviation2 + bend2sq)$fit
  lines(x,fit2b)
@ 
\end{itemize}

\section{FIrst week: Running R}

Let's look at a simple data set from your first regression class here
 at Penn. First get some data.  For me, I use the command line, just
 like your grandfather used:
\begin{verbatim}
 wget http://www-stat.wharton.upenn.edu/~waterman/fsw/datasets/txt/Cleaning.txt
\end{verbatim}
You of course have this new fangled deviced called a mouse--so use it!  Now start R.
First read in the file:
<<step1,eval=FALSE>>=
read.table("Cleaning.txt")
@ 
%
Oops, that generates too much output, and doesn't put it anywhere.
So let's assign all this mess to a data frame.
<<step2,eval=FALSE>>=
clean = read.table("Cleaning.txt")
@ 
%
Just look at what we have by typing ``clean'' again.  Oops--we have
the first row with the names of the variables in it.  So let's try again: 
<<step3,eval=TRUE>>=
clean = read.table("Cleaning.txt",header=TRUE)
@ 
%
NOTE: We could have avoided some pain here if we had read the
documentation ahead of time.  A quick reminder of how to use a
function is to use ``?read.table''.  But in general, the text book/
the web / google provides better documentation.

Checking with ``clean'' shows we only have numbers.  How happy can
you get?!? Now for the fun part, let's run a regression.
<<step4>>=
lm(clean$RoomsClean ~ clean$NumberOfCrews)
@ 
%
Kinda a different world view than JMP.  It just gives the minimal
amount of output possible.  So to see a bit more, try
<<step5>>=
summary(lm(clean$RoomsClean ~ clean$NumberOfCrews))
@ 
%
That should look very similar to other tables you have seen.  But what
of pictures?  Well, let's do a plot:
<<step5,fig=TRUE>>=
plot(lm(clean$RoomsClean ~ clean$NumberOfCrews))
@ 
\end{document}
