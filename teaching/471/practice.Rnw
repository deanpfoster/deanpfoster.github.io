\documentclass[10pt]{extarticle}
\usepackage{hyperref}
\begin{document}

\title{Practice in R}
\maketitle
(\href{practice.pdf}{pdf version})


\section{Sivan's practice}

Her practice file should be (\href{homework_1_practice.R}{here}), or
check the web for a more useful pointer.

\section{Hetroskadasticity}

\begin{itemize}
\item  Let's make some hetroskadastic data:
<<fig=TRUE>>=
x <- 1:20
y <- 10 + x *(3 +rnorm(20))
plot(x,y)
abline(lm(y ~ x)$coef)
@ 

\item Now with the weighted line:
<<fig=TRUE>>=
plot(x,y)
abline(lm(y ~ x,weights = 1/x^2)$coef)
@ 
This is nice since the equation it generates will still use $\alpha$
and $\beta$ as we have been using them in the equations already.  
\item Second, you can create the two new variables $Y/x$ and $1/x$ and
do the regression yourself.  This allows you control and will work in
any regression package (i.e. excel).  But you then have to
interpret the slope and intercept carefully.  So it looks like:
<<fig=TRUE>>=
newx = 1/x
newy = y/x
plot(newx,newy)
abline(lm(newy ~ newx)$coef)
@ 

\item Compare the coeffients: 
<<>>=
summary(lm(y ~ x, weights=1/x^2))
summary(lm(newy ~ newx))
@ 
\item To check that $1/x$ is the wrong weights:
<<>>=
summary(lm(y ~ x,weights=1/x))
@ 
\item Log-log on log-log scale
<<fig=TRUE>>=
logY = log(y)
logX = log(x)
loglog <- lm(logY ~ logX)
plot(logX, logY)
abline(loglog$coef)
@ 
Now for the fun part--let's get the back to the original scale. 
<<fig=TRUE>>=
plot(x,y)
lines(x,exp(predict(loglog)))
@ 

\end{itemize}

\section{Residuals}

Let's look at doing some of these things in R.  Start by grabbing some
data (I'll simulate here since that makes the document self
contained): 
<<data>>=

x <- (1:100)/10   # data between 0 and 10
y <- 3 + 2 * x + 8.3 * rnorm(100)
model <- lm(y ~ x)

@ 

If you have an actual data file you have read in, your names will be
more interesting than $y$ and $x$.  Let's look at the basic fit.  THis
means we want to plot $y$ vs $x$ but we also want the line added to
the plot.
<<basicPlot,fig=TRUE>>=

      plot(x,y)
      abline(model$coef)

@ 
Now to look at the residuals.  We can summarize them with:
<<residSummary>>=
      summary(model$residuals)
@ 
but that isn't very informative.  Let's do a qqplot:
<<qqplot,fig=TRUE>>=
      qqnorm(model$residuals)
@ 
Ok, that is looking better.  How about some other diagnostics.  THe
easiest is just residuals vs $x$:
<<curvature,fig=TRUE>>=
      plot(x,model$residuals)
@ 
If we want to check curvature via a polynomial, we could do the
following: 
<<poly>>=
      summary(lm(model$residuals ~ x + I(x**2) + I(x**3)))
@ 
The $I()$ are there for a reason.  Don't ask.  (Why do we divide by
$n-1$?  Don't ask!  Yes both reasons are good--but we don't really
want to know.)  Putting this together in a nice picture we get:
<<poly,fig=TRUE>>=
    ploy.fit <- lm(model$residuals ~ x + I(x**2) + I(x**3))$fitted.values
   plot(x, model$residuals)
   lines(x, ploy.fit)
@ 

Much of this can be generated by the simple command of plotting your
model.  It will generate several plots--the last one is the following:
<<everthing,fig=TRUE>>=
plot(model)
@ 




\section{Week 2: Doglegs}

\begin{itemize}
\item Let's start by drawing a dog leg:
<<bentstick,fig=TRUE>>=
  x <- seq(1:100)
  large.x <- (x > 50)
  deviation <- (x - 50) * large.x
  y = x + 5 * deviation
  plot(x,y,type="l")
@ 
Now grab some data, and put a dog leg in the middle of it.  Basically
define a new large.x and a new deviation to match your new data.  I'll
do a simulation since that is what I had in the notes.  In that case,
we need to create a $y$ variable:
<<regression>>=
  y = x + 5 * deviation + 30 *rnorm(100)
@ 
The rnorm command generates random normals.  
\item Fitting is via multiple regression
<<fig=TRUE>>=
  plot(x,y)
  summary(lm(y ~ x + deviation))
  fit <- lm(y ~ x + deviation)$fit
  lines(x,fit)
@ 
\item More than one dogleg: Let's add a second dog leg.  Call it
bend2:
<<>>=
bend2 <- (x - 75) * ( x > 75)
@ 
We can now fit on both bends:
<<fig=TRUE>>=
  plot(x,y)
  fitB <- lm(y ~ x + deviation + bend2)$fit
  lines(x,fitB)
@ 
Run the command
<<eval=FALSE>>=
  summary(lm(y ~ x + deviation + bend2))
@ 
and confirm that bend2 doesn't have a significant $t$-statistic.
\item TO make more smooth bends, square the deviation:
<<fig=TRUE>>=
  plot(x,y)
  deviation2 = deviation**2
  fit2 <- lm(y ~ x + deviation2)$fit
  lines(x,fit2)
@ 
\item We can put them together with
<<fig=TRUE>>=
  plot(x,y)
  bend2sq <- bend2 ** 2
  fit2b <- lm(y ~ x + deviation2 + bend2sq)$fit
  lines(x,fit2b)
@ 
\end{itemize}

\section{FIrst week: Running R}

Let's look at a simple data set from your first regression class here
 at Penn. First get some data.  For me, I use the command line, just
 like your grandfather used:
\begin{verbatim}
 wget http://www-stat.wharton.upenn.edu/~waterman/fsw/datasets/txt/Cleaning.txt
\end{verbatim}
You of course have this new fangled deviced called a mouse--so use it!  Now start R.
First read in the file:
<<step1,eval=FALSE>>=
read.table("Cleaning.txt")
@ 
%
Oops, that generates too much output, and doesn't put it anywhere.
So let's assign all this mess to a data frame.
<<step2,eval=FALSE>>=
clean = read.table("Cleaning.txt")
@ 
%
Just look at what we have by typing ``clean'' again.  Oops--we have
the first row with the names of the variables in it.  So let's try again: 
<<step3,eval=TRUE>>=
clean = read.table("Cleaning.txt",header=TRUE)
@ 
%
NOTE: We could have avoided some pain here if we had read the
documentation ahead of time.  A quick reminder of how to use a
function is to use ``?read.table''.  But in general, the text book/
the web / google provides better documentation.

Checking with ``clean'' shows we only have numbers.  How happy can
you get?!? Now for the fun part, let's run a regression.
<<step4>>=
lm(clean$RoomsClean ~ clean$NumberOfCrews)
@ 
%
Kinda a different world view than JMP.  It just gives the minimal
amount of output possible.  So to see a bit more, try
<<step5>>=
summary(lm(clean$RoomsClean ~ clean$NumberOfCrews))
@ 
%
That should look very similar to other tables you have seen.  But what
of pictures?  Well, let's do a plot:
<<step5,fig=TRUE>>=
plot(lm(clean$RoomsClean ~ clean$NumberOfCrews))
@ 
\end{document}
