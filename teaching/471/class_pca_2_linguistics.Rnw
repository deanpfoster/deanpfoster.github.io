\documentclass[14pt]{extarticle}
\renewcommand{\baselinestretch}{1.2}
\usepackage{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% defining some commands that I like:
%
%                       \dpf   \note    \DOC    \TESTING
%
\usepackage[usenames]{color}\definecolor{mypurple}{rgb}{.3,0,.5}
\newcommand{\dpf}[1]{\noindent{\textcolor{mypurple}{\{{\bf dpf:} \em #1\}}}}
\newcommand{\note}[1]{\noindent{\textcolor{red}{\{{\bf NOTE:} \em #1\}}}}
\SweaveOpts{prefix.string=.figures/linguistics}

\begin{document}


\title{Class: PCAs (continued) for Lingustics}
\maketitle
(\href{class_pca_2_lingustics.pdf}{pdf version})

\section*{Admistrivia}

\begin{itemize}
\item article about \href{http://www.wired.com/magazine/2010/02/ff_google_algorithm/all/1}{google}
\end{itemize}


\begin{itemize}
\item distance to line in any direction: first principle component
\item How do you describe a pancake? Distance to a plane, first 2
principle components
\item Higher dimistions?
\end{itemize}

\section*{Story: Regression to the mean}
\begin{itemize}
\item Stephen Stiegler (you may of heard of his father)
\item ``The history of statistics in 1933.''
\item Reviews book by Secrist
\begin{itemize}
\item Most winners lose in second round
\item Good companies go bad
\item Everything is falling apart
\item We should do something about it!
\item Moral decay!
\end{itemize}
\item Steve argues that this is a water shed year for statistics since
this book was eveserated in two journals that are alive and kicking
today.
\item Idea called ``regression to the mean.''  Known to Galton
\end{itemize}
\section*{PCA's via regression}
\subsection*{Wrong way regressions}

Idea: slope = correlation * ratio of standard deviations

So to turn a regression around, you keep the correlation the same,
just change the ratio.  

Very counter-intuitive

\subsection*{Using it for PCA's}
\begin{itemize}
\item Suppose we want to find components $C$ for a dataset $X$
\item Idea: if we had the first PCA, how would we use it?  We could
estimate the $\beta$'s in the following useful regression:
\begin{displaymath}
X_{ij} = \beta_i C_j + \sigma \epsilon_{ij}
\end{displaymath}
NOTE: the $\epsilon$'s are IID, ``normal zero-one's''.
\item But we don't know the first PCA.  No problem, suppose we knew
the $\beta$'s.  Then we could use the regression equation to say that: 
\begin{displaymath}
C_{j} = \beta_i X_{ij} + \hbox{noise}
\end{displaymath}
There are many such equations.  Each is equaly accurate.  So use:
\begin{displaymath}
C_{j} = \frac{\sum_{i=1}^k \beta_i X_{ij}}{k}
\end{displaymath}
where $k$ is the number of $X$'s.
\item Both easy!  So use ``fixed point idea.''  Just iterate.
\end{itemize}

\subsection*{What do we get?}

\begin{itemize}
\item We find a $C$ which is a linear combination of the $X$'s and it
is close to all of them.  This is exactly what we want.
\item We can now take ALL these residuals and repeat the process.
This gets a 2nd principle component
\item Continuing in this fashion we can get more and more.
\end{itemize}


\subsection*{Ok, what about words?}

Reduces the dimision greatly.  



\newpage
\subsection*{Methodolgy}

Look at the .R file for details on how we read in the federalist
papers.  It is just like before.  For those seeing the Rnw source--you
can look at the 10 commands below.

<<readingInData,echo=FALSE>>=
  
federal <- read.csv("federalist.csv",header=TRUE)
federal$words <- strsplit(as.character(federal$text)," ")
federal$num.words <- c(lapply(federal$words,length),recursive=TRUE)
federal$num.char <- c(lapply(federal$text,nchar),recursive=TRUE)
codes <- c(NA, "HAM", "mad")
federal$training <- codes[1 + (federal$author == "HAMILTON") + 2 * (federal$author == "MADISON")]
federal$training01 <- (federal$training == "HAM")

@ 



<<sampleCounts,echo=FALSE>>=

federal[["freq.<c>"]] <- c(lapply(federal$words,function(t){sum(t == "<c>")}),recursive=TRUE) / federal$num.words
federal[["freq.<p>"]] <- c(lapply(federal$words,function(t){sum(t == "<p>")}),recursive=TRUE) / federal$num.words
federal$freq.the <- c(lapply(federal$words,function(t){sum(t == "the")}),recursive=TRUE) / federal$num.words
federal$freq.of  <- c(lapply(federal$words,function(t){sum(t == "of")}),recursive=TRUE) / federal$num.words
federal$freq.to  <- c(lapply(federal$words,function(t){sum(t == "to")}),recursive=TRUE) / federal$num.words
federal$freq.and <- c(lapply(federal$words,function(t){sum(t == "and")}),recursive=TRUE) / federal$num.words
federal$freq.in  <- c(lapply(federal$words,function(t){sum(t == "in")}),recursive=TRUE) / federal$num.words
federal$freq.a   <- c(lapply(federal$words,function(t){sum(t == "a")}),recursive=TRUE) / federal$num.words
federal$freq.be  <- c(lapply(federal$words,function(t){sum(t == "be")}),recursive=TRUE) / federal$num.words
federal$freq.that<- c(lapply(federal$words,function(t){sum(t == "that")}),recursive=TRUE) / federal$num.words
federal$freq.is  <- c(lapply(federal$words,function(t){sum(t == "is")}),recursive=TRUE) / federal$num.words
federal$freq.which<- c(lapply(federal$words,function(t){sum(t == "which")}),recursive=TRUE) / federal$num.words
federal$freq.it  <- c(lapply(federal$words,function(t){sum(t == "it")}),recursive=TRUE) / federal$num.words
@ 

Last time we constructed words by hand.  It looked something like:

<<sampleConstruction>>=

federal$freq.by  <- c(lapply(federal$words,function(t){sum(t == "by")}),recursive=TRUE) / federal$num.words

@ 

This time we want to create many of them.  So we need to first get a
list of all the words we want.

<<makeCounts>>=

all.words = c(federal$words,recursive=TRUE)
counts <- as.data.frame(table(all.words))
counts <- counts[order(counts$Freq,decreasing=TRUE),]
names(counts) <- c("words","counts")

@ 

Now we have to loop through the first 80 most common and add them as
variables to our data.frame.

<<pcs,echo=FALSE>>=

  top.words <- counts[1:200,]

  for(w in as.character(top.words$words))
  {
    my.name <- paste("freq",w,sep=".")
    federal[[my.name]] <- c(lapply(federal$words,function(t){sum(t == w)}),recursive=TRUE) / federal$num.words
  }

few.variables <- c(11:85)
variables <- c(11:200)
pcs <- prcomp(federal[,variables])$rotation

@ 
\section*{The components themselves}

The first PC (loadings):

<<compoents,fig=TRUE>>=

federal$pc1 <- as.matrix(federal[,variables]) %*% pcs[,1]
colors <- c("red","white","black","gray","blue")[federal$author]
plot(federal$pc1, pch=16 , col=colors)

@ 

The 2nd PC (loadings):

<<compoent2,fig=TRUE>>=

federal$pc2 <- as.matrix(federal[,variables]) %*% pcs[,2]
federal$pc3 <- as.matrix(federal[,variables]) %*% pcs[,3]
colors <- c("red","white","black","gray","blue")[federal$author]
plot(federal$pc2, pch=16 , col=colors)

@ 

The weights thesemves (PC1):
<<loadings, fig=TRUE>>=

plot(pcs[,1])

@ 

The weights thesemves (PC2):
<<loadings, fig=TRUE>>=

plot(pcs[,2])

@ 

\section*{Predictions}

Finally, let's predict using the first principle component:

<<plotForecast,fig=TRUE>>=

colors <- c("red","white","black","gray","blue")[federal$author]
regr <- lm(federal$training01 ~ federal$pc1)
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 

Or using the first 3 principle components:

<<pc123,fig=TRUE>>=

regr <- lm(federal$training01 ~ federal$pc1 + federal$pc2 + federal$pc3)
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 

With 9 PC's:


<<pc9,fig=TRUE>>=

federal$pc4 <- as.matrix(federal[,variables]) %*% pcs[,4]
federal$pc5 <- as.matrix(federal[,variables]) %*% pcs[,5]
federal$pc6 <- as.matrix(federal[,variables]) %*% pcs[,6]
federal$pc7 <- as.matrix(federal[,variables]) %*% pcs[,7]
federal$pc8 <- as.matrix(federal[,variables]) %*% pcs[,8]
federal$pc9 <- as.matrix(federal[,variables]) %*% pcs[,9]
regr <- lm(federal$training01 ~ federal$pc1 + federal$pc2 + federal$pc3
           +federal$pc4 + federal$pc5 + federal$pc6 +federal$pc7 + federal$pc8 + federal$pc9 )
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 


\section*{Why bother?}

If we instead used all the variables, it would look something like:

<<fig=TRUE>>=

equation <- paste("training01 ~", paste(names(federal)[variables],collapse="+"))
regrFull <- lm(equation,data=federal)
predictions <- predict(regrFull,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 

So we have a disaster if we don't do PCA's!


\end{document}
