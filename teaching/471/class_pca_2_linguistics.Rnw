\documentclass[14pt]{extarticle}
\renewcommand{\baselinestretch}{1.2}
\usepackage{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% defining some commands that I like:
%
%                       \dpf   \note    \DOC    \TESTING
%
\usepackage[usenames]{color}\definecolor{mypurple}{rgb}{.3,0,.5}
\newcommand{\dpf}[1]{\noindent{\textcolor{mypurple}{\{{\bf dpf:} \em #1\}}}}
\newcommand{\note}[1]{\noindent{\textcolor{red}{\{{\bf NOTE:} \em #1\}}}}
\SweaveOpts{prefix.string=.figures/linguistics}

\begin{document}


\title{Class: PCAs (continued) for Lingustics}
\maketitle
(\href{class_pca_2_lingustics.pdf}{pdf version})

\section*{Admistrivia}

\begin{itemize}
\item Homework due today
\item article about \href{http://www.wired.com/magazine/2010/02/ff_google_algorithm/all/1}{google}
\end{itemize}


\begin{itemize}
\item distance to line in any direction: first principle component
\item How do you describe a pancake? Distance to a plane, first 2
principle components
\item Higher dimistions?
\end{itemize}

\subsection*{Ok, what about words?}

Reduces the dimision greatly.  



\newpage
\subsection*{Methodolgy}

Look at the .R file for details on how we read in the federalist
papers.  It is just like before.  For those seeing the Rnw source--you
can look at the 10 commands below.

<<readingInData,echo=FALSE>>=
  
federal <- read.csv("federalist.csv",header=TRUE)
federal$words <- strsplit(as.character(federal$text)," ")
federal$num.words <- c(lapply(federal$words,length),recursive=TRUE)
federal$num.char <- c(lapply(federal$text,nchar),recursive=TRUE)
codes <- c(NA, "HAM", "mad")
federal$training <- codes[1 + (federal$author == "HAMILTON") + 2 * (federal$author == "MADISON")]
federal$training01 <- (federal$training == "HAM")

@ 



<<sampleCounts,echo=FALSE>>=

federal[["freq.<c>"]] <- c(lapply(federal$words,function(t){sum(t == "<c>")}),recursive=TRUE) / federal$num.words
federal[["freq.<p>"]] <- c(lapply(federal$words,function(t){sum(t == "<p>")}),recursive=TRUE) / federal$num.words
federal$freq.the <- c(lapply(federal$words,function(t){sum(t == "the")}),recursive=TRUE) / federal$num.words
federal$freq.of  <- c(lapply(federal$words,function(t){sum(t == "of")}),recursive=TRUE) / federal$num.words
federal$freq.to  <- c(lapply(federal$words,function(t){sum(t == "to")}),recursive=TRUE) / federal$num.words
federal$freq.and <- c(lapply(federal$words,function(t){sum(t == "and")}),recursive=TRUE) / federal$num.words
federal$freq.in  <- c(lapply(federal$words,function(t){sum(t == "in")}),recursive=TRUE) / federal$num.words
federal$freq.a   <- c(lapply(federal$words,function(t){sum(t == "a")}),recursive=TRUE) / federal$num.words
federal$freq.be  <- c(lapply(federal$words,function(t){sum(t == "be")}),recursive=TRUE) / federal$num.words
federal$freq.that<- c(lapply(federal$words,function(t){sum(t == "that")}),recursive=TRUE) / federal$num.words
federal$freq.is  <- c(lapply(federal$words,function(t){sum(t == "is")}),recursive=TRUE) / federal$num.words
federal$freq.which<- c(lapply(federal$words,function(t){sum(t == "which")}),recursive=TRUE) / federal$num.words
federal$freq.it  <- c(lapply(federal$words,function(t){sum(t == "it")}),recursive=TRUE) / federal$num.words
@ 

Last time we constructed words by hand.  It looked something like:

<<sampleConstruction>>=

federal$freq.by  <- c(lapply(federal$words,function(t){sum(t == "by")}),recursive=TRUE) / federal$num.words

@ 

This time we want to create many of them.  So we need to first get a
list of all the words we want.

<<makeCounts>>=

all.words = c(federal$words,recursive=TRUE)
counts <- as.data.frame(table(all.words))
counts <- counts[order(counts$Freq,decreasing=TRUE),]
names(counts) <- c("words","counts")

@ 

Now we have to loop through the first 80 most common and add them as
variables to our data.frame.

<<pcs,echo=FALSE>>=

  top.words <- counts[1:80,]

  for(w in as.character(top.words$words))
  {
    my.name <- paste("freq",w,sep=".")
    federal[[my.name]] <- c(lapply(federal$words,function(t){sum(t == w)}),recursive=TRUE) / federal$num.words
  }

variables <- c(11:85)
pcs <- princomp(federal[,variables])$loadings

@ 

Finally, let's predict using the first principle component:

<<plotForecast,fig=TRUE>>=

colors <- c("red","white","black","gray","blue")[federal$author]
federal$pc1 <- as.matrix(federal[,variables]) %*% pcs[,1]
regr <- lm(federal$training01 ~ federal$pc1)
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 

Or using the first 3 principle components:

<<multipleRegression,fig=TRUE>>=

federal$pc2 <- as.matrix(federal[,variables]) %*% pcs[,2]
federal$pc3 <- as.matrix(federal[,variables]) %*% pcs[,3]
regr <- lm(federal$training01 ~ federal$pc1 + federal$pc2 + federal$pc3)
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 


If we instead used all the variables, it would look something like:

<<fig=TRUE>>=

equation <- paste("training01 ~", paste(names(federal)[variables],collapse="+"))
regrFull <- lm(equation,data=federal)
predictions <- predict(regrFull,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 

\end{document}
