\documentclass[14pt]{extarticle} % 14, 17, 20 all exist
\usepackage{hyperref}

\usepackage[usenames]{color}\definecolor{mypurple}{rgb}{.6,.0,.5}\newcommand{\note}[1]{\noindent{\textcolor{mypurple}{\{{\bf note:} \em #1\}}}}
\newcommand{\tech}[1]{\noindent{\textcolor{red}{\{{\bf technical note:} \em #1\}}}}
\usepackage{xypic}

\renewcommand{\baselinestretch}{1.4}
\begin{document}
\title{Still Paranoid?}
\maketitle

\section{Admistrivia}
\begin{itemize}
\item data analysis is now due 
\item Next week, we'll have in class presentations
\begin{itemize}
\item 12 minute presentations.
\item (Aim for 10, at 12, you will be cut off in mid-sentence. which
is very anoying for everyone!)
\item Handout is necessary
\item Powerpoint / Beamer is optional
\item The target audence is your fellow students.
\item Each student should provide two ``instant feedbacks'' for two
 different presentations.
\end{itemize}
\end{itemize}

\section{Review from last time}

Recall:
\begin{itemize}
\item Each period we see either return: $p_t$ or $q_t$.
\item Wealth is $p_1 * p_2 * p_3 * \cdots p_t$.
\item Our returns: $r_t = w p_t + (1-w) q_t$.
\item Figure of merit: $\overline{\log(p)} = (1/T) \sum_{t=1}^T
\log(p_t)$. 
\end{itemize}

Theorem:
There exists a method such that $\overline{\log(r)} \approx
\max\{\overline{\log(p)}, \overline{\log(q)} \}$.

Proof:

Buy and hold.  So our wealth is $(p_1 p_2 p_3\cdots p_t + q_1 q_2 q_3
\cdots q_t)/2$.  And we are done.  Huh?  Suppose someone tells us that
$p$ has kicked our butt.  We argue, no:
\begin{eqnarray*}
\overline{\log{r}} & = &   (1/T) \sum_{t=1}^T \log(r_t)\\
 & = &   (1/T)  \log(r_1 r_2 r_3 \cdots r_t)\\
 & = &   (1/T)  \log((p_1 p_2 p_3 \cdots p_t + q_1 q_2 q_3 \cdots q_t)/2)\\
 & \ge & (1/T)  \log((p_1 p_2 p_3 \cdots p_t)/2)= \overline{\log{p}} - \log(2)/T
\end{eqnarray*}

Note:
\begin{displaymath}
w = \frac{p_1 p_2 p_3\cdots p_t}{p_1 p_2 p_3\cdots p_t + q_1 q_2 q_3
\cdots q_t}
\end{displaymath}

\section{Something completely different: Predicting the future}

Three horse run in each race.  Each of two friends predicts the
probability of these horses winning.  Each horse has a jockey who wears
either red, green or blue jersey. So it looks something like: 

 \begin{tabular}{r|c|c|c|c|c|c|c|c|c|c}
      race number  & $0$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$  & $8$ &$\cdots$
      \\ \hline 
      p: red & $.2$ & $.4$ & $.1$ & $.6$ & $.8$ & $.4$ & $.3$ & $.3$  & $.3$
      &$\cdots$ \\ 
      p: green & $.6$ & $.4$ & $.8$ & $.2$ & $.1$ & $.3$ & $.3$ & $.4$  & $.3$
      &$\cdots$ \\ 
      p: blue & $.2$ & $.2$ & $.1$ & $.2$ & $.1$ & $.3$ & $.4$ & $.3$  & $.4$
      &$\cdots$ \\ \hline
      q: red & $.4$ & $.2$ & $.1$ & $.3$ & $.2$ & $.1$ & $.3$ & $.3$  & $.3$
      &$\cdots$ \\ 
      q: green & $.6$ & $.4$ & $.1$ & $.5$ & $.7$ & $.8$ & $.3$ & $.4$  & $.3$
      &$\cdots$ \\ 
      q: blue & $.0$ & $.4$ & $.8$ & $.2$ & $.1$ & $.1$ & $.4$ & $.3$  & $.4$
      &$\cdots$ \\ \hline \hline
      outcome & & & & & & & & & & \\ \hline\hline
      p & & & & & & & & & & \\ \hline
      q & & & & & & & & & & \\ \hline

    \end{tabular}

\begin{itemize}
\item Fill in some outcomes and probabilities
\item Log probabilies are one scoring rule
\item Sum log probability is entropy
\item average log probability is entropy/race
\item Low entropy is good
\end{itemize}

\section{Problem: Can we get as low an entropy as better forecaster?}

\begin{itemize}
\item Yes--just like in finance
\item ``buy and hold probabilities.''
\item Minor difficulty: Do we actually have probabilities?
\item If so, what are they?
\end{itemize}

\section{AKA: Bayesian probability}

 The same result comes out if we believe the following model:
\begin{itemize}
\item At time zero, nature tosses a coin
\item After that, she uses the ``predictions'' of winning player to
randomize the next winner.
\end{itemize}
Under this model, we can comput the probability of the next winner
being the red horse.



\end{document}
