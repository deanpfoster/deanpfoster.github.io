\documentclass[14pt]{extarticle}
\renewcommand{\baselinestretch}{1.2}
\usepackage{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% defining some commands that I like:
%
%                       \dpf   \note    \DOC    \TESTING
%
\usepackage[usenames]{color}\definecolor{mypurple}{rgb}{.3,0,.5}
\newcommand{\dpf}[1]{\noindent{\textcolor{mypurple}{\{{\bf dpf:} \em #1\}}}}
\newcommand{\note}[1]{\noindent{\textcolor{red}{\{{\bf NOTE:} \em #1\}}}}
\SweaveOpts{prefix.string=.figures/linguistics}

\begin{document}


\title{Class: PCAs for Lingustics}
\maketitle
(\href{class_pca_lingustics.pdf}{pdf version})

\section*{Admistrivia}

\begin{itemize}
\item Homework due wednesday
\end{itemize}

\section*{Story time: Lens model}
\begin{itemize}
\item Show people: SATs, GPAs, teacher evaluations, field.
\item Ask to predict 1st year grades in school, call these $\hat{y}$.
\item Now use linear regression to estimate these fits: call these $\hat{\hat{y}}$.
\item Which is smaller? $\sum (Y - \hat{y})^2$ or $\sum (Y - \hat{hat{y}})^2$?
\item Weird fact: linear model is better!
\item Humans are noisy and non-linear when they shouldn't be.
(I.e. This guy is going into english, who cares about the SAT-math?
This girl is going to be an engineer, who cares if the Essay is poorly
written?)
\item citation: Hirsh, Hommond, Hirsh, 1964, and Tucker 1964.
\end{itemize}


\section*{Todays Topic: PCA}

Using the $Y$ pollutes your forcasts with noise.  Can we do a fit
without using $Y$?  

\subsection*{Errors in variables}

\begin{itemize}
\item First, what makes a $Y$ variable?
\begin{itemize}
\item It is the one with noise.
\item $Y = \beta X + noise$, the $X$ is perfect!
\end{itemize}
\item What if the $X$ has noise?
\begin{itemize}
\item Maybe even the $X$ has more noise, so do the regression the
other way around.
\item Called Errors in variables.
\end{itemize}
\item Full model:
\begin{displaymath}
Y = \beta X + \hbox{noise}_Y
\end{displaymath}
But also,
\begin{displaymath}
\hat{X} = X + \hbox{noise}_X
\end{displaymath}
But we only observe $\hat{X}$.  What is the regression of $Y$ on
$\hat{X}$? 
\begin{eqnarray*}
Cov(Y,\hat{X}) &=& Cov(\beta X + \epsilon_Y, X + \epsilon_X)\\
 &=& \beta Cov(X,X) + Cov(\epsilon_Y,\epsilon_X)
\end{eqnarray*}
So if the errors are independent, then $ Cov(\epsilon_Y,\epsilon_X) =
0$.  So it looks like no change?  NO!
\item Regular least squares looks at COV/VAR.  It would be right if we
used var$(X)$, but we infact use var$(\hat{X})$.  So we need to
correct this.
\item $\hat{\beta} = \hat{\beta}_{ls} \frac{var(\hat{X})}{var(X)}$
\end{itemize}







\subsection*{Methodolgy}

Look at the .R file for details on how we read in the federalist
papers.  It is just like before.  For those seeing the Rnw source--you
can look at the 10 commands below.

<<readingInData,echo=FALSE>>=
  
federal <- read.csv("federalist.csv",header=TRUE)
federal$words <- strsplit(as.character(federal$text)," ")
federal$num.words <- c(lapply(federal$words,length),recursive=TRUE)
federal$num.char <- c(lapply(federal$text,nchar),recursive=TRUE)
codes <- c(NA, "HAM", "mad")
federal$training <- codes[1 + (federal$author == "HAMILTON") + 2 * (federal$author == "MADISON")]
federal$training01 <- (federal$training == "HAM")

@ 



<<sampleCounts,echo=FALSE>>=

federal[["freq.<c>"]] <- c(lapply(federal$words,function(t){sum(t == "<c>")}),recursive=TRUE) / federal$num.words
federal[["freq.<p>"]] <- c(lapply(federal$words,function(t){sum(t == "<p>")}),recursive=TRUE) / federal$num.words
federal$freq.the <- c(lapply(federal$words,function(t){sum(t == "the")}),recursive=TRUE) / federal$num.words
federal$freq.of  <- c(lapply(federal$words,function(t){sum(t == "of")}),recursive=TRUE) / federal$num.words
federal$freq.to  <- c(lapply(federal$words,function(t){sum(t == "to")}),recursive=TRUE) / federal$num.words
federal$freq.and <- c(lapply(federal$words,function(t){sum(t == "and")}),recursive=TRUE) / federal$num.words
federal$freq.in  <- c(lapply(federal$words,function(t){sum(t == "in")}),recursive=TRUE) / federal$num.words
federal$freq.a   <- c(lapply(federal$words,function(t){sum(t == "a")}),recursive=TRUE) / federal$num.words
federal$freq.be  <- c(lapply(federal$words,function(t){sum(t == "be")}),recursive=TRUE) / federal$num.words
federal$freq.that<- c(lapply(federal$words,function(t){sum(t == "that")}),recursive=TRUE) / federal$num.words
federal$freq.is  <- c(lapply(federal$words,function(t){sum(t == "is")}),recursive=TRUE) / federal$num.words
federal$freq.which<- c(lapply(federal$words,function(t){sum(t == "which")}),recursive=TRUE) / federal$num.words
federal$freq.it  <- c(lapply(federal$words,function(t){sum(t == "it")}),recursive=TRUE) / federal$num.words
@ 

Last time we constructed words by hand.  It looked something like:

<<sampleConstruction>>=

federal$freq.by  <- c(lapply(federal$words,function(t){sum(t == "by")}),recursive=TRUE) / federal$num.words

@ 

This time we want to create many of them.  So we need to first get a
list of all the words we want.

<<makeCounts>>=

all.words = c(federal$words,recursive=TRUE)
counts <- as.data.frame(table(all.words))
counts <- counts[order(counts$Freq,decreasing=TRUE),]
names(counts) <- c("words","counts")

@ 

Now we have to loop through the first 80 most common and add them as
variables to our data.frame.

<<pcs,echo=FALSE>>=

  top.words <- counts[1:80,]

  for(w in as.character(top.words$words))
  {
    my.name <- paste("freq",w,sep=".")
    federal[[my.name]] <- c(lapply(federal$words,function(t){sum(t == w)}),recursive=TRUE) / federal$num.words
  }

variables <- c(11:85)
pcs <- princomp(federal[,variables])$loadings

@ 

Finally, let's predict using the first principle component:

<<plotForecast,fig=TRUE>>=

colors <- c("red","white","black","gray","blue")[federal$author]
federal$pc1 <- as.matrix(federal[,variables]) %*% pcs[,1]
regr <- lm(federal$training01 ~ federal$pc1)
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 

Or using the first 3 principle components:

<<multipleRegression,fig=TRUE>>=

federal$pc2 <- as.matrix(federal[,variables]) %*% pcs[,2]
federal$pc3 <- as.matrix(federal[,variables]) %*% pcs[,3]
regr <- lm(federal$training01 ~ federal$pc1 + federal$pc2 + federal$pc3)
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 


If we instead used all the variables, it would look something like:

<<fig=TRUE>>=

equation <- paste("training01 ~", paste(names(federal)[variables],collapse="+"))
regrFull <- lm(equation,data=federal)
predictions <- predict(regrFull,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 

\end{document}
