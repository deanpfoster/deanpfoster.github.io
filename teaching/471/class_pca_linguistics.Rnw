\documentclass[14pt]{extarticle}
\renewcommand{\baselinestretch}{1.2}
\usepackage{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% defining some commands that I like:
%
%                       \dpf   \note    \DOC    \TESTING
%
\usepackage[usenames]{color}\definecolor{mypurple}{rgb}{.3,0,.5}
\newcommand{\dpf}[1]{\noindent{\textcolor{mypurple}{\{{\bf dpf:} \em #1\}}}}
\newcommand{\note}[1]{\noindent{\textcolor{red}{\{{\bf NOTE:} \em #1\}}}}
\SweaveOpts{prefix.string=.figures/linguistics}

\begin{document}


\title{Class: PCAs for Lingustics}
\maketitle
(\href{class_pca_lingustics.pdf}{pdf version})

\section*{Admistrivia}

\begin{itemize}
\item Homework due wednesday
\end{itemize}

\section*{Story time: Lens model}
\begin{itemize}
\item Show people: SATs, GPAs, teacher evaluations, field.
\item Ask to predict 1st year grades in school, call these $\hat{y}$.
\item Now use linear regression to estimate these fits: call these $\hat{\hat{y}}$.
\item Which is smaller? $\sum (Y - \hat{y})^2$ or $\sum (Y - \hat{hat{y}})^2$?
\item Weird fact: linear model is better!
\item Humans are noisy and non-linear when they shouldn't be.
(I.e. This guy is going into english, who cares about the SAT-math?
This girl is going to be an engineer, who cares if the Essay is poorly
written?)
\item citation: Hirsh, Hommond, Hirsh, 1964, and Tucker 1964.
\end{itemize}


\section*{Todays Topic: PCA}

Using the $Y$ pollutes your forcasts with noise.  Can we do a fit
without using $Y$?  

\subsection*{Errors in variables}

\begin{itemize}
\item First, what makes a $Y$ variable?
\begin{itemize}
\item It is the one with noise.
\item $Y = \beta X + noise$, the $X$ is perfect!
\end{itemize}
\item What if the $X$ has noise?
\begin{itemize}
\item Maybe even the $X$ has more noise, so do the regression the
other way around.
\item Called Errors in variables.
\end{itemize}
\item Full model:
\begin{displaymath}
Y = \beta X + \hbox{noise}_Y
\end{displaymath}
But also,
\begin{displaymath}
\hat{X} = X + \hbox{noise}_X
\end{displaymath}
But we only observe $\hat{X}$.  What is the regression of $Y$ on
$\hat{X}$? 
\begin{eqnarray*}
Cov(Y,\hat{X}) &=& Cov(\beta X + \epsilon_Y, X + \epsilon_X)\\
 &=& \beta Cov(X,X) + Cov(\epsilon_Y,\epsilon_X)
\end{eqnarray*}
So if the errors are independent, then $ Cov(\epsilon_Y,\epsilon_X) =
0$.  So it looks like no change?  NO!
\item Regular least squares looks at COV/VAR.  It would be right if we
used var$(X)$, but we infact use var$(\hat{X})$.  So we need to
correct this.
\item $\hat{\beta} = \hat{\beta}_{ls} \frac{var(\hat{X})}{var(X)}$
\end{itemize}

\subsection*{Science aside}

Relationship between blood presure and heart attacks see
(\href{blood_presure.pdf}{``Blood presure and long term Coranary heart
disease...'', EHJ 2000})
\begin{itemize}
\item $Y$ = chance of death
\item $X$ = blood presure
\item Look at the data. There is a weak relationship in a plot of
death rate with single cuff reading of blood presure.
\item But looking at an average of 3 blood presure readings and you
get a steeper relationship.
\item So DON'T TAKE YOUR BLOOD PRESURE MORE THAN ONCE!!!  (just kidding.)
\item What is the real relationship?
\item A 50 point increase in blood presure will double your chance of
death.
\begin{itemize}
\item 1/500 chance per year
\item For reference, driving a car 100k miles is the same risk
(2.8/100 million miles)
\item Same as smoking 30 cigarette's a day
\end{itemize}
\item So rule of thumb: 1 pt blood presure = 1 cigarette/day = 1 mile
walking less walking/day = 10 miles driving/day
\end{itemize}


\subsection*{Idea: distance to line in any direction}

\begin{itemize}
\item So, if you scale X and Y to make their errors correct, you can
look for the distance to the line as your best fit
\begin{itemize}
\item For regression--X gets spread out.
\item for errors in variables, X compressed, so steeper
\end{itemize}
\item This fit is called the First Principle Component
\end{itemize}

\subsection*{PCA}

How do you describe a pancake?
\begin{itemize}
\item You need two diminsions to say where it is.
\item These are the first two principle components
\item In high diminsion, there are many
\end{itemize}

\subsection*{A PCA you have already seen}
Consider all the stocks.  What is the thing that is close to every
 stock?  Called the VW (aka ``the market.'')  You can actually do
 this.  Grab 100s of stock returns and compute the PCA on them.  It
 will look very close to the VW index itself.  (We used to do this in
621--but not any longer.) 

\subsection*{Ok, what about words?}

Reduces the dimision greatly.  



\newpage
\subsection*{Methodolgy}

Look at the .R file for details on how we read in the federalist
papers.  It is just like before.  For those seeing the Rnw source--you
can look at the 10 commands below.

<<readingInData,echo=FALSE>>=
  
federal <- read.csv("federalist.csv",header=TRUE)
federal$words <- strsplit(as.character(federal$text)," ")
federal$num.words <- c(lapply(federal$words,length),recursive=TRUE)
federal$num.char <- c(lapply(federal$text,nchar),recursive=TRUE)
codes <- c(NA, "HAM", "mad")
federal$training <- codes[1 + (federal$author == "HAMILTON") + 2 * (federal$author == "MADISON")]
federal$training01 <- (federal$training == "HAM")

@ 



<<sampleCounts,echo=FALSE>>=

federal[["freq.<c>"]] <- c(lapply(federal$words,function(t){sum(t == "<c>")}),recursive=TRUE) / federal$num.words
federal[["freq.<p>"]] <- c(lapply(federal$words,function(t){sum(t == "<p>")}),recursive=TRUE) / federal$num.words
federal$freq.the <- c(lapply(federal$words,function(t){sum(t == "the")}),recursive=TRUE) / federal$num.words
federal$freq.of  <- c(lapply(federal$words,function(t){sum(t == "of")}),recursive=TRUE) / federal$num.words
federal$freq.to  <- c(lapply(federal$words,function(t){sum(t == "to")}),recursive=TRUE) / federal$num.words
federal$freq.and <- c(lapply(federal$words,function(t){sum(t == "and")}),recursive=TRUE) / federal$num.words
federal$freq.in  <- c(lapply(federal$words,function(t){sum(t == "in")}),recursive=TRUE) / federal$num.words
federal$freq.a   <- c(lapply(federal$words,function(t){sum(t == "a")}),recursive=TRUE) / federal$num.words
federal$freq.be  <- c(lapply(federal$words,function(t){sum(t == "be")}),recursive=TRUE) / federal$num.words
federal$freq.that<- c(lapply(federal$words,function(t){sum(t == "that")}),recursive=TRUE) / federal$num.words
federal$freq.is  <- c(lapply(federal$words,function(t){sum(t == "is")}),recursive=TRUE) / federal$num.words
federal$freq.which<- c(lapply(federal$words,function(t){sum(t == "which")}),recursive=TRUE) / federal$num.words
federal$freq.it  <- c(lapply(federal$words,function(t){sum(t == "it")}),recursive=TRUE) / federal$num.words
@ 

Last time we constructed words by hand.  It looked something like:

<<sampleConstruction>>=

federal$freq.by  <- c(lapply(federal$words,function(t){sum(t == "by")}),recursive=TRUE) / federal$num.words

@ 

This time we want to create many of them.  So we need to first get a
list of all the words we want.

<<makeCounts>>=

all.words = c(federal$words,recursive=TRUE)
counts <- as.data.frame(table(all.words))
counts <- counts[order(counts$Freq,decreasing=TRUE),]
names(counts) <- c("words","counts")

@ 

Now we have to loop through the first 80 most common and add them as
variables to our data.frame.

<<pcs,echo=FALSE>>=

  top.words <- counts[1:80,]

  for(w in as.character(top.words$words))
  {
    my.name <- paste("freq",w,sep=".")
    federal[[my.name]] <- c(lapply(federal$words,function(t){sum(t == w)}),recursive=TRUE) / federal$num.words
  }

variables <- c(11:85)
pcs <- princomp(federal[,variables])$loadings

@ 

Finally, let's predict using the first principle component:

<<plotForecast,fig=TRUE>>=

colors <- c("red","white","black","gray","blue")[federal$author]
federal$pc1 <- as.matrix(federal[,variables]) %*% pcs[,1]
regr <- lm(federal$training01 ~ federal$pc1)
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 

Or using the first 3 principle components:

<<multipleRegression,fig=TRUE>>=

federal$pc2 <- as.matrix(federal[,variables]) %*% pcs[,2]
federal$pc3 <- as.matrix(federal[,variables]) %*% pcs[,3]
regr <- lm(federal$training01 ~ federal$pc1 + federal$pc2 + federal$pc3)
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 


If we instead used all the variables, it would look something like:

<<fig=TRUE>>=

equation <- paste("training01 ~", paste(names(federal)[variables],collapse="+"))
regrFull <- lm(equation,data=federal)
predictions <- predict(regrFull,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)

@ 

\end{document}
