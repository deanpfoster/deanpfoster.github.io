\documentclass{book}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\parnote}[1]{%
  \setlength{\marginparwidth}{.75in} 
  \setlength{\marginparsep}{.1in} 
  \marginpar{\raggedright\small\bf #1}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In order for gray pages to ``bleed'' over to the next page, it is     %
% required that the postscript file be run through the perl script      %
%                                                                       %
% called:                                                               %
%       graypages                                                       %
%                                                                       %
\newcommand{\gray}{  }
\newcommand{\grey}{}
%
%


\newcommand{\white}{  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{assumption}{Assumption}[chapter]
\newtheorem{claim}{Claim}[chapter]
\newtheorem{correlary}{Correlary}[chapter]
\newtheorem{example}{Example}[chapter]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   This is the ``referenced Problem'' homework macros.  Hopefully
%   they will be easier to use then they were to write!  The idea is
%   that a homework problem can be shoved in the text to hopefully be
%   solved as one is reading the chapter.  But, if you want to assign
%   homework problems, it is nice to have them easy to find.  So, at
%   the end of the chapter, they page they are actually on will be
%   referred.  Here is a sample:
%
%\begin{textHW}
%\item[foo] The first in text problem.
%\item[bar] A modification on the above problem.
%\end{textHW}
%
%
%\begin{HW}
%\item Normal homework question at end of chapter.
%\item \seepage{foo}
%\item \seepage{bar}
%\end{HW}
%
%
\newcounter{homeworkcounter}[chapter]
\renewcommand{\thehomeworkcounter}{%
  \fbox{\bf\thechapter.\arabic{homeworkcounter}}%
  }
\newenvironment{textHW}{
  \noindent\rule{\textwidth}{1pt}%
  \begin{list}{}{
      \setlength{\labelwidth}{1cm}
      \setlength{\labelsep}{0.3cm}
      \setlength{\leftmargin}{1.3cm}
      \setlength{\rightmargin}{1cm}
      \setlength{\parsep}{0.5ex plus0.2ex minus0.1ex}
      \setlength{\topsep}{1pt plus3pt minus1pt}
      \setlength{\itemsep}{0ex plus0.2ex} 
      \renewcommand{\makelabel}[1]{\label{thw:##1}{\ref{##1}}}
      \sl}}%
  {\end{list}\rule{\textwidth}{1pt}}
\newcommand{\seepage}[1]{
    \label{#1} 
    See page \pageref{thw:#1}. 
  }
\newenvironment{HW}{
  \begin{list}{\thehomeworkcounter\hfill}{
      \usecounter{homeworkcounter}
      \setlength{\labelwidth}{1cm}
      \setlength{\labelsep}{0.3cm}
      \setlength{\leftmargin}{1.3cm}
      \setlength{\rightmargin}{1cm}
      \setlength{\parsep}{0.5ex plus0.2ex minus0.1ex}
      \setlength{\itemsep}{0ex plus0.2ex} \sl}}%
  {\end{list}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%       this macro will count for you.  To start it, use \startI.
%       To print out the numbers 1,2,3, use \I,\I,\I.
%
\newcounter{deanscounter}
\newcommand{\I}{\arabic{deanscounter}\stepcounter{deanscounter}}
\newcommand{\startI}{\setcounter{deanscounter}{1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Rough Draft of: \\ \quad
  \\
  Strategic Statistics}


\author{Dean Foster\\ (215) 898-8233 \\ 
        {\it foster@hellspark.wharton.upenn.edu}
\and
Rick Vohra \\ (614) 292 4600 \\ {\it vohra.1@osu.edu}} 

\date{first draft: Summer 1994\\ 
  this version: \today 
   }

\newcommand{\notes}{\index{personal notes}}

\newcommand{\ROW}{\hbox{R\raisebox{-.2ex}{\sex \symbol{204}}W}}
% USED to be -.9ex for column
\newcommand{\COLUMN}{\hbox{C\raisebox{-1.1ex}{\sex \symbol{202}}LUMN}}
\newcommand{\row}{\ROW}
\newcommand{\column}{\COLUMN}
\newcommand{\Row}{\ROW}
\newcommand{\Column}{\COLUMN}

\makeindex
\index{Games!zero-sum|see{zero-sum games}}




\begin{document}

\chapter{Worst-case Statistics}
\label{chap:worst_case} 
 
    \section{Story: Which base rate to use?}

Consider the clinic again.  Which base rate should they use:
\begin{itemize}
\item US base rate?
\item SF bay area base rate?
\item Patients at the clinic?
\item Males at the clinic?
\item White males at the clinic?
\item Single white males at the clinic?
\item Single white males age 30-35 at the clinic?
\item Single white heterosexual males, age 30-35 at the clinic?
\item . . .
\item Patients with same social security number!
\end{itemize}

There has been much more data collected about the rate of AIDS in the
whole US than has been collected about the rate of AIDS in the SF Bay
area.  Likewise, more accurate knowledge is available about the bay
area than about the clinic.  We know less and less about each smaller
subset than the previous one.  This continues right on down to the
individual whose base rate we don't know at all.  In fact, merely
saying the base rate of an individual sounds strange.  So, the
population we have the most accurate information about is the largest
one.

On the other hand, as we get to smaller and smaller sets, we are
dealing with a more homogeneous population.  So, the base rate is more
appropriate.  The base rate for AIDS for people who come to the clinic
might be significantly higher than for the SF bay area since they are
all self selected.  Likewise, once we include more demographic
variables, we are in a much more homogeneous population.  So, the
population which is most relevant to our patient is the smallest one! 

One way out of this problem is to assume a probabilistic model which
depends on each of the attributes of the individual; this is called
hierarchical modeling.  Unfortunately this has its own difficulty: the
need to estimate more parameters, with the possible result of
over-fitting.  So the goal of this chapter is to solve this trade-off.
Said more clearly, if we have a good model of who has AIDS, then we
will be able to make good estimates of the prior probability of each
individual having AIDS and so have accurate posterior probabilities.
But, if our model is wrong, we don't want the model to make us even
worse off.  So, the trade-off is between the efficiency of the model
and the robustness of not assuming a particular model.

If you specify the ``base rate'' down to the individual level, or if
you specify a rich enough model, then we end up with having a
parameter for each person.  The most extreme version of either of
these is a model where each person has prior probability of 0 or a
prior probability of 1 of having AIDS.  The amazing thing is that we
can work with this extreme model and still get nice results.  This
is the motivation for considering a worst-case approach to the actual
data, not just to the parameters.  This is a new way of doing
statistics --- leave the probabilistic models behind.  

{\bf Note} This is the first research chapter in this book.  It is
based primarily on work I've done with Rakesh Vohra over the past few
years.  So, I apologize if I have too many details in this chapter.
It is obviously dear to my heart. 

    \section[example]{A toy mathematical example}

%
This section will discuss an example of how to work without
probabilistic assumptions.  It is based on work by Thomas Cover
(199?).
 
        \subsection{Investing}

%


Suppose you have two friends who are hot-shot financial
wizards.\footnote{Should I name them? A and B? Row and column? Assets
and Bonds?  Rich and somebody?}  They
come from different schools of thought and both believe the other to
be totally clueless.  So, in actual fact, you have one friend who is a
financial wizard, and one friend who is an impostor.\footnote{Consider
  yourself lucky: The impostor rate is actually much higher in the real
  world!  Brokers are paid on trades whether they turn out to be
  profitable or not.  Further, even if your broker had private
  information it wouldn't be to his advantage to share it with you.
  Instead, he wants you to buy what you don't already have and sell
  what you do.  We will discuss this sort of ``friend'' in the
  Public Probabilities chapter (\ref{chap:knowledge_games}) after we have
  more knowledge of strategic considerations. } But, you don't know
which is which!  You want to get as rich as your financial wizard
friend but don't want to delve into economics and finance to decide
which friend is correct.  So you are looking for a statistical
investment scheme which will do about as well as whichever of your
richer friend.  Of course, if your statistical combination of your
two friends ends up making more money than either friend you wouldn't
be surprised because you, a statistician, know statistics is very
powerful!

Call the wealth of your first friend at time $t$ $A_t$.  Likewise,
$B_t$ is your second friend's wealth at time $t$.  For simplicity we
will assume that all three of you
start out with one dollar at $t=0$.  At each point in time, you can
have your first friend invest some of your wealth for you, and your
second friend invest the remaining wealth.  The goal then is to (1)
statistically figure out which friend is going to do better (2) have
him/her manage your money so that (3) you will get as rich as he/she gets.

\begin{definition}[IRR]  \label{def:log} The Internal Rate of Return
(IRR) is the constant return on capital that will generate the same
  final wealth.  \label{def:IRR_1} The Internal Rate of Return for $A$,
  then, is: \begin{displaymath} \hbox{IRR}_t(A) = \log_e(A_t/A_0)/t.
  \end{displaymath}
\end{definition}
``A penny saved is a penny earned.''  If Benjamin Franklin had saved a
penny at a bank that would pay interest of 3.5 percent per year, then
in the  4 score years plus between the revolutionary war and the Civil
War, it would double 4 times to all of 16 pennies.  But, in the
remaining 5 score years to the bicentennial it would grow to about
\$10.00.  Alternatively, if he invested this same amount in a bank
paying 7 percent, he would have \$25 at the civil war, and \$10,000 at
modern times. 

The point of the IRR is that it makes sense of long term growth. Of
course, real economists (for example Lord J. M. Keynes) aren't
interested in the long term that we will consider here since they all
plan on being dead by then!  But we are not pretending that we care
how our investment will be doing when the next ice age comes, merely
that {\it if} we cared, then we would want to use something like the
IRR.

\begin{textHW}
\item[hw:IRR] The definition of IRR given above can be generalized to
  have payments in many different time periods.  To do this though, we
  need some other definitions.

  \begin{definition}[NPV] \label{def:NPV} Let $C_t$ be a stream of cash
    flows for $t=1,\ldots,T$.  (Negative values mean you pay, positive
    means money paid to you).  Define the Net Present Value of $C_t$
    to be:

    \begin{displaymath}
      \hbox{NPV}_r(C) = \sum_{t=1}^{T} C_t e^{-rt}
    \end{displaymath}
  \end{definition}

  The $r$ is called the discount factor.  We can now write IRR as:
  
  \begin{definition}  \label{def:IRR} The IRR of $C_t$ is a discount
    rate $r$ such that the $NPV_r(C) = 0$.
  \end{definition}
  \begin{itemize}
  \item Show that this definition agrees with the definition
    \ref{def:IRR_1} on page \pageref{def:IRR_1} when both apply.
  \item {\sloppy \notes Give some data for a house and have them compute
      the IRR for it}
  \item Do some simple calculations of them using Visicalc, or a more
    modern spread sheet.

  \end{itemize}

\item[hw:roots_of_IRR] Show that IRR is the root of a polynomial.
  Show that if there is one payout and one payback, IRR is
  unique. Show that if there is one payout and followed by an income
  stream that is paid in, then then IRR is unique. (Hint: take two
  derivatives.)
\item[hw:existence] Prove (or provide counterexample) that the IRR always
  exists.
\item[hw:non_unique_IRR] Find an example of a 3 period model which has a
  non-unique IRR.  Plot the NPV vs. the discount rate for the example
  you came up with.
\item[hw:particular_non_unique_IRR] Same as above, but have the two
  solutions to the IRR be 10\% and 15\% respectively.
\item[hw:positive_NPV] Define ${\cal D}$ to be the set discount rates
  such that an income stream has positive NPV at that discount rate.
  Show that the IRR is a boundary point of ${\cal D}$.
\end{textHW}


To make life easy, we will assume that initially you and both of
your friends have one dollar each.  Let $A_t$ be the wealth of your
first friend at time $t$.  We can compute the return on $A$'s
investments at time $t$ as:
\begin{displaymath}
R_{A,t} = A_t/A_{t-1}
\end{displaymath}
Let $r_{A,t} = \ln(R_{A,t}) = \log_e(R_{A,t})$ be the grown rate at time $t$.  It
is the rate quoted by banks --- something like 6\% compounded
continuously.  The IRR is just the average of these growth rates.
Note that if $R \approx 1 $ then $r \approx 0$.\footnote{A more exact
  approximation would be $r \approx R -1$.}  Further, we can
reconstruct the final wealth $A_t$ from the return $R_t$ as follows:
\begin{displaymath}
A_T =  \quad \prod_{t=1}^{T} R^A_t \left(= R_{A,1} \times R_{A,2}
\times \cdots \times R_{A,T} \right)
\label{def:product}
\end{displaymath}

At each time period you invest a $w_{t}$ fraction of your
wealth in friend A and a $1-w_{t}$ fraction of your wealth in friend B.
The first fraction grows from $w_t$ to $w_t R_{A,t+1}$ at the end of
the the time period.  The remaining fraction of your wealth $1-w_t$
grows to $(1-w_t)R_{B,t+1}$.  Adding these together and your total
wealth grows as: 
\begin{displaymath}
R_{C,i}  =  w_{i-1} R_{A,i} + (1-w_{i-1}) R_{B,i}
\end{displaymath}


Your goal is to have $C_{t}\cong  \max(A_{t},B_{t})$ which is about
the same thing as having $\hbox{IRR}(C) \cong
\max(\hbox{IRR}(A),\hbox{IRR}(B)) $.

\begin{example}[Invest with your best friend]  One way of hoping to do
  as well as the better of your two friends is to hope that the past
  predicts the future.  So have whichever friend is ahead at time $t$
  invest all of your wealth.  In other words, $w_{t}= 1$
  if $A_{t-1}\ge B_{t-1}$, and $w_{t}= 0$ otherwise.\footnote{Since A
    is a ``slightly'' better friend, break ties in his/her favor.}

  Consider the following situation.  Suppose the wealth of your first
  friend (A) grows as: $1,1,2,2,4,4,8,8,\ldots$ and your second friend
  (B) grows as: $1,2,2,4,4,8,8,16,\ldots$ (see table
  \ref{tab:bad_data_1}). 
  \begin{table*}[htbp]
    \begin{center}
      \leavevmode
      \begin{tabular}{r|c|c|c|c|c|c|c|c|c|c}
        time & $0$ & $  1 $ & $  2 $ & $  3 $ & $  4 $ & $  5 $ & $  6
        $ & $  7 $ & $  \ldots $ & i\\ 
        $A_t $ & $  1 $ & $  1 $ & $  2 $ & $  2 $ & $  4 $ & $  4 $ &
        $  8 $ & $  8 $ & $  \ldots $ & $2^{\lfloor i/2 \rfloor}$\\ 
        $B_t $ & $  1 $ & $  2 $ & $  2 $ & $  4 $ & $  4 $ & $  8 $ &
        $  8 $ & $  16$ & $ \ldots $ & $2^{\lfloor (i+1)/2 \rfloor}$
      \end{tabular}
    \end{center}
    \caption[First bad data]{First bad data sequence}
    \label{tab:bad_data_1}
  \end{table*}%
  Our scheme now will pick friend A for the first investment because
  ties are broken in his/her favor.  But, A gets no return, whereas B
  doubles in value.  So, we switch to B.  But now the story goes the
  other way.  So we switch back.  Each time picking the loser!  Thus,
  $S_{t}= 1$ at all times.  As your friend's wealth grows
  astronomically, your personal wealth doesn't grow at all.

  A's wealth grows exponentially at rate $\cong
  2^{t/2}$.\footnote{Which we should write as $\cong e^{t \ln(2)/2}$ to
    maintain our membership in the mathematics guild.} Whereas 
  $S_{t}$ ``grows'' exponentially at rate $e^{0t}$:
  \begin{eqnarray*}
    IRR_t(A) &\cong & 35\% \quad (=\log(2)/2), \\
    IRR_t(B) &\cong & 35\% \quad (=\log(2)/2), \\
    IRR_t(S) &  =   & 0\%.
  \end{eqnarray*}

\end{example}

The above investment scheme is too sensitive to which is doing better.
Maybe being sensitive is a bad thing.  Let's try a less sensitive
combination --- always have each friend invest 1/2 of your wealth.
This is as unsensitive as you can get---it ignores past performance
entirely.

\begin{example}[Equal weight] \label{exam:equalWEIGHTS} Take $w_{t}= 1/2$ for all $t$.
  Let's consider the same sequence as in the previous example.  We see
  that $C_{1}=1, C_{2}=1.5, C_{3}= 2.25, \ldots $.  In other words,
  $C_{t}\cong (2.25)^{t/2}$, so IRR$_t(C) = .8? \gg .35$!  Thus,
  $C_{t}$ grows exponentially faster than either $A_{t}$ or $B_{t}$.
  In this case, this investment scheme is wonderful. 

  But, will this system always work in all cases?

  Suppose that A doubles his/her wealth every time period, but B's wealth
  never grows.  In other words,
  \begin{eqnarray*}
    A_{t}&=&2^{t-1} \\
    B_{t}&=&1 
  \end{eqnarray*}
  It is easy to see that ${\log_{2}(A_{t})\over t} = 1,
  {\log_{2}(B_{t})\over t} = 0$.  A is a good investor and B is
  awful.  The 50/50 combination
  scheme gets a return of 1.5 every time period.  So,
  $C_{t}=1.5^{t-1}$.  So, ${\log_{2}(C_{t})\over t} =
  \log(1.5)/\log(2) < 1$.  So, S does better than B, but exponentially
  worse than A!
  \begin{eqnarray*}
    IRR_t(A) & = & 69\% \quad (=\log(2)), \\
    IRR_t(B) & = & 0\%, \\
    IRR_t(S) &  =   & .4? \quad (=\log(1.5)/\log(2)).
  \end{eqnarray*}

   Comparing $C_{t}$ to the BETTER of $A_{t}$ and $B_{t}$ on this
  data shows that this scheme is bad.  
\end{example}
  
Neither ``use best'' nor ``$50/50$'' is ALWAYS good.  The first is too
sensitive --- it switches back and forth.  The second won't recognize a
clear cut winner.  The next example shows that a compromise between
these two extremes always works well.

\begin{example}[Value weighted] \label{example:value_weighted}  A value
  weighted portfolio or combination is when the amount invested in a
  stock is proportional to the total value of the stock.  An example
  is the S\&P 500.  It is the ``average'' of the top 500 stocks with
  GM, ATT, and ? ? counting for the biggest shares and \_\_ and \_\_
  the smallest shares.

  In our example, the value weighted portfolio is:
  $$w_{t}=\frac{A_{t-1}}{A_{t-1}+B_{t-1}}.$$
  For example, at time 0, both $A_0$ and $B_0$ are 1, so we should use
  equal weights.  Thus, at time 1, $C_1 = (A_1 + B_1)/2$.  Now we
  invest a fraction $A_1/(A_1+B_1)$ of our wealth with A, and a
  fraction 
  $B_1/(A_1+B_1)$ of our wealth with B.  This works out to $A_1/2$
  total dollars with A, and $B_1/2$ total dollars with B.  So, at time
  2, our total wealth will grow to $(A_2+B_2)/2$.  This pattern
  repeats.  At time $t$ our wealth is $(A_t + B_t)/2$.

  In other words, this investment scheme merely gives 50 cents to A to
  invest at time zero and 50 cents to B to invest at time zero.
  Whatever each friend makes is reinvested with that friend.  This
  type of strategy is often called ``buy and hold.''

  Thus, $C_{t} \ge  \max(A_{t},B_{t})/2$.  In other words, you'll have at
  least 1/2 the money of your richer friend.  This doesn't sound very
  impressive until we compute the growth rates:
  \begin{eqnarray*}
    \hbox{IRR}(C) &=&     \log(C_t)/t \\
    &\ge& \log(\max(A_t,B_t)/2)/t \\
    &&= \max(\log(A_t)/t,\log(B_t)/t) -\log(2)/t \\
    && = \max(\hbox{IRR}(A),\hbox{IRR}(B)) - \log(2)/t \\
    && \cong \max(\hbox{IRR}(A),\hbox{IRR}(B)) 
  \end{eqnarray*}
  So, the growth rate of C is asymptotically equal to the better of
  the growth rates A or B.  
\end{example}

The equal weighting scheme will grow as fast as the better of A and B
regardless of the sequences of ``wins'' and ``losses'' for A and B.
This scheme can't possibly be fooled.  It is statement of this
sort that we will investigate in this chapter.  

\begin{textHW}
\item[hw:Kfriends] Suppose you have $k$ friends instead of just 2.
  Construct a scheme which will generate an IRR within $1/k$ of your
  best friend.  Prove you achieve this bound.
\item[hw:manyFRIENDS] Suppose that over time you meet new friends.
  When you meet a new friend they always happen to have exactly \$1.
  You in fact meet exactly one new friend each and every day.  Come up
  with a scheme that will do as well as any of your friends you ever
  happen to meet (in terms of IRR).
\item[hw:infiniteFRIENDS]  This time you hold a party on day zero, and
  all the friends of the previous problem show up.  Find a scheme
  which does as well as any of your countable number of friends.
\item[hw:continuiumFRIENDS] Suppose you had an uncountable number of
  friends.  Prove that there is a collection of friends and a data
  sequence such that any scheme will have a friend that it doesn't
  beat in the limit.
\end{textHW}


\paragraph{What have we learned}  The important message is not about
financial investment, but instead what might be possible in the worst
case and what isn't possible. 
\begin{itemize}
\item The first message is that we will never be able to do as well as
the maximum of $A$ and $B$ since that would require guessing correctly
on the first move and staying with that person.  So, we have to be
careful to have a utility function which doesn't punish errors too
cruelly.

Recall our Ben Franklin example.  If he put 1/2 a penny at 3\% and
  1/2 a penny at 7\%, after 200 years, his friends would have grown
  their pennies to \$10 and \$10,000 respectively, whereas he would
  have grown his to \$5,005.  On a log scale, \$5,000 is much closer to
  \$10,000 than to \$10.  But, it still sounds like Benjamin is
  \$4,995 short of having made the correct decision!  


 \item The second point is that what works well in the worst-case, is
   exactly what is recommended by real finance professors --- namely
   buy a portfolio and stick with it.  So, good performance in the
   worst-case might be identical to good performance in the
   average-case.

   This fact is almost a ``meta''-theorem in computer science.  They
   have found that when somebody finds an algorithm that works for
   random data often a worst-case solution that is just as fast is
   right around the corner.  When this doesn't occur, the excuse is
   that the random case was actually a trivial random case --- not really
   the correct model for data!  In other words, via the minimax
   theorem, good average-case performance for some random situation is
   identical to the worst-case overall.
\item Another take home message is that we aren't looking for absolute
  performance, only performance relative to that of our friends.  So,
  we don't have a procedure which guarantees an IRR of .07, only one
  that guarantees a performance as good as that of our friends.  This
  is often called a competitive ratio.
\end{itemize}



        \subsection{Forecasting}

We will now modify our statements about finance to statements about
probability.  The math will stay the same, only the names will change.
Thus, keep in mind the problems that made the finance example less
that totally convincing: needing 100s of years, requiring the use of
log dollars instead of real dollars, and only generating results that
compare to friends instead of absolute performance.  Hopefully these
won't be as painful as before.

 Instead of having two friends who are experts in finance, we will
consider two friends who are experts in meteorology.  Each day it
either rains or it doesn't rain.  Our goal is to forecast this event.
Each day, we ask our two friends for their estimated probability
of rain on the next day.  We will call these two forecasts of rain on
day $i$: $\widehat{p}_{A,i}$ and $\widehat{p}_{B,i}$.  Again, we don't
want to delve into meteorology to decide which friend has a better
theory of weather --- instead we want to use statistics to figure out
which is better.  Our utility of a forecast
will be: (this utility function was discussed in section
\ref{sec:intro_to_loss_functions}, on page 
\pageref{sec:intro_to_loss_functions})
\begin{equation}
  \label{eqn:log_forecasting_utility}
  U(\widehat{p},X) = X \log(\widehat{p}) + (1-X) \log(1-\widehat{p})
\end{equation}
We will evaluate our two friends on the average utility they generate
on a sequence of forecasts.  Namely:
\begin{displaymath}
 \bar{U}_X(\widehat{p})  \equiv \sum_{i=1}^t 
\frac{U(\widehat{p}_i,X_i) }{t}.
\end{displaymath}
To see that this definition is vaguely useful, consider the
alternating data sequence $1,0,1,0,1,\ldots$  If a forecast always
generated $\widehat{p} = 1/3$, then in the limit the average utility
would be $\log(1/3)/2 + log(2/3)/2 \cong - ? ?$. On the other hand, a
forecast of always 1/2 would generate a average utility of $log(1/2)
\cong .69$ which is better than the 1/3 forecast.  Neither of these is
wonderful since they don't notice the alternating pattern.  So, a
forecast that does notice this pattern and forecasts $1,0,1,0,\ldots$
generates a average utility of $\log(1) = 0$ --- the best possible.  So,
this utility captures our intuition of what forecasting quality is all
about.  

Define the likelihood for forecasts done by A as:
 $$A_t = \prod_{i=1}^t \widehat{p}_{A,i}^{X_i}(1 -
 \widehat{p}_{A,i})^{1-X_i}.$$
Notice that $A_0$ is by definition 1.  The average utility of the forecasts
generated by A is $\log(A_t)/t$.  If we think of $A_t$ as a
wealth, our utility of forecasts using A is identically equal to
$\hbox{IRR}(A)$.  So, forecasting well is identical to getting rich!

This suggests that we should use the valued weighted scheme of example
\ref{example:value_weighted} on page \pageref{example:value_weighted}.  We
will forecast
$$\widehat{p}_{S,t} = w_t \widehat{p}_{A,t} + (1-w_t)
\widehat{p}_{B,t}.$$
with weights $w_t = \frac{A_{t-1}}{A_{t-1} + B_{t-1}}$.



\begin{theorem} \label{thm:twoFORECASTS} Given any two forecasting
  schemes: $\widehat{p}_{A,i}$ 
  and $\widehat{p}_{B,i}$ we can find a scheme $\widehat{p}_{S,i}$
  such that $\bar{U}_t(S) \ge \max\{\bar{U}_t(A),\bar{U}_t(B)\} -
  \log(2)/t$ for the log utility given in equation
  \ref{eqn:log_forecasting_utility}.
\end{theorem}


\begin{textHW}
\item[hw:probLOG] Show that 
        \begin{equation}
        \label{eqn:convex_log}
        U(\widehat{p}_c,X_i) \ge w
          U(\widehat{p}_a,X_i) + (1-w) U(\widehat{p}_b,X_i).
        \end{equation}
\item[hw:proveLOG] Prove Theorem \ref{thm:twoFORECASTS}.
\item[hw:exact_when_using_randomization]  Show that if instead of taking
  a weighted combination of each forecast, one of the two forecasts is
  chosen at random, then the expected utility is equal to the
  expected log of money made. In other words, equation
  \ref{eqn:convex_log} is exact. 
\item[hw:worst_equals_bayes] Consider two models for a sequence of binary
  events $X_i$.  Assign a prior of 1/2 to each of these two models.
  Show that $E(X_i|X_1,\ldots,X_{i-1})$ is identical to the forecast
  used in the worst-case analysis above.
\end{textHW}

{\notes

\begin{itemize}
\item Possible under quadratic loss (Ref: foster) $(|{\cal X}|=2)$

\item Possible under absolute loss (ref: Blackwell$) (| {\cal
    X}| =2)$

\item  Do we need a different theorem for each loss?
  \begin{enumerate}
  \item No.  A general result exists (combining forecasts) (Ref: Foster
    and Vohra)
  \item  Holds for any bounded loss function
  \item Unfortunately, changing the loss function will change the
    forecast
  \item In each case, the past loss will predict the future loss.  So
    minimizing past loss is a good way of having low future loss
  \end{enumerate}
\item similar to Bayes --- ie full model
\end{itemize}
}
 
    \section[Ratifiability]{Ratifiability: A mathematically convenient concept}
        \subsection{Story: Regretting bad investments}
%
The following appeared in the {\it Wall Street Journal} on March 15,
1995 and was written by Ben Stein.
\begin{quote}
{\small

  In 1979, my wife and I bought a house in the best part of Aspen,
  Colorado.  We paid so little for it I could cry.  In 1982 I decided that
  Aspen had seen better days and sold the house.  I had a good gain
  when I sold it --- for about one-fourth of what it would be worth now.
  It used to torment me.  When I moved to Los Angeles, I could have
  bought a house in the flats of Beverly Hills.  It would have been a
  stretch, but I could have done it.  I didn't and now I couldn't buy
  one if I were Bill Bennett.

I learned about Berkshire-Hathaway when it was \$900 a share.  I could
easily have bought a hundred --- and if I had, I now would be able to
retire, since it trades at around \$22,000 a share these days.  I did
buy a few, and a broker talked me into selling some when a share was
about \$7,000.

In 1980, I looked at co-ops in Manhattan.  I saw a small but lovely
penthouse at 72nd and Park for \$260,000.  I decided it was too high.
That used to wake me up at five in the morning with self-loathing and
torment.  

I bought in Malibu, after 14 years of brooding about it, very near the
top of the market, before real estate in the 'Bu began a long,
sickening fall that's still going on.  I often can't think straight
when I think of how much I have lost.  

That's why I want to thank Nicholas Leeson so much.  I love him and
his brothers Robert Citron and Joe Jett, of Orange Country and
Kidder, Peabody respectively.  God bless these worthy fellows.  Misery
loves company.  Misery especially loves to see someone far, far worse
off. 

In particular, when Ben Stein is feeling bad about his investments, he
loves to see that people who make their living buying and selling
investments can lose absolutely everything --- not just the down payment
on a house, but hundreds of millions, maybe billions of Other People's
Money.  Why should I possibly blame myself for my piddling
mistakes --- which did not bankrupt me or anyone else {\it yet} --- when
there is someone who was paid grandly by the oldest merchant bank in
London, and lost over a billion dollars of the bank's money?  Why
should I, a lawyer and an economist and an {\em actor}, for heaven's
sake, feel even a little bad about betting wrong on one house --- and
thereby feel guilty about taking long vacations --- when a man who was
paid for it bet wrong on the whole Japanese stock market and killed
Baring Brothers?

I sort of feel as if I owe something to Mr. Leeson for making me feel
so much better.  I can put down the antidepressants, put down the
number of the 700 Club, put down the shame.  Thank you, Nick.  Thank
you, Robert.  Thank you, Joe.  You have shown me that there are far
bigger losers than I am, far worse fools about investments.  You're my
pals.  Wherever you are, Nick, I'm thanking you.   You're an
investor's best friend: You do something better than make me feel
rich.  You make me feel {\em smart.} }
\end{quote}

This quote is from a fellow who knows regret!  


What things are appropriate to have regret over and what things aren't
appropriate?  When you are retired, you might:
\begin{enumerate}
\item Feel regret over not having put any
money in savings.  
\item  Feel regret about putting it in a low-return
  passbook account. 
\item  Feel regret about losing lots of it in options or
  other high risk items.  
\end{enumerate}

But, you would not feel regret that you didn't buy IBM on
  July 6, 1985, and sell it on Aug 8th, 1986, and buy a portfolio of
  biotech firms which you sell exactly on Dec 9th, 1989, and buy the
  NIKKIE (Japan stock market index) and wait for it to rise to exactly
  3089(??), at which point you short the NIKKIE.

{\notes the above needs fixing up.  The concept confused a reader.}

The idea is that it is only reasonable to feel regret over a simple
enough action that you MIGHT have been smart enough to figure out.
So, you don't feel regret that you didn't exactly guess the market,
instead you feel regret for simple things --- like not saving anything at
all!
 
        \subsection{Defining Ratifiability}

{\notes Use the word ratifiability or just use No-regret?}

The first three scenarios above have the property that a simple action
would have generated a much better result than what actually occurred.
In other words, it is reasonable to have hoped to have been smart
enough to have figured out the correct answer.  But, the last case is
not reasonable.  So we won't call that a reasonable regret. 

Ratifiability is a simple concept: It says that when you did an
action $A$, you wouldn't be better off having done action $B$.  This
wouldn't be a reasonable regret if you only did A once --- how were
you to know it would turn out badly compared to B?  But, if you did it
twice, and B turns out to be better, that is a more reasonable regret:
``Fool me once, shame on you, Fool me twice shame on
me.''\footnote{\notes Is this the correct phrase?  Is it supposed to
  rhyme?} If this now happened many times, then you could change a
whole mess of A's into a whole mess of B's.  This is what we are
looking for in regret.  There were enough times that B was better than
A so it was reasonable to have learned B.  If we didn't learn B, then
we would feel regret.

In terms of a sequence, no-regret means that at the end of a long
sequence of decisions, you wouldn't have been better off if you
switched all your $A$ actions to $B$ actions.  

To make this concept more precise we have the following definition: 

\begin{definition}[No Regret]  The regret generated by changing all
$i$ actions to $j$ actions is called $ R^{i \rightarrow j}$.  If this
regret is large, then we are disappointed and have regrets, if it is
zero (or even negative) then we are happy and have no regret.
\end{definition}

For example, suppose over 100 decisions, we choose action A 12 times.
If on these 12 times our total loss was 45 and our total loss would
have been 30 if we took action B on those 12 times, then $R^{A
\rightarrow B} = 15$.  Notice that if we played B on the remaining 88
times, we could still feel regret the other way!  So, $R^{A
\rightarrow B}$ and $R^{A \rightarrow B}$ might both be positive at
the same time.  Hopefully this won't occur very often.

The procedure we will follow has the following intuition.  Suppose
that $R^{A \rightarrow B } = 5$ and $R^{B \rightarrow A} = -10$ then
if we play B, our the regret $R^{A \rightarrow B }$ won't go up, and
and $R^{B \rightarrow A}$ has room to absorb some errors.  So, playing
B makes sense.  We will spend the next few paragraphs checking that
the following definition captures this intuition:

\begin{definition}[Flow balanced weights]  The weights $w$ will be
called flow balanced if for all $i$:
$$ \sum_{j=1}^k \left(R^{ji} \vee 0 \right) w_j = w_i \sum_{j=1}^k
\left(R^{ij} \vee 0\right)$$
and the sum of the $w_i$'s is equal to one.
\end{definition}

\begin{figure*}[htp]
\fbox{\parbox{\textwidth}{
\centerline{\fbox{\parbox{5in}{\vspace{3ex} 
\vspace{1in} 
  \centerline{see the orange page}
\vspace{1in}
}}}
\caption[Flow diagram]{{\bf Flow diagram.}  Notice that there is a
flow from A to B but not the other way around.  That is because 
$R^{A \rightarrow B}$ is positive but $R^{B \rightarrow A}$ is
negative.} 
\label{fig:simpleREGRET}
}}
\end{figure*}
Figure \ref{fig:simpleREGRET} show the regret for the case where that
$R^{A \rightarrow B } = 5$ and $R^{B \rightarrow A} = -10$.  Notice
that if water were flowing in this diagram, it would accumulate at the
B node.  So, we want to play B with probability one.

Now consider a three choice situation.  If 
\begin{eqnarray*}
R^{A \rightarrow B } &=& 5 \\
R^{B \rightarrow A } &=& -10 \\
R^{B \rightarrow C } &=& 9 \\
R^{C \rightarrow B } &=& -1 \\
R^{A \rightarrow C } &=& -20 \\
R^{C \rightarrow A } &=& -30
\end{eqnarray*}

\begin{figure*}[htp]
  \setlength{\fboxrule}{.1mm}
  \setlength{\fboxsep}{1ex}
  \fbox{\parbox{\textwidth}{
      \vspace{3ex}
  \centerline{
  \setlength{\unitlength}{1in}
  \begin{picture}(2,1.5)
    \thicklines
    \put(0,.1){\framebox(2,1.5)[lt]}
    \put(.5, 1.2){\circle{.25}\makebox(0,0){1}}
    \put(1.5,1.2){\circle{.25}\makebox(0,0){2}}
    \put(1,.35){\circle{.25}\makebox(0,0){3}}
  \end{picture}}
        \centerline{\notes add arcs}
\caption[Flow diagram]{{\bf Flow diagram for three choices.}} 
\label{fig:threeREGRET}
}}
\end{figure*}
This is portrayed in figure \ref{fig:threeREGRET}.   We don't want to
play A because we might feel more regret toward B.  But, we don't want
to play B either since we might feel regret toward C.  C then is
perfect since it has no regret flowing out of it at all.  Thus, in
this case, the flow balance weights would be to play C with
probability one.


For our final example, consider a situation where we have regret in
both directions.  
\begin{eqnarray*}
R^{A \rightarrow B } &=& 5 \\
R^{B \rightarrow A } &=& -10 \\
R^{B \rightarrow C } &=& 9 \\
R^{C \rightarrow B } &=& {\bf 1} \\
R^{A \rightarrow C } &=& -20 \\
R^{C \rightarrow A } &=& -30
\end{eqnarray*}
\begin{figure*}[htp]
  \setlength{\fboxrule}{.1mm}
  \setlength{\fboxsep}{1ex}
  \fbox{\parbox{\textwidth}{
      \vspace{3ex}
  \centerline{
  \setlength{\unitlength}{1in}
  \begin{picture}(2,1.5)
    \thicklines
    \put(0,.1){\framebox(2,1.5)[lt]}
    \put(.5, 1.2){\circle{.25}\makebox(0,0){1}}
    \put(1.5,1.2){\circle{.25}\makebox(0,0){2}}
    \put(1,.35){\circle{.25}\makebox(0,0){3}}
  \end{picture}}
  \centerline{\notes Decorate above graph with arcs}
\caption[Flow diagram]{{\bf Flow diagram for three choices.}} 
\label{fig:threeREGRETloop}
}}
\end{figure*}
We have only changed one number and that one only slightly.  We
changed $R^{C \rightarrow B}$ from minus 1 to plus 1.  So, we don't
expect the flow balance weights to change very much.  This is
portrayed in Figure \ref{fig:threeREGRET}.  Again we don't want to
play A or B.  But, this time we don't want to play C either!  So what
are we to do?  The amount by which we don't want to play B is much larger than
the amount by which we don't want to play C, so it is better to play
mostly C and somewhat B.  In fact, the flow balance works out to be
90\% C and 10\% B.  

What happens then is that if B turns out to be worse than C, it is a
good thing since we were playing C mostly anyway.  But, if C turns out
to be better, then our regret $R^{C \rightarrow B}$ will increase and
our $R^{B \rightarrow C}$ will go down.  So, the regret that was worse
got better!  The next section will prove that this balancing act in
fact works.

\begin{theorem}[No regret]  Suppose that at each time period $t$, an action is
randomly chosen with probability $w$ from a list of $k$ possible
actions.  If $w$ is a flow balanced weight, then the maximum regret up
to time $T$ is approximately the square root of $T$.  
\end{theorem}


\gray
        \subsection{Existence of ratifiable forecasts*}
%
Let $C_t$ be a combination of forecasts $F^1_t,F^2_t,\ldots,F^k_t$.
Let $L^i_t = L(X_t,F^i_t)$ be the loss of using forecast $F^i$ at time
$t$.  We will assume that $C$ is a convex combination of the $F$'s
(i.e. a random combination).  So, the loss of using $C$ at time $t$ is
$\bar{L}^C_t = \sum L^i_t w^i_t$ where $w^i$ is a weight (i.e.
probability) of the $F_i$ at time $t$.  Define $R^{ij}_t$ as follows:
\begin{equation}
  R^{i \rightarrow j} = R^{ij}_t \equiv \sum_{s=1}^t w^i_s (L^i_s -
  L^j_s)
  \label{def:pairwise_regret}
\end{equation}

This is thought of as the regret of replacing forecast $i$ by
forecast $j$.  In other words, it is how much the loss would improve
by if {\it every time} $C$ was going to use forecast $i$, the forecast
$j$ was used instead.  Obviously, if $R^{ij}$ is negative, there is no
regret. 

The average loss incurred by forecast $F^j$ is:
\begin{equation}
  \label{eqn:BetterThan}
  \bar{L}^j_t = \sum_{s=1}^t L_s^j =  \bar{L}^C_t - \sum_i R^{ij}_t 
\end{equation} 
Thus, $C$ is ``better than'' $F^j$ if\footnote{Since $\sum_i R^{ij}_t$
  can be negative, we need the $\le o(t)$ instead of just saying $=
  o(t)$.}
\begin{equation}
\sum_i R^{ij}_t \le o(t)
  \label{eqn:BetterThan2}
\end{equation}
  If this held for all
$j$, then $C$ would be better than the best forecast which was used to
create $C$.

This section will instead look at a stronger condition --- namely
``no-regret.''  No-regret is captured by

\begin{equation}
  R^{ij}_t \dot{\le} 0
  \label{eqn:NoRegTs}
\end{equation}
for all $i$ and $j$.  By summing over $i$ of equation
(\ref{eqn:NoRegTs}) we get exactly equation (\ref{eqn:BetterThan2}).  Thus
``no-regret'' implies ``better than.''

\begin{textHW}
\item[hw:k=2:regret=betterthan] Show that for $k=2$ ``no-regret'' is
  identical to ``better than.''
\end{textHW}

\begin{definition}[Regret]   Define the sup-regret as:
  \begin{equation}
    R_t \equiv \sup_{i,j} R^{ij}_t
    \label{eqn:MaxRegret}
  \end{equation}
  It will some times be convenient to talk about the component
  regret: 
\begin{displaymath}
R^i_t \equiv \sup_{j} R^{ij}_t.
\end{displaymath}
Obviously, $R_t = \sup_i
  R^i_t.$
\end{definition}
Note that $R^i_t \ge 0$ since $R^{ii} = 0$.  The regret
measures how much better we could have done if we had the ``same
pattern matching skill'' but we took different actions.  The smaller
the regret is the better the combination is.

\begin{definition}[No regret/Ratifiability] We will say that $C$
  combines forecasts $F^1,F^2,$ $\ldots,F^k$ without regret if, $R_t$
  is $o(t).$  (This will sometimes be called a ratifiable combination.)
\end{definition}
 
        \subsection{Existence of a ratifiable forecast*}
           \subsubsection{Defining ``a''*}
%

We will use the function $a_\epsilon$ as a tool in the following: (This
section will define $a_\epsilon$ and show some of its basic properties)

\begin{equation}
  A_\epsilon(x) \equiv \left\{ 
  \begin{array}{l@{\qquad}l}
    \displaystyle
    \frac{\epsilon x^2}{2}  & \displaystyle x \ge 0 \\
    0                                               & x \le 0
  \end{array}
  \right.  
  \label{defA}
\end{equation}

\begin{equation}
  a_\epsilon(x) = A_\epsilon' = 0 \vee \epsilon x = \left\{ 
  \begin{array}{l@{\qquad}l}
    \epsilon x            & x \ge 0 \\
    0                           & x \le 0
  \end{array}
  \right.  
  \label{eqn:A'}
\end{equation}

\begin{equation}
  a_\epsilon'(x)= A_\epsilon'' = \epsilon I_{x \ge 0} = \left\{ 
  \begin{array}{l@{\qquad}l}
    \epsilon    &  x \ge 0 \\
    0             & x \le  0
  \end{array}
  \right.  
  \label{eqn:A''}
\end{equation}
It is easy to see that:
\begin{equation}
  A_\epsilon(x) \ge x - \frac{1}{2\epsilon}, 
  \label{eqn:A_ge_x}
\end{equation}

\begin{equation}
  A_\epsilon(x) \ge 0,
  \label{eqn:A_ge_0}
\end{equation}
 
 
           \subsubsection{Relating regret to ``A''*}

We can bound the sup-regret by:
\begin{eqnarray*}
R_t & = & \sup_{i,j} R^{ij}_t \\
    & = & 1/\epsilon + \sup_{i,j} (R^{ij} - 1/\epsilon) \\
    &\le& 1/\epsilon + \sum_{i,j} \max\{R^{ij} - 1/\epsilon,0\} \\
    &\le& 1/\epsilon + \sum_{i,j} A^\epsilon(R^{ij})
\end{eqnarray*}
Define $\tilde{R}_t$ as:
\begin{displaymath}
\tilde{R}^\epsilon_t \equiv \sum_{i,j:j \neq i} g_\epsilon(R^{ij}_t)
\end{displaymath}
So,  $R_t \le \tilde{R}^\epsilon_t + 1/\epsilon$.  This generates
the following obvious claim: 
\begin{claim} If  $ \lim\limits_{t \to \infty} \inf\limits_{\epsilon
    \ge 0} \left(\tilde{R}^\epsilon_t + 1/\epsilon\right)/t =
  0,$ then $C$ combines $F^1,F^2,\ldots,F^k$ without regret.
\end{claim}
 
           \subsubsection{Choosing the weights*}



\begin{figure*}[htp]
  \setlength{\fboxrule}{.1mm}
  \setlength{\fboxsep}{1ex}
  \fbox{\parbox{\textwidth}{
      \vspace{3ex}
  \centerline{
  \setlength{\unitlength}{1in}
  \begin{picture}(2,1.5)
    \thicklines
    \put(0,.1){\framebox(2,1.5)[lt]}
    \put(.5, 1.2){\circle{.25}\makebox(0,0){1}}
    \put(1.5,1.2){\circle{.25}\makebox(0,0){2}}
    \put(1,.35){\circle{.25}\makebox(0,0){3}}
  \end{picture}}
\caption[short title]{  Regret between three forecasts: 1,2,3: 
  $ R^{12} = 10$, $R^{23} = 5$,  $R^{32} = 5$, $R^{21} = -3$, $R^{13}
  = -2$, and $R^{31} = -5$. 

 {\notes Note: Draw arcs from 1 to 2 (labeled 10), and 2 to 3 (labeled 5), and
    3 to 2 (labeled 5). }}
\label{fig:regret}
}}
\end{figure*}


\begin{itemize}
\item Pick $w$ such that for all $i$:
$$ \sum_{j=1}^k \left(R^{ji} \vee 0 \right) w_j = w_i \sum_{j=1}^k
\left(R^{ij} \vee 0\right)$$
\end{itemize}

\begin{lemma}
  There exists a $w_t$ such that for all $L$:
$$\sum_{i,j:i \ne j} (w^i_t L^i - w^i_t L^j_t)  g_\epsilon'(R^{ij}_{t-1}) = 0$$
\end{lemma}


 Proof of lemma 1:
Note that:

$$\sum_{i,j:i \ne j} (w^i_t L^i - w^i_t L^j_t) g_\epsilon'(R^{ij}_{t-1}) = w^TAL $$ 

where $A^{ij} \equiv g_\epsilon'(R^{ij}_{t-1})$ for $i \neq j$ and $A^{ii} \equiv - \sum\limits_{j:j
  \ne i} g_\epsilon'(R^{ij}_{t-1})$.  The following claim
will complete our proof.
Claim: There exists a $w$, such that $w_i>0$,
        $\sum_i w_i = 1$, and $w^TA = 0$.

Proof of claim:

(Note: this claim can also be proved using the minimax theorem.  The
idea is to view $A$ as a payoff matrix.  Then a best response for
``w'' is to play what ever strategy has the largest ``L''.  This
guarantees a payoff of zero. So the value of the game is zero.  Thus,
there exists a $w$ which achieves this zero payoff.)

Let $P_t = \exp\{t \cdot A \}$, where $\exp$ is the matrix exponential
operator.  Then, $P$ is a transition matrix for some Markov chain.
Thus, there exists a stable distribution for $P$, which we will call
$\pi$.  I.e. $\pi^TP = \pi$.  Note that $\lim_{\epsilon \to 0}
(P_\epsilon - P_0)/\epsilon = A$.  Thus, $\pi^TA = 0$, so $w = \pi$ is
our desired solution.

\hfill q.e.d. (claim)

\hfill q.e.d. (lemma 1)
           \subsubsection[Properties*]{Recursive properties of $\tilde{R}$*}
%       
We will now repeat our analysis for $k=2$ in the general setting.  As
when $k=2$, the primary step is generating a recursive relationship
for $\tilde{R}^\epsilon_t$ in terms of $\tilde{R}^\epsilon_{t-1}$. 

A Taylor series expansion for $ A_\epsilon(R^{ij}_t)$ around the
point $R^{ij}_{t-1}$ is:
\begin{eqnarray*}
  A_\epsilon(R^{ij}_t) - A_\epsilon(R^{ij}_{t-1}) \le
   & (w^i_t L^i - w^i_t L^j_t)   A_\epsilon'(R^{ij}_{t-1}) +
  \\
   & +(w^i_t L^i - w^i_t L^j_t)^2  \max_x A_\epsilon''(x)
\end{eqnarray*}

 Using the fact that $H''(x) \le \epsilon$ we get:
\begin{eqnarray}
 \tilde{R}^\epsilon_t -\tilde{R}^\epsilon_{t-1} &\le& \sum_{i,j:i \ne j}
 \left( (w^i_t L^i - w^i_t L^j_t) g_\epsilon'(R^{ij}_{t-1}) +
 2\epsilon(w^i_t L^i - w^i_t L^j_t)^2 \right) \nonumber \\
 &=& \epsilon \sum_{i,j:i \ne j}
 \left( (w^i_t L^i - w^i_t L^j_t) (X \vee 0)\right) + (k-1)\epsilon
 \nonumber \\
 &=& \epsilon \sum_{i=1}^{k} L^i \left( \sum_{j=1}^k \left(R^{ji} \vee
 0 \right) w_j - w_i \sum_{j=1}^k\left(  R^{ij} \vee 0 \right) \right) +
 (k-1)\epsilon \nonumber \\ 
 &=& (k-1)\epsilon \label{eqn:bound_on_regret}\\ 
\end{eqnarray}


\begin{theorem}[Existence of no-regret forecast]
  There exists a combination procedure $C$ which has no-regret.
\end{theorem}

Proof: By equation \ref{eqn:bound_on_regret}, $\tilde{R}^\epsilon_t
-\tilde{R}^\epsilon_{t-1} \le \displaystyle \frac{(k-1)}{2\epsilon}$.
Thus, $\tilde{R}^\epsilon_t - \tilde{R}^\epsilon_{0} \le t
\frac{(k-1)}{2\epsilon}$.  But, we know that $\tilde{R}^\epsilon_{0} =
k(k-1)\epsilon/4$.  Now, $\tilde{R}^\epsilon_{t} \le t
\frac{(k-1)}{2\epsilon} + k(k-1)\epsilon/4$.  So, $R_t/t \le
\frac{(k-1)}{2\epsilon} + o(1/t)$.  Which by choosing $\epsilon$ as
large as we like, the regret can be made as small as we like for a
sufficiently large $t$.

\hfill q.e.d.

{\notes Note: The above should really deal with a changing $\epsilon$ (i.e. a
time dependent $\epsilon_t$.) as was done in the $k=2$ case.} 
\begin{displaymath}
 \displaystyle \sum_{i,j:i \ne j}(w^i_t L^i - w^i_t L^j_t)^2 \le
\sum_{i,j:i \ne j} (w^i_t)^2 \le k-1 \max_w \sum_i (w_t^i)^2 = (k-1) 
\end{displaymath}


 
           \subsubsection[Summary*]{Mathematical summary of ratifiability theorem*}
%

Let $C_t$ be a combination of forecasts $F^1_t, F^2_t, \ldots,F^k_t$.
Let $L^i_t$ be the loss of using forecast $F^i$ at time $t$. The
expected loss of using $C$ at time $t$ is $\sum L^i_t w^i_t$ where
$w^i_t$ is a probability that $C_t$ picks $F^i_t$.  Define:
$$ R^{ij}_t \equiv \sum_{s=1}^t w^i_s(L^i_s - L^j_s) \qquad \hbox{and}
\qquad R_t \equiv \sup_{i,j} R^{ij}_t.
$$ To say $C$ combines the $F$'s with no-regret is to say $R_t$ is
$o(t).$ Define

        $$ \tilde{R}^\epsilon_t \equiv \sum_{i,j:j \neq i}
        A_\epsilon(R^{ij}_t) \quad \hbox{where} \quad
        A_\epsilon(x) \equiv \left\{
  \begin{array}{l@{\qquad}l}
    \epsilon x^2/2 & x \ge 0 \\ 0 & x \le 0
  \end{array}
\right.
$$ Then $ \displaystyle R_t \le \sum_{i,j:j \neq i} 0 \vee R^{ij}_t
\le 1/\epsilon + \tilde{R}^\epsilon_t$ where $\vee$ is the maximum
function and where the second inequality follows from $ 0 \vee (x
-1/\epsilon) \le A_\epsilon(x)$.

Because $|A_\epsilon''(x)| \le \epsilon)$ a Taylor expansion of
$A_\epsilon(x)$ shows:
$$ \tilde{R}^\epsilon_t -\tilde{R}^\epsilon_{t-1} \le \sum_{i,j:i \ne j}
w^i_t(L_t^i - L_t^j) g_\epsilon'(R^{ij}_{t-1}) +
\frac{(w^i_t)^2( L_t^i - L_t^j)^2}{2\epsilon}
$$ By choosing $w_t$ carefully the first derivative term is always
zero.  Assuming that $|L^i_t - L^j_t| \le 1$ we get, $ \displaystyle
\tilde{R}^\epsilon_t -\tilde{R}^\epsilon_{t-1} \le \sum_{i,j:i \ne j}
(w^i)^2\epsilon \le (k-1)\epsilon$.  Then


  \begin{eqnarray*}
    \displaystyle R_t \le & \displaystyle \tilde{R}^\epsilon_t =
    \sum_{s=1}^t \left(\tilde R^{\epsilon_s}_s - \tilde
    R^{\epsilon_{s-1}}_s\right) + \sum_{s=1}^t \left(\tilde
    R^{\epsilon_{s-1}}_s - R^{\epsilon_{s-1}}_{s-1}\right) \le \\ &
    \displaystyle \le k(k-1)\sqrt{t/k} + (k-1)\sqrt{tk} \le
    2k\sqrt{t} = o(t).
  \end{eqnarray*}

\white
    \section{Calibration} \label{sec:calibration}

        \subsection{Relating calibration to no regret}

\begin{itemize}
        \item Define calibration function

        \item Relate calibration to regret $C_t \approx \sum_i \max_j
          R^{ij}_t/t$ 
        \item Define exact calibration function (handout from talk)
        \item Show randomization allows better than Oakes
        \item Claim theorem and describe algorithm
        \item Define calibration for distributional forecast
\end{itemize}

\gray
        \subsection[Existence*]{Existence of Calibrated Forecasts*}


\begin{theorem}[Calibration] There exist a calibrated forecast.
\end{theorem}
 
\begin{textHW}
\item[hw:proper_and_calibrated] {\notes Prove a theorem that says that,
    given a sequence of forecasts, there is a calibrated sequence that
    is ``better'' according to all proper scoring rules.}
\end{textHW}
 
            \subsubsection{Stolen from newer calibration proof*}  

%

Let $\hat{p}_t \in \{0,\frac{1}{n}, \frac{2}{k}, \ldots,
\frac{k-1}{k},1\}$ be a randomized forecast for the event $X_t$ at
time $t$, such that the probability that $P(\hat{p}_t = \frac{i}{k}) =
w_t(i)$.  Modify $n$, $\rho$ and $C$ to get:

\begin{equation} % defining new n, \rho and C
\begin{array}{c|c}
\hbox{Old} & \hbox{New} \\ \hline
\displaystyle
n_t(i) \equiv \sum_{s=1}^{t} I_{\hat{p}_t=\frac{i}{k}} & 
\displaystyle
\tilde{n}_t(i) \equiv \sum_{s=1}^{t} w_s(i) \\
\displaystyle
\rho_t(i) \equiv \sum_{s=1}^{t}
        \frac{I_{\hat{p}_t=\frac{i}{k}}I_{X_s}}{n_t(i)} &
\displaystyle
\tilde{\rho}_t(i) \equiv \sum_{s=1}^{t}
        \frac{w_s(i)I_{X_s}}{\tilde{n}_t(i)} \\
\displaystyle
C_t \equiv \sum_{j=0}^k \frac{n_t(j)}{t}
        \left(\rho_t(j) - \frac{j}{k} \right)^2 &
\displaystyle
\tilde{C}_t \equiv \sum_{j=0}^k \frac{\tilde{n}_t(j)}{t}
        \left(\tilde{\rho}_t(j) - \frac{j}{k} \right)^2
\end{array}
\end{equation}

Note that $n_t(i) - \tilde{n}_t(i)$, and $\rho_t(i)n_t(i) -
\tilde{\rho}_t(i)\tilde{n}_t(i)$ are both martingales.  This allows
us to approximate $C$ by $\tilde{C}$.

\begin{theorem}[Relating $C$ and $\tilde{C}$] $C_t - \tilde{C}_t
  \rightarrow 0$ almost surely.  
\end{theorem}

{\bf Proof:} The function ${\cal C}(\cdot)$:
\begin{equation}{\cal C}(X_0,X_1,\ldots,X_k,Y_0,Y_1,\ldots,Y_k) =
  \sum_{j=0}^k X_j \left(\frac{Y_j}{X_j} - \frac{j}{k}\right)^2
\end{equation}
allows us to write $C_t$ and $\tilde{C}_t$ as:
\begin{eqnarray*} % C = C(.) and tilde(C) = C(.)
 C_t& =& {\cal C}\!\left(\frac{n_t(0)}{t}, \frac{n_t(1)}{t}, \ldots,
\frac{n_t(k)}{t}, \frac{\rho_t(0)n_t(0)}{t}, \frac{\rho_t(1)n_t(1)}{t},
\ldots, \frac{\rho_t(k)n_t(k)}{t}\right) \\
\tilde{C}_t& =& {\cal C}\!\left(\frac{\tilde{n}_t(0)}{t},
\frac{\tilde{n}_t(1)}{t}, \ldots, \frac{\tilde{n}_t(k)}{t}, 
\frac{\tilde{\rho}_t(0)\tilde{n}_t(0)}{t},
\frac{\tilde{\rho}_t(1)\tilde{n}_t(1)}{t}, \ldots, 
\frac{\tilde{\rho}_t(k)\tilde{n}_t(k)}{t}\right)
\end{eqnarray*}

Since ${\cal C}$ is a continuous function over the compact set $0 \le
Y_i \le X_i \le 1$ (and hence absolutely continuous) all that remains
is showing that the differences in the arguments converge to zero. 

 Since the jumps in the process $n_t(i) - \tilde{n}_t(i)$, and
the process $\rho_t(i)n_t(i) - \tilde{\rho}_t(i)\tilde{n}_t(i)$ are
both bounded by $1$, using martingale theories, we see that
$Var(n_t(i) - \tilde{n}_t(i)) \le t$, and $Var\left(\rho_t(i)n_t(i) -
\tilde{\rho}_t(i)\tilde{n}_t(i)\right) \le t$.  \hfill
$qed$\footnote{The box signifies the end of the proof.}
\label{def:Box}
Define
\begin{equation} 
  \tilde{R}^\epsilon_t \equiv \sum_{i,j:j \neq i} g_\epsilon(R^{ij}_t) 
\end{equation}

where
\begin{eqnarray*}
    R^{ij}_t &\equiv& \sum_{s=1}^t w^i_s \left(\left(X_s -
    \frac{j}{k}\right)^2  -\left(X_s - \frac{i}{k}\right)^2 \right) \\
 & = & 2 n^i_t\frac{i-j}{k}(\rho_i - \frac{i+j}{2k}) \\
& \le & n^i_t (\rho - i/k)^2
  \end{eqnarray*}
  

 \begin{theorem}[Dominating calibration] $\tilde{C}_t \le
    \tilde{R}_t^\alpha/t + 1/(4k^2)$.
 \end{theorem}

Proof:  First look at
\begin{eqnarray} % lots of math!
  \displaystyle
  \tilde{n}_t(i) \left(\tilde{\rho}_t(i) - \frac{i}{k}\right)^2& = &
        R^{ij}_t + \tilde{n}_t(i)  \left(\tilde{\rho}_t(j)
        - \frac{j}{k} \right)^2 \label{eqn:quadratic}\\ 
%
 & \le &  \max_{j} R^{ij}_t + \frac{\tilde{n}_t(i)}{4k^2}
\label{eqn:japproxrho}\\ 
%
  & \le & \sum_j \max\{R^{ij}_t,0\} +
  \frac{\tilde{n}_t(i)}{4k^2} \label{eqn:maxvsmin} \\ 
%
  & \le & \sum_j g_\alpha(R^{ij}_t) +
  \frac{\tilde{n}_t(i)}{4k^2}  \label{eqn:final}
\end{eqnarray} 
Where (\ref{eqn:quadratic}) follows from the definition.  Where
(\ref{eqn:japproxrho}) follows because $\hbox{arg}\min_{j} T^{ij}_t$
will be within $1/(2k)$ of $\rho$.  Where (\ref{eqn:maxvsmin}) follows
because at least one of the terms in the sum equals $T^{ii}_t -
\min_{j} T^{ij}_t$.  Where (\ref{eqn:final}) follows from ? ? {\notes
  what does this refer to?}.  Summing both sides of (\ref{eqn:final})
and noting that $\sum_i \tilde{n}_t(i) = t$ we see that $t\tilde{C}(t)
\le \tilde{R}_t + t/(4k^2)$. \hfill $qed$


\begin{correlary} For all $\epsilon >0$, if $k>\epsilon^{-1/2}$, 
$\alpha > k/\epsilon$, and $t_0 > k^2\alpha/\epsilon$, then for all $t
\ge t_0$  there exists a $w_t$ such that $\tilde{C}_t \le \epsilon$.
Further, there exist a $t_1 > t_0$, such that for all $t \ge t_1$,
$P(C_t < \epsilon) \ge 1 - \epsilon$.
\end{correlary}
\noindent {\bf Proof:}  Obvious. \hfill $qed$

Note: With care, $\epsilon$  is $O(t_0^{-2/7})$.

 \white
        \subsection[Calibration and Bayes]{calibrated models}

\begin{itemize}
        \item Some models are better than others (i.e. some are calibrated)
        \item Calibrated models are less dogmatic --- always fit the data
(at to some extent)
        \item Other models might fit better though
        \item Calibrated models have a personal probability equal to
                frequency 
        \item Useful in motivation Bayes to frequentists.  A
                reasonable sense of probability can now be defined for
                any sequence of number, not just IID.
\end{itemize}
 
        \subsection{Calibration as a scoring rule}


{\notes The following is a note to Jim} 

I'm also bother by using calibration as a direct scoring rule.  I think
the following rather convoluted story is actually a better motivation.
Unfortunate, it is too confusing.

Suppose Forecaster A runs a business forecasting the weather.  The
people she sells this information to evaluate it via some proper
scoring rule.  The difficulty is that if another forecaster comes
along and can achieve a better score, the users will switch forecasters.

Suppose Mr. C, (where C stands for calibration) comes along and wants
to break into this forecasting market.  Mr. C doesn't know anything
about the weather at all.  But, Mr. C can buy a forecast from A.  Now
if Mr. C tries to publish that forecast directly, he will be sued for
intellectual property theft.  If he adds noise to the forecast, it will
be worse, and he won't be able to find any clients.  BUT, if he
modifies the forecast so that it becomes calibrated, he will generate
a new forecast which has a better score than A's, and will be able to
break in the market.

The only statistical defense that A has against this form of attack,
is for her to have a calibrated forecast to begin with.  Thus, she
wants to be calibrated to keep a secondary market from developing.
Said differently, she wants to be calibrated so she won't look
stupid --- if a simple modification of her forecast does better than her
forecast itself, she loses face.

 
        \subsection{A letter from Jim Joyce}

{\notes This section is stolen completely from an email from  James M
  Joyce\footnote{\it jjoyce@umich.edu.}, on 14 Aug 1994.}
\begin{quote}
{\small
I am a little concerned about your use of calibration as a scoring
rule in the motivational example.  Since it is not a fair scoring
rule, the game you describe encourages the forecaster (in at least
some instances) to dishonestly skew his forecasts to attain a better
calibration score.  This leads me to wonder whether the same sorts of
nice asymptotic results obtain when fair scoring rules are used.  (I'd
suspect that they do.)

As to your question, an arbitrary mixture of calibration and
discrimination is not a fair scoring rule.  In general, the only fair
scoring rules in which loss is a function of the difference between p
and $X$ have the form $k(X)(p-X)^2$ where $k$ is a positive real
function.  (Savage, ``Elicitation of Personal Probabilities and
Expectations'', p. 787.)  (One might think of $k$ as measuring the
relative ``importance'' of correctly forecasting $X$.) It is,
unfortunately, not true that arbitrary mixtures of calibra tion and
discrimination have this form.  In fact, the even $(1/2,1/2)$ mixture
is the only one that has this feature; anything else encourages you to
lie about your personal probabilities to improve your score on one of
the two indices at the expense of the other.  For example, consider
the penalty

$$ \hbox{Loss}(p,X) = 0.9 \hbox{Calibration} + 0.1
\hbox{Discrimination }$$

and imagine that the first four forecasts have gone like this

\centerline{\begin{tabular}{r|r}
  Forecast & Outcome\\
\hline\hline \\
 0.6 & 1\\
 0.6 & 1 \\
 0.6 & 0 \\
 0.6 & 0
\end{tabular}}

Your personal probability for rain on Day 5 is one.  However,
announcing this would not be in your interest since you do better,
whatever the weather, by announcing 0.6.  The numbers are:

\centerline{\begin{tabular}{c|c|c}
& Outcome = 0    &      Outcome = 1 \\
\hline\hline
Forecast = 1 & Loss = 0.228   & 0.028 \\
Forecast = 0.6 & Loss = 0.064   & 0.024.
\end{tabular}}

Thus, the loss function described is not a fair scoring rule. One can
cook up examples like this for any weights except (1/2,1/2).  As one
would expect, however, the loss function becomes a better
approximation to a fair scoring rule the closer to (1/2,1/2) one gets.
}
\end{quote}
 

\begin{textHW}
\item[hw:forecastLoss2usualLoss] {\notes This homework is lost.  Where
does it go?}  Assuming a binomial model.  collect data on
  $X_1,\ldots,X_n$.  Consider $\hat{\theta}$ which depends on this
  data.  Suppose this $\hat{\theta}$ was used to predict the
  observations $X_{n+1},X_{n+2},\ldots$, compute the limit of its
  average loss on these forecasts.  Compute the smallest value that
  any forecast could possible obtain on this sequence.  Subtract these
  two losses.  The result should look like a usual loss function.

  I know this works for quadratic error (since I did it in my annals
  paper).  Does it work for log loss?  I'd guess yes, but it should be
  checked. 
\end{textHW}
 
 
    \section{Homework}
%

\begin{HW}
\item \seepage{hw:IRR}
\item \seepage{hw:roots_of_IRR}
\item \seepage{hw:existence}
\item \seepage{hw:non_unique_IRR}
\item \seepage{hw:particular_non_unique_IRR}
\item \seepage{hw:positive_NPV}

\item  \seepage{hw:Kfriends}
\item \seepage{hw:manyFRIENDS}
\item \seepage{hw:infiniteFRIENDS}
\item \seepage{hw:continuiumFRIENDS}

\item \seepage{hw:probLOG}
\item \seepage{hw:proveLOG}
\item \seepage{hw:exact_when_using_randomization}
\item \seepage{hw:worst_equals_bayes}

\item \seepage{hw:k=2:regret=betterthan}

\item \seepage{hw:proper_and_calibrated}


\item \seepage{hw:forecastLoss2usualLoss}

\item Consider a random income stream.  Find an example
  where the $E(IRR)$ is different than the IRR of the expected cash
  flows. Which seems more reasonable?
\item Let $W_t$ be your wealth at time $t$ on a \$1
  investment if you use your current strategy.  Assume $W_t$ is a
  positive random variable.  Suppose you are considering taking
  $\epsilon$ of your initial wealth and investing it in  stock X.
  Assume that $X_0 = 1$ and that $X_t$ is a positive random variable.
  Thus your wealth at time $t$ will now be $(1-\epsilon)W_t +
  \epsilon X_t$.  If there exists an $\epsilon>0$ for which this yields
  a higher IRR than $W$ by itself we say that $X$ is a good investment
  opportunity:
  \begin{equation}
    \label{eqn:good_investment}
    E\left(\log\left((1-\epsilon)W_t + \epsilon X_t\right)\right) >
    E\left(\log\left(W_t\right)\right)
  \end{equation}
  What condition will guarantee that equation \ref{eqn:good_investment} holds?
\item Find examples of:
  \begin{itemize}
  \item $E(X) < 1$, but $X$ is still a good investment.
  \item $E(IRR(X)) > E(IRR(W))$ but X is a bad investment.
  \item etc
  \end{itemize}
\item {\notes For short periods of time, a log
    normal is approximated by a normal.  So, do some IRR for log
    normals over long and short time periods.  The goal here is to set
    up the approximations necessary for the next problem.}
\item Suppose that $W$ and $X$ are both jointly log
  normal.  That is to say that $[\log(W),\log(X)] \sim
  N(\mu,\Sigma)$, with $[\mu_1,\mu_2] =
  [E(\log(W)),E(\log(X))]$, and $\Sigma_{11}$ the variance of
  $\log(W)$, $\Sigma_{22}$ the variance of $\log(X)$ and
  $\Sigma_{12}$ the covariance.  Find the conditions under which which $X$ is a good
  investment --- i.e. equation \ref{eqn:good_investment} holds.
\item Let $X^1,X^2,\ldots,X^k$ be the total value of $k$
  different investment possibilities.  Let $W_0 = \sum_i X_0^i$.  Let
  $R^i = X^i_1/X^i_0$ be the return of the $i$th investment.  Assume
  the $R^i$'s are all jointly log normal.

  {\sloppy \notes Ack!  $R^M$ is not log normal!  Use SDEs? Use normals?}

  \begin{itemize}
  \item Show $R^M = W_1/W_0$ is log normal.  Compute the parameters of
    for its distribution.
  \item Suppose you currently ``own the market.''  That is to say the
    amount of $i$ you own is  proportional to $X_i$.  Compute the
    joint distribution between your $R^M$ and $R^i$.
  \item If $X_i$ were a good investment you would purchase more of it.
    This would drive up the price and it would no longer be a good
    investment.  Write down the condition that $X_i$ and the return on
    the market must satisfy in order for $X_i$ not to be a good nor a bad
    investment.  In other words, satisfy equation
    \ref{eqn:good_investment} exactly.
  \item If the $\hbox{Cov}(\log(X_i),\log(market))$ increases, what can one
    infer about $E(X_i)$?
  \item If $E(X_i)$ is negative, might you still want to hold it in
    your portfolio?  (This type of asset is called a ``hedge.'')
  \item Hint:  This model is called the CAPM in finance.  Read any
    finance book for the answers to the above parts!
  \end{itemize}

\item We saw that the equal weighting of two investments sometimes does
better than either investment itself (example
\ref{exam:equalWEIGHTS} on \pageref{exam:equalWEIGHTS}).  Consider two
investment schemes, A and B.  Find a scheme that will do as well as
$pA + (1-p)B$ for any $p$ in the interval $[0,1]$.

\item {\sloppy \notes Make a homework problem on calibratable.  }

\item Suppose you have two friends named $A$ and $B$.  Each day you
ask one of them to make a forecast of an event for you.  You then take
this forecast as your own.  In other words, $C_i = A_i$ if you ask
friend $A$ at time $i$, and $C_i = B_i$ if you ask friend $B$ at time
$i$.  Our goal is to figure out if you will be calibrated.  Assume
that each of your friends forecasts values which lie in a finite set
$\cal{S}$.  That is to say, $A_i \in \cal{S}$ for all $i$ and likewise
for $B$, where $|\cal{S}|<\infty$.

\begin{enumerate}\item\begin{enumerate}
        \item Suppose the following: $A$ and $B$ don't bother to make
forecasts unless you ask them for a forecast.  Each of them is
calibrated on their own sequence of forecasts.  Prove that you will be
calibrated. (Do not assume you choose which friend to ask randomly.)

\item Using the Strong Law of Large Numbers, and a great deal of care,
  it can be shown that if $A$ is calibrated, but we only ask $A$ for a
  forecast on days that a coin lands heads, then $A$ is calibrated on
  the resulting sub-sequence.  Use the fact for the following problem: 

  Suppose the following: Each day $A$ and $B$ make forecasts and only
  tell you their forecast if you ask for it.  Both $A$ and $B$ are
  calibrated.  You choose which friend to ask by tossing a fair coin.
  Prove you are calibrated.

        \item Suppose the following: Each day $A$ and $B$ make
forecasts and only tell you their forecast if you ask for it.  Both
are in fact calibrated.  Find an example where you choose between
which to forecast in a deterministic fashion and you are not
calibrated.
\end{enumerate} \end{enumerate}

\item Consider a sequence of forecast $\widehat{p}_i$ of events $X_i$.
  The quadratic difference between these is called the Brier score:

$$\hbox{Brier score} = B_N(\widehat{p}) =  \sum_{n=1}^{N} \left.
        \left( X_n - \widehat{p}_n \right)^2\right/N,$$
\label{def:brier_score}

\noindent (See Brier, G. (1950).  ``Verification of forecasts expressed
in terms of probability.'' {\it Monthly Weather Review} {\bf 78} 1-3.
Wasn't it easy to get published back in 1950?  All Brier did in his
paper is give this definition!)  Define resolution as follows:

$$\hbox{Resolution} = R_N(p) =  \sum_{n=1}^{N} \left.
        \left( \rho_N(\widehat{p}_n) - \bar{X}_N \right)^2\right/N,$$

\noindent where $\bar{X}_N$ is the average number of successes up to
time $N$ and $\rho_N()$ is the calibration of $\widehat{p}$ up to time
$N$.  Recall the calibration score is $C_N =\sum_{n=1}^{N} \left.
\left( \rho_N(\widehat{p}_n) - \widehat{p}_n\right)^2\right/N,$  Define
refinement as: 

$$ \hbox{Refinement} = F_N(p) =  \sum_{n=1}^{N} \left.
        \left( \rho_N(\widehat{p}_n) - X_n \right)^2\right/N,$$

\noindent In the following, $g_N()$ will be a function of $[0,1]$ into
$[0,1]$.  We will call $\widehat{g}_n = g_N(\widehat{p}_n)$.

\begin{enumerate}\item\begin{enumerate}
        \item  Prove that the  $B_N = C_N + F_N = C_N + SST/N - R_N$,
                where $SST$ is the sum of squares total. 

        \item Prove that any $\widehat{g}_N$ has larger or equal
                refinement and  a smaller or equal resolution than
                that of $\widehat{p}$ itself.   

        \item Prove that $g_N() = \rho_N()$ has a calibration score of
                zero.  Further, prove that its refinement is equal to
                the refinement of $\widehat{p}$.

        \item Prove that $g_N()=\rho_N()$ has the lowest Brier
                score at time $N$ of any such $\widehat{g}_N$.

        \item Prove that if for all sequences of functions $g_N$, 

                $$\liminf\limits_{N \to \infty} \left( B_N(g_N(f_n)) -
                        B_N(f_n)\right) \ge 0,$$ 

                if and only if $f_n$ is calibrated.  
\end{enumerate}\end{enumerate}

\item {\bf Calibration and proper scoring rules}

       Define proper scoring rule $L$.  Assume L is bounded.

       Show $\limsup_{N \to \infty} \sum_{n=1}^{N} \left(L(g(f_n)) -
L(f_n)\right)/N$ is zero for calibrated forecasts of $f_n$ and any
function $g$. 

       Prove that if $\limsup loss(g(f_n)) - loss(f_n)$ doesn't
converge to zero, then $f_n$ is not calibrated.

       Find a way of calibrating a forecast 

       {\bf (Bonus)} Prove that if for all sequences of functions $g_N$, 

$$\limsup\limits_{N \to \infty} \sum_{n=1}^{N} 
       \left(L(g_N(f_n)) - L(f_n)\right)^2 = 0$$,

then $f_n$ is calibrated. 

 (Note the above needs to work with sequences.)
 
\begin{definition}[Better than] Consider two forecasting rules A and
B.  Suppose they both make forecasts of a data sequence $X_i$ over
time.  Use a utility function $U(\widehat{\theta},X)$ to measure how
good the forecasts are.  We say that A is better than B if for all
$\epsilon > 0$, there exist a $T_\epsilon$ such that for all $T \ge
T_\epsilon$, and for all possible sequences of $X_i$,
\begin{equation}
\sum_{t=1}^T \left(U(A,i) - U(B,i)\right)/T \ge -\epsilon.
\label{eqn:better}
\end{equation}
\end{definition}

\item  Recall the lottery payoff function of \ref{eqn:lottery} on
page \pageref{eqn:lottery}.  Show that for any data sequence, a calibrated
forecast will be better than any fixed forecast.
\item Show that if A is better than B for two utilities $U_1$ and
$U_2$ then it is better than B for the utility which is the sum of the
two utilities.
\item Show that for any finite sum of lottery functions,  a
calibrated forecast is better than any constant forecast.
\begin{definition}[Honest forecast]   A is an
  honest forecast if for all bounded honesty provoking payoff
  functions, A is better than any constant forecast.
\end{definition}
\item Prove that calibration implies honesty.  In other words, show
that for any bounded honesty provoking payoff function, a calibrated
forecast is better than any constant forecast. (You might find
reviewing the homework problem \ref{hw:allHonest} helpful on
\pageref{hw:allHonest}.) 

\item Define $F_A(p) \equiv \sum_{t=1}^T I_{A_t \le p}$ and $G_A(p)
\equiv \sum_{t=1}^T X_t \times I_{A_t \le p} $.  
\begin{itemize}
  \item Show that if A is calibrated then a plot of $F_A$ vs $G_A$ is
convex. 
\item Show that at each point $(N_A(p),C_A(p))$ on the curve, the
tangent line has a slope of $p$.
\item Show that if A is an honest forecast than both the point $0,0$
and the point $N,N$ is above the tangent line for all points
$(N_A(p),C_A(p))$. 
\item Prove or provide counter example: honesty implies calibration.
\end{itemize}
\begin{definition}[Honestly better]   A is honestly better
than B if for all bounded honesty provoking payoff functions, A is
better than B.
\end{definition}
\item Given two forecasts A and B, find a combination C which is
honestly better than A and honestly better than B.
\item Generalize the above to a countable number of forecasts.
\item \begin{enumerate} \item Find the function $H(x)$ which has the
smallest value of $H(0)$ subject to the conditions that:
\begin{eqnarray*}
H(x) & \ge& 0 \\
H(x) & \ge& x \\
H''(x) & \le & 1
\end{eqnarray*}
\item Let $G_\epsilon(x) = h(\epsilon x)/\epsilon$.  Define
$\hat{R}_\epsilon = \sum_{i,j} G_\epsilon(R^{i \rightarrow j})$.  Show
that $R^{i \rightarrow j} \le \hat{R}_\epsilon$ for all $i,j$ and
$\epsilon > 0$. 
\item Show that for all $\delta >0$ there exists a $T$ and there
  exists an 
$\epsilon > 0 $ such that for all $t > T$ there is a $w$ such that
$\hat{R}_\epsilon \le \delta$.
      \end{enumerate}
\end{HW}
 
 
 
\end{document}
