\documentclass[14pt]{extarticle}
\renewcommand{\baselinestretch}{1.2}
\usepackage{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% defining some commands that I like:
%
%                       \dpf   \note    \DOC    \TESTING
%
\usepackage[usenames]{color}\definecolor{mypurple}{rgb}{.3,0,.5}
\newcommand{\dpf}[1]{\noindent{\textcolor{mypurple}{\{{\bf dpf:} \em #1\}}}}
\newcommand{\note}[1]{\noindent{\textcolor{red}{\{{\bf NOTE:} \em #1\}}}}
\SweaveOpts{prefix.string=.figures/linguistics}

\begin{document}

\author{Dean Foster}
\title{Class: Lingustics introduction}
\maketitle
(\href{class_lingustics.pdf}{pdf version})

\section*{Admistrivia}

\begin{itemize}
\item R-weave introduction after class
\end{itemize}


(emailed from bob):

Linguists Out Men Impersonating Women On Twitter
Posted by timothy on Thursday July 28, @06:58PM 
from the anything-that-can-be-described-can-be-faked dept.
Hugh Pickens writes
``Remember when the Gay Girl in Damascus revealed himself as a
middle-aged man from Georgia? On a platform like Twitter, which
doesn't ask for much biographical information, it's easy (and fun!) to
take on a fake persona but now linguistic researchers have developed
an algorithm that can predict the gender of a tweeter based solely on
the 140 characters they choose to tweet. The research is based on the
idea that women use language differently than men. 'The mere fact of a
tweet containing an exclamation mark or a smiley face meant that odds
were a woman was tweeting, for instance,' reports David Zax. Other
research corroborates these findings, finding that women tend to use
emoticons, abbreviations, repeated letters and expressions of
affection more than men and linguists have also developed a list of
gender-skewed wordsused more often by women including love, ha-ha,
cute, omg, yay, hahaha, happy, girl, hair, lol, hubby, and
chocolate. Remarkably, even when only provided with one tweet, the
program could correctly identify gender 65.9% of the
                                % time. (PDF). Depending on how
                                % successful the program is proven to
                                % be, it could be used for
                                % ad-targeting, or for
                                % socio-linguistic research."




\section*{Story time: Colorless green ideas sleep furiously.}

\subsection*{Noam Chomsky}
\begin{itemize}
\item Studied at U Penn
\item Works at MIT
\item writes more than anyone else in the world!  
\item Most cited author (alive) (Only 7th when he dies: Marx, Lenin,
 Shakespere, Aristotal, God, Plato, Freud)
\end{itemize}

\subsection*{Mitch Marcus}
\begin{itemize}
\item Studied at MIT
\item Works at Penn
\item Almost never writes papers--but still was president of
Lingustics society?  How?  His students!
\end{itemize}

\subsection*{The two aproaches}
\begin{itemize}
\item Chomsky builds a huge computer program to process language
\item Marcus builds a trivial statistical model
\item Chomsky ruled lingustics for 40 years (1950 - 1990)
\item Variations of Marcus' statistical models now are used:
\begin{itemize}
\item IBM's speech understanding system
\item Best parsers in the world
\item my current research
\end{itemize}
\end{itemize}

\section*{Todays Topic: Lingustic data analysis}

\subsection*{Classical statistics is numbers}
\begin{itemize}
\item Originally we coded Men = 1, women = 0
\item Then we coded Men = ``M'' and women = ``W''
\item more recently we started: listing company = ``Evil empire'', owner = ``Bill Gates''
\item More and more lingustic like data
\item We can have whole paragraphs in data files now
\begin{itemize}
\item Medical records have human diagnostics attached
\item Billing records have transripts of previous exchanges
\item Purchase patterns have comments on who purchased what
\item reputation isn't just measured in ``stars'' but also in short sentences
\end{itemize}
\end{itemize}

\subsection*{Sample problems}
\begin{itemize}
\item Big problems
\begin{itemize}
\item machine translation
\item Google queries
\item machine reading (i.e. language understanding)
\item author identification
\end{itemize}
\item Applications in science:
\begin{itemize}
\item finding related articles on a topic (MEDLINE)
\end{itemize}
\item Business applications:
\begin{itemize}
\item sentiment analysis 
\item email sorting (filter email to billing vs. sales)
\end{itemize}
\end{itemize}

\subsection*{How to get started}

\begin{itemize}
\item What can we do with statistics?
\begin{itemize}
\item we count
\item we average
\item We build probabilistic models
\end{itemize}
\item how can this help?
\end{itemize}

\subsection*{Example: author identification}

(Frederick Mosteller and David L. Wallace. Inference and Disputed
Authorship: The Federalist. Addison-Wesley, Reading, Mass., 1964.)

\begin{itemize}
\item Who wrote the federalist papers? (85 in all)
\item Offical writer: Publius
\item Most likely though:
\begin{itemize}
\item Hamilton:
\begin{quotation}
AFTER an unequivocal experience of the inefficacy of the
 subsisting federal government, you are called upon to deliberate on
 a new Constitution for the United States of America. The subject
 speaks its own importance; comprehending in its consequences
 nothing less than the existence of the UNION, the safety and welfare
 of the parts of which it is composed, the fate of an empire in many
 respects the most interesting in the world. It has been frequently
 remarked that it seems to have been reserved to the people of this
 country, by their conduct and example, to decide the important
 question, whether societies of men are really capable or not of
 establishing good government from reflection and choice, or whether
 they are forever destined to depend for their political
 constitutions on accident and force. If there be any truth in the
 remark, the crisis at which we are arrived may with propriety be
 regarded as the era in which that decision is to be made; and a
 wrong election of the part we shall act may, in this view, deserve
 to be considered as the general misfortune of mankind.
\end{quotation}
\item Madison
\begin{quotation}
AMONG the numerous advantages promised by a wellconstructed
 Union, none deserves to be more accurately developed than its
 tendency to break and control the violence of faction. The friend
 of popular governments never finds himself so much alarmed for their
 character and fate, as when he contemplates their propensity to this
 dangerous vice. He will not fail, therefore, to set a due value on
 any plan which, without violating the principles to which he is
 attached, provides a proper cure for it. The instability,
 injustice, and confusion introduced into the public councils, have,
 in truth, been the mortal diseases under which popular governments
 have everywhere perished; as they continue to be the favorite and
 fruitful topics from which the adversaries to liberty derive their
 most specious declamations. The valuable improvements made by the
 American constitutions on the popular models, both ancient and
 modern, cannot certainly be too much admired; but it would be an
 unwarrantable partiality, to contend that they have as effectually
 obviated the danger on this side, as was wished and expected.
 Complaints are everywhere heard from our most considerate and
 virtuous citizens, equally the friends of public and private faith,
 and of public and personal liberty, that our governments are too
 unstable, that the public good is disregarded in the conflicts of
 rival parties, and that measures are too often decided, not
 according to the rules of justice and the rights of the minor party,
 but by the superior force of an interested and overbearing majority.
 However anxiously we may wish that these complaints had no
 foundation, the evidence, of known facts will not permit us to deny
 that they are in some degree true. It will be found, indeed, on a
 candid review of our situation, that some of the distresses under
 which we labor have been erroneously charged on the operation of our
 governments; but it will be found, at the same time, that other
 causes will not alone account for many of our heaviest misfortunes;
 and, particularly, for that prevailing and increasing distrust of
 public engagements, and alarm for private rights, which are echoed
 from one end of the continent to the other. These must be chiefly,
 if not wholly, effects of the unsteadiness and injustice with which
 a factious spirit has tainted our public administrations.
\end{quotation}
\item Jay
\begin{quote}
WHEN the people of America reflect that they are now called upon
 to decide a question, which, in its consequences, must prove one of
 the most important that ever engaged their attention, the propriety
 of their taking a very comprehensive, as well as a very serious,
 view of it, will be evident.
\end{quote}
\end{itemize}
\item Can't go by the ``ideas'' since many were new and so couldn't be
realated to the individual authors
\item Use of grammer and syntax! (Or more accurately word counts)
\item Look at the distribution of words:
\begin{itemize}
\item Can't use ``freedom'' and ``government'' since the different
articles will use them differently based on topic
\item Can use ``the'', ``this'', ``that''!  
\end{itemize}
\end{itemize}

\subsection*{Methodolgy}

Let's replicate Mosteller and Wallace.  First we need to read in all
the words:
<<reading in data>>=

federal <- read.csv("federalist.csv",header=TRUE)

@ 
\note{Notice: I spent an hour preprocessing this in an editor to make it a
bit easier for R to deal with. So for example I added titles for the 3
columns.}

<<>>=

names(federal)

@ 

Now let's use R to split this into ``words.''  There are smarter ways
of doing this, but for now we will do the most trivial:

<<>>=

federal$words <- strsplit(as.character(federal$text)," ")

federal$num.words <- c(lapply(as.character(federal$words),length),recursive=TRUE)
federal$num.char <- c(lapply(as.character(federal$text),nchar),recursive=TRUE)

@ 

Now a messy way of generating a variable to do the regression on.
 We know some of the authors but we don't know them all:

<<>>=

codes <- c(NA, "HAM", "mad")
federal$training <- codes[1 + (federal$author == "HAMILTON") + 2 * (federal$author == "MADISON")]
federal$training01 <- (federal$training == "HAM")

@ 
Now let's check that we have done something sensiable:

<<results=tex,echo=FALSE>>=

r <-as.matrix(federal[c(1,2,3,6,10,15:19,45,55),c("author","training","training01")])
require("xtable")
print(xtable(r),NA.string="NA",floating=F,size="small")

@ 

\\


Ok, finally we can try a trivial regression.  If we think like a
statistician, about all we can do is count the number of charecters /
words in the document.  This isn't very predictive:

\begin{center}
<<results=tex,echo=FALSE>>=

regr <- lm(training01 ~ num.words+num.char,data=federal)
print(xtable(summary(regr)),floating=F,size="small")

@ 
\end{center}
Even knowing the R-squared is $\Sexpr{round(summary(regr)[["r.squared"]],2)}$
doesn't help.  
\subsection*{Thinking like a linguist}

So let's think like a linguistist.  Content words (``freedom'',
``liberty'', etc) are related to content.  But function words
(``the'', ``an'', etc) are related to style.  So let's try the
frequency of these words to predict authorship.

First we grab all the words together in one place:
<<>>=
all.words = c(federal$words,recursive=TRUE)
@ 
Now let's count them.
<<>>=
counts = table(all.words)
@ 
Finally, let's look at the most common such words
<<>>=
sort(counts[which(counts > 1600)],decreasing=TRUE)
@ 

Putting this in a pretty latex table, we see the most common words
are:
\begin{center}
<<results=tex,echo=FALSE>>=
pretty <- data.frame(sort(counts[which(counts > 1600)]))
names(pretty) <- c("count")
print(xtable(pretty),floating=F,size="small")
@ 
\end{center}

Now for some R magic.  Let's make a function that will identify whether a words is "the" or not.

<<>>=
is.the <- function(t){sum(t == "the")}
is.the(3)
is.the("three")
is.the("the")
@ 

We can use this function now to pick out all the words that are
infact ``the'' in the text.  Because we are statisticians, we
normalize by the number of words in the document.

<<keep.source=TRUE>>=
federal$freq.the <- c(lapply(federal$words,is.the),
                             recursive=TRUE
                      ) / federal$num.words
@ 

We can repeat this for the other most common words.

  \note{We don't actually have to give these new functions names.
  This makes the file look a bit prettier--albeit a bit more
 confusing.}

<<echo=FALSE>>=
federal$freq.c   <- c(lapply(federal$words,function(t){sum(t == "<c>")}),recursive=TRUE) / federal$num.words
federal$freq.of  <- c(lapply(federal$words,function(t){sum(t == "of")}),recursive=TRUE) / federal$num.words
federal$freq.to  <- c(lapply(federal$words,function(t){sum(t == "to")}),recursive=TRUE) / federal$num.words
federal$freq.p   <- c(lapply(federal$words,function(t){sum(t == "<p>")}),recursive=TRUE) / federal$num.words
federal$freq.and <- c(lapply(federal$words,function(t){sum(t == "and")}),recursive=TRUE) / federal$num.words
federal$freq.in  <- c(lapply(federal$words,function(t){sum(t == "in")}),recursive=TRUE) / federal$num.words
federal$freq.a   <- c(lapply(federal$words,function(t){sum(t == "a")}),recursive=TRUE) / federal$num.words
federal$freq.be  <- c(lapply(federal$words,function(t){sum(t == "be")}),recursive=TRUE) / federal$num.words
federal$freq.that<- c(lapply(federal$words,function(t){sum(t == "that")}),recursive=TRUE) / federal$num.words
federal$freq.is  <- c(lapply(federal$words,function(t){sum(t == "is")}),recursive=TRUE) / federal$num.words
federal$freq.which<- c(lapply(federal$words,function(t){sum(t == "which")}),recursive=TRUE) / federal$num.words
federal$freq.it  <- c(lapply(federal$words,function(t){sum(t == "it")}),recursive=TRUE) / federal$num.words
federal$freq.by  <- c(lapply(federal$words,function(t){sum(t == "by")}),recursive=TRUE) / federal$num.words
@ 

Running a regression on the 5 most common symbols we see that the
function words are useful, but commas and sentence boundaries
aren't useful.

\begin{center}
<<results=tex,echo=FALSE>>=
regr <- lm(training01 ~ num.words + 
                        freq.the  + freq.c + freq.of + freq.to + freq.p,
                        data=federal)
print(xtable(summary(regr)),floating=F,size="small")
@ 
\end{center}

The R-squared has improved to
$\Sexpr{round(summary(regr)$r.squared,2)}$. We seem to be on a
roll.  Let's get rid of the punctuation and add a few more words:
\begin{center}
<<results=tex,echo=FALSE>>=
regr <- lm(training01 ~ freq.the  + freq.of + freq.to + freq.and + freq.in + freq.a + freq.be + freq.that + freq.is + freq.which + freq.it + freq.by,
                        data=federal)
print(xtable(summary(regr)),floating=F,size="small")
@ 
\end{center}
Ah, now we have a respectable R-squared of $\Sexpr{round(summary(regr)$r.squared,2)}$.  Let's see how well it
predicts.  First let's come up with some color codes for the various authors:
<<>>=
colors <- c("red","white","black","gray","blue")[federal$author]
@ 
This relies on the fact that R stores these as {\em level}.  So with a
bit of futzing we can figure out that 1 = Hamilton, etc.  So now we
can make our predictions for each article.


<<preds,include=FALSE,fig=TRUE,echo=FALSE>>=
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)
@ 
\begin{center}
\includegraphics[width=3in]{.figures/linguistics-preds}
\end{center}

\newpage
\section*{Other models}

<<results=tex,echo=FALSE>>=
regr <- lm(training01 ~ freq.the  + freq.of + freq.to + freq.and + freq.by,
                        data=federal)
print(xtable(summary(regr)),floating=F,size="small")
@ 

\\

With fewer variables the R-squared is a bit lower:
$\Sexpr{round(summary(regr)$r.squared,2)}$.

   Adding alot of variables improves it:
   
<<results=tex,echo=FALSE>>=
regr <- lm(training01 ~ freq.the  + freq.of + freq.to + freq.and + freq.by +
           I(sqrt(freq.the))  + I(sqrt(freq.of)) + I(sqrt(freq.to)) + I(sqrt(freq.and)) + I(sqrt(freq.by))
+ number + I(number^2),
                        data=federal)
print(xtable(summary(regr)),floating=F,size="small")
@ 

\\

R-squared is $\Sexpr{round(summary(regr)$r.squared,2)}$.  Let's see how well it
predicts. 

<<fig=TRUE,echo=FALSE>>=
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
abline(.5,0)
@ 
\end{document}
