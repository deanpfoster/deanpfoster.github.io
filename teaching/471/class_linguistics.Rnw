\documentclass[14pt]{extarticle}
\renewcommand{\baselinestretch}{1.2}
\usepackage{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% defining some commands that I like:
%
%                       \dpf   \note    \DOC    \TESTING
%
\usepackage[usenames]{color}\definecolor{mypurple}{rgb}{.3,0,.5}
\newcommand{\dpf}[1]{\noindent{\textcolor{mypurple}{\{{\bf dpf:} \em #1\}}}}
\newcommand{\note}[1]{\noindent{\textcolor{red}{\{{\bf NOTE:} \em #1\}}}}

\begin{document}


\title{Class: Lingustics introduction}
\maketitle
(\href{class_lingustics.pdf}{pdf version})

\section*{Admistrivia}

\begin{itemize}
\item R-weave introduction after class
\end{itemize}

\section*{Story time: Colorless green ideas sleep furiously.}

\subsection*{Noam Chomsky}
\begin{itemize}
\item Studied at U Penn
\item Works at MIT
\item writes more than anyone else in the world!  
\item Most cited author (alive) (Only 7th when he dies: Marx, Lenin,
 Shakespere, Aristotal, God, Plato, Freud)
\end{itemize}

\subsection*{Mitch Marcus}
\begin{itemize}
\item Studied at MIT
\item Works at Penn
\item Almost never writes papers--but still was president of
Lingustics society?  How?  His students!
\end{itemize}

\subsection*{The two aproaches}
\begin{itemize}
\item Chomsky builds a huge computer program to process language
\item Marcus builds a trivial statistical model
\item Chomsky ruled lingustics for 40 years (1950 - 1990)
\item Variations of Marcus' statistical models now are used:
\begin{itemize}
\item IBM's speech understanding system
\item Best parsers in the world
\item my current research
\end{itemize}
\end{itemize}

\section*{Todays Topic: Lingustic data analysis}

\subsection*{Classical statistics is numbers}
\begin{itemize}
\item Originally we coded Men = 1, women = 0
\item Then we coded Men = ``M'' and women = ``W''
\item more recently we started: listing company = ``Evil empire'', owner = ``Bill Gates''
\item More and more lingustic like data
\item We can have whole paragraphs in data files now
\begin{itemize}
\item Medical records have human diagnostics attached
\item Billing records have transripts of previous exchanges
\item Purchase patterns have comments on who purchased what
\item reputation isn't just measured in ``stars'' but also in short sentences
\end{itemize}
\end{itemize}

\subsection*{Sample problems}
\begin{itemize}
\item Big problems
\begin{itemize}
\item machine translation
\item Google queries
\item machine reading (i.e. language understanding)
\item author identification
\end{itemize}
\item Applications in science:
\begin{itemize}
\item finding related articles on a topic (MEDLINE)
\end{itemize}
\item Business applications:
\begin{itemize}
\item sentiment analysis 
\item email sorting (filter email to billing vs. sales)
\end{itemize}
\end{itemize}

\subsection*{How to get started}

\begin{itemize}
\item What can we do with statistics?
\begin{itemize}
\item we count
\item we average
\item We build probabilistic models
\end{itemize}
\item how can this help?
\end{itemize}

\subsection*{Example: author identification}

(Frederick Mosteller and David L. Wallace. Inference and Disputed
Authorship: The Federalist. Addison-Wesley, Reading, Mass., 1964.)

\begin{itemize}
\item Who wrote the federalist papers? (85 in all)
\item Offical write: Publius
\item Most likely though:
\begin{itemize}
\item Hamilton
\item Madison
\item Jay
\end{itemize}
\item Can't go by the ``ideas'' since many were new and so couldn't be
realated to the individual authors
\item Use of grammer and syntax! (Or more accurately word counts)
\item Look at the distribution of words:
\begin{itemize}
\item Can't use ``freedom'' and ``government'' since the different
articles will use them differently based on topic
\item Can use ``the'', ``this'', ``that''!  
\end{itemize}
\end{itemize}

\subsection*{Methodolgy}

Let's replicate Mosteller and Wallace.  First we need to read in all
the words:
<<reading in data>>=
federal <- read.csv("federalist.csv",header=TRUE)
@ 
\note{Notice: I spent an hour preprocessing this in an editor to make it a
bit easier for R to deal with. So for example I put titles for the 3
columns.}
<<>>=
names(federal)
@ 
Now let's use R to split this into ``words.''  There are smarter ways
of doing this, but for now we will do the most trivial:
<<>>=
federal$words <- strsplit(as.character(federal$text)," ")
federal$num.words <- c(lapply(federal$words,length),recursive=TRUE)
federal$num.char <- c(lapply(federal$text,nchar),recursive=TRUE)
@ 
To see what we have done, let's look at the 10th row:
<<>>=
federal$number[[10]]
federal$author[[10]]
federal$num.words[[10]]
federal$words[[10]][1:40]
@ 

Now a messy way of generating a variable to do the regression on.
  (I'm sure Sivan will have a better way.)  We know some of the
 authors but we don't konw them all:

<<>>=
codes <- c(NA, "HAM", "mad")
federal$training <- codes[1 + (federal$author == "HAMILTON") + 2 * (federal$author == "MADISON")]
federal$training01 <- (federal$training == "HAM")
colors <- c("red","white","black","gray","blue")[federal$author]
@ 
Now let's check that we have done something sensiable
<<>>=
federal[1:30,c("author","training","training01")]
@ 

Ok, finally we can try a trivial regression.  If we think like a
statistician, about all we can do is count the number of charecters /
words in the document.  THis isn't very predictive:
<<results=tex>>=
require("xtable")
regr <- lm(training01 ~ num.words+num.char,data=federal)
xtable(summary(regr))
@ 
Even knowing the R-squared is $\Sexpr{round(summary(regr)$r.squared,2)}$
doesn't help.  So let's think like a linguistist:

<<>>=
all.words = c(federal$words,recursive=TRUE)
counts = table(all.words)
sort(counts[which(counts > 1600)])
@ 

So the most common words are:
<<results=tex,echo=FALSE>>=
pretty <- data.frame(sort(counts[which(counts > 1600)]))
names(pretty) <- c("count")
xtable(pretty)
@ 

<<>>=
federal$freq.the <- c(lapply(federal$words,function(t){sum(t == "the")}),recursive=TRUE) / federal$num.words
@ 

We can repeat this for the other most common words.

<<echo=FALSE>>=
federal$freq.c   <- c(lapply(federal$words,function(t){sum(t == "<c>")}),recursive=TRUE) / federal$num.words
federal$freq.of  <- c(lapply(federal$words,function(t){sum(t == "of")}),recursive=TRUE) / federal$num.words
federal$freq.to  <- c(lapply(federal$words,function(t){sum(t == "to")}),recursive=TRUE) / federal$num.words
federal$freq.p   <- c(lapply(federal$words,function(t){sum(t == "<p>")}),recursive=TRUE) / federal$num.words
federal$freq.and <- c(lapply(federal$words,function(t){sum(t == "and")}),recursive=TRUE) / federal$num.words
federal$freq.in  <- c(lapply(federal$words,function(t){sum(t == "in")}),recursive=TRUE) / federal$num.words
federal$freq.a   <- c(lapply(federal$words,function(t){sum(t == "a")}),recursive=TRUE) / federal$num.words
federal$freq.be  <- c(lapply(federal$words,function(t){sum(t == "be")}),recursive=TRUE) / federal$num.words
@ 

<<results=tex>>=
regr <- lm(training01 ~ num.words + 
                        freq.the  + freq.c + freq.of + freq.to + freq.p,
                        data=federal)
xtable(summary(regr))
@ 
The R-squared has improved to
$\Sexpr{round(summary(regr)$r.squared,2)}$. We seem to be on a
roll--let's try a few more:
<<results=tex>>=
regr <- lm(training01 ~ num.words + 
                        freq.the  + freq.of + freq.to + freq.and + freq.in + freq.a + freq.be,
                        data=federal)
xtable(summary(regr))
@ 

Ah, now we hvae a respectable R-squared:
$\Sexpr{round(summary(regr)$r.squared,2)}$.  Let's see how well it
predicts. 
<<fig=TRUE>>=
predictions <- predict(regr,newdata=federal)
plot(predictions ~ federal$number , pch=16 , col=colors)
@ 
\end{document}
