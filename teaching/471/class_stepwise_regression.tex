\documentclass{article}

\usepackage{hyperref}

\usepackage[usenames]{color}


\begin{document}
\title{Class: Stepwise regression}
\maketitle

(\href{class_stepwise_regression.pdf}{pdf version})

\section{suggested readings from the web}
\begin{itemize}
\item \href{http://en.wikipedia.org/wiki/Model_selection}{Wiki on
model selection}
\item \href{http://en.wikipedia.org/wiki/Bonferroni_correction}{Wiki
on Bonferroni}
\item A fairly readable research paper with Bob Stine:
\href{http://www-stat.wharton.upenn.edu/\~stine/research/honests2.pdf}{Honest
RMSE}
\end{itemize}

\section{Status so far}

The model
\begin{displaymath}
Y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \cdots + \beta_p
x_p + \epsilon
\end{displaymath}
where $\epsilon$ is $N(0,\sigma^2)$.  

\textcolor{red}{{Notation: here the subscripts identify which variable we are
 talking about not which observation we are talking about.  If you
 need to do both, then you have to write it as: $Y_i = \alpha +
 \beta_1 x_{i,1} + \beta_2 x_{i,2} + \beta_3 x_{i,3} + \cdots + \beta_p
 x_{i,p} + \epsilon_i$.  But this is cumbersome.}}

\begin{itemize}
\item The problem is $p$ can be very large
\item So fitting all these variables will lead to a bad estimate of $Y$
\item Solution: only use some of the variables
\end{itemize}

\section{Software: JMP}

\begin{itemize}
\item Use ``fit model'' and change the ``personality'' to stepwise regression.
\item Add as many X's as you have!
\item To make even more X's you can
\begin{itemize}
\item Use the macros for response surface (this works for continuous
variables only).  To do this, highlight some variables in the left
hand column and hit macro-response surface.
\item Use the macros factorial for discrete variables.  If you don't
want exponentially many variables,
\item Cross variables: (Highlight some X's already added and then
highlight some more from the left hand column and hit cross)
\item Do all three of the above!  (Response surface your continuous
ones, factorial your discrete ones, and cross your continuous with
your discrete.)
\end{itemize}
\item Run your stepwise regression 
\begin{itemize}
\item Change the probability to enter to something like $.05/p$, where $p$
is the number of variables.
\item Change the ``Rules'' to ``no rules''.  (This makes each item act
individually of all other items.  So $X*Z$ is unrelated to either $X$
or to $Z$.)
\item Hit ``step'' to add one variable--repeat to add more variables
\item Hit ``go'' to keep adding variables until none satisfy your
p-value criterion.
\end{itemize}
\item Make a model: You won't get any diagnostics out of stepwise
 regression.  It is truely only a fitting platform.  So to get a
better view (residuals, leverage plots, etc) use the make model
button. 
\end{itemize}

\section{Alternative selection rules}
\begin{itemize}
\item Maximize the R-squared:
\begin{itemize}
\item Problem: Will add all the variables
\item Insample fit is perfect
\item Out of sample fit is often twice as bad as Y's themselves of $p$
is large
\end{itemize}
\item Maximize the adjusted R-sq'd
\begin{itemize}
\item Sounds better than using the R-sq'd--but the adjustment is not
 for selection, only for degrees of freedom (I.e. Sum/n vs Sum/(n-1)
 issues.)
\item Same thing as using the minimized RMSE (which has the same
adjustment aplied to it).
\item So, will stil overfit.
\item For a more fully adjusted R squared see my paper with Bob Stine:
(\href{http://www-stat.wharton.upenn.edu/\~stine/research/honests2.pdf}{Honest RMSE})
\end{itemize}
\item AIC / C-P / BIC / neumerous others
\begin{itemize}
\item Sometimes over fit
\item Sometimes work OK
\end{itemize}
\item Bonferroni
\begin{itemize}
\item Number of extranious variables is limited to .05 in expectation
\item Hence will not over fit.
\item For some nasty mathematics justifing this approach, see my
\href{\~foster/research/risk_inflation.pdf}{paper} with Edward George.
\end{itemize}
\end{itemize}


\end{document}
