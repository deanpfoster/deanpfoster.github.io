\documentclass[11pt]{article}
\usepackage{hyperref}
\usepackage{enumerate}

\begin{document}
\section*{Assignment 4: PCA's and lingustics}

% (\href{assignment4.pdf}{pdf version})  the html is unreadable

\begin{enumerate}

\item In class, we used PCA's on words to identify the authorship of
the federalist papers.  In this problem you will have compute some
PCA's for pieces of the market.  We will first have to download the data.

Here is your mission.
\begin{enumerate}[CRSP step 1:]
\item Log onto
\href{http://wrds.wharton.upenn.edu/}{http://wrds.wharton.upenn.edu/},
the Wharton data server.
\item Go to the CRSP section. (This is data from Chicago).
\item Read the blurb, but don't get too distracted.  We want the menu
on the left here.  So pick the induces and deciles link.
\item Now follow the monthly link to the actual data manufacturing page
\item Grab data starting from 1925, grab all 10 deciles.  Tic the box
to generate both returns and values.  Grab the
value weighted return (with dividends).  Also grab the SP500 returns.
Finally tell it to generate a csv file.
\item Submit your request and you are off and running.  It should read
easily into R.
\item But while you are here, switch to the ``Treasury and inflation''
menu and save some 30 day t-bills and inflation.
\item Now merge these two data set together.  Check that the dates line up.
\item Now correct each of the 10 deciles to reflect the 30day tbill.
(I.e. create 10 new columns for which you have subtracted out the tbill.)
\end{enumerate}
Now that you have your data in a file, let's start analysing it.  (If
you have problems with the above steps, send me an email and I can
either provide you with help, or if necessary the actual data file.)
\begin{enumerate}
\item First confirm that this makes sense.  We know that the top
deciles should look a lot like the SP500 since they are the 500 biggest
stocks.  The question is, are the big stocks in Cap1ret or in
Cap10ret?  So make a plot of both of these vs the SP500 returns.
Which seems to match the SP500 better?  Read the documentation and
confirm these are the big stocks.
\item Now make a scatter plot matrix of all 10 deciles.  They are all
highly correlated.  
\item Ask R to compute the principle components (prcomp).  Do a
 ``plot'' of this object to see what the authors thought made pretty
 pictures.  Determine the first principle component.  Is this
 something you could purchase?\footnote{THe problem we are woried
about here is whether the prcomp command subtracts out the mean.  If
it does, then the components it creates all have means of zero.  These
aren't tradable assests since they are a linear combination of deciles
{\em plus a constant.}  If we could get a constant return that was
different than the risk free rate we would have what is called a pure
arbitrage.  It is the Nirvana of finance.  So. is the mean fixed to be
zer or not?}  Play around with the ``center'' and ``scale'' function to end up
 with things you can purchase.
\item You now have 10 principle components that are tradable assets.
Are any of them things you would want to buy?  Look at the means of
each of them and test the hypothesis that their mean is zero.  Which
look significant if any?  (Don't forget to use some sort of Bonferroni
correction for your 10 tests.)
\item The first principle component is what is driving most of the 10
deciles.  So what is it exactly?  To understand it, run a regression
of the 10 deciles (as X's) on this component (be sure to remove the
intercept since we don't want it in the regression).  This will tell
us how much weight each of the deciles correspond to in this principle
component.  What does this component seem to be?  Does this justify it
having a non-trivial return?
\item Make a scatter-plot of your first few principle components.  Do
they look different than the deciles did?
\end{enumerate}

\item Lingustics:  We will analyse the text called ``Alice in
Wonderland.'' First we want to grab it down from the Gutenberg
project.  They have collected up over 30,000 books that you can read
or play with.  So surf for the Gutenberg project and find an ascii
version of Alice in wonderland that you can download.  If that doesn't
work, you can just click on
\href{http://www.gutenberg.org/files/11/11.txt}{http://www.gutenberg.org/files/11/11.txt},
but that would be ``cheating.''

\begin{enumerate}
\item After you download it, you can read it into R with the scan
command.  Or you can read it directly via the command:
<<reading>>=
alice <- scan("http://www.gutenberg.org/files/11/11.txt",what="character",quote="")
@ 
This command reads the whole file in as a vector of ``words.''  If you
try it without the \tty{quote=""} it will read all the quoted material
as single words.  Probably not what we want.
\item First we will look at the frequency of the words themselves.
\begin{enumerate}[Ziff step 1:]
\item Using the table command get the counts of the various words.
  Now sort them by frequency.  What are the 10 most common words?  Are
 they significantly different than the 10 most common words in the
 federalist papers?  Does this seem resonable?
\item Now we will make the classic Ziff plot.  We want a plot of the
log of the frequency of the word (or just the log count) vs the log of
the index of the word.
\item Add a regression line to this plot.  Yikes!  It seems to miss
most of the data.  We can eliminate the first 10 words since they
don't ``fit the line'' all that well and the last bunch.  So fit a
line which uses something like the 10'th through the 1000'th
observation.
\item Is the slope you compute similar to the one computed for the
federalist papers?  How about the wikipedia Ziff slope?  Is there a
story here?
\end{enumerate}
\item Let's see how well you predict words compared to how well google
predicts words.  
\begin{enumerate}[LZ 1:]
\item First pick locations at random in the text.  First determine how
 many words there are and then generate a random index from 1 up to
 the last possible word.  Now print out the previous say 20 words or
so.  Write down several possible next words.  What probability do you
give to each of these words?  Ok, look at which word actually occured?
What probability did you give to this word? So
for example, I choose the index
<<random>>=
index <- round(runif(1)*24384)
index
@ 
Then if we look at these words:
<<words>>=
cat(alice[(index-10):(index-1)])
@ 
For example what word do you think comes next in the above example? 
In my random draw, the words were ``I'm late! I'm late! For a very
important \_\_\_\_''.\footnote{For the purists, this actually doesn't
occur in the original text--but only in the Disney version.}
The problem is to fill in the missing word.  So one might guess the
words: event, date, meeting, activity.  Now you give probabilities to
each of these, say P(event) = .1, P(date) = .4, P(meeting) = .2,
P(activity) = .1.  Note these probabilities don't add up to one since
I should also have probabilities for other words that I haven't
bothered to write down.  Now look and see the correct word is:
<<answer>>=
alice[index]
@ 
For the example I'm using the correct word is ``date'' which I
assigned a probability of $.4$.  Do this 10 times.  How often was one
of the words you guessed the correct word?
\item Compute the average of the log probabilities for your guesses.
Assume that any time you missed the word altogether, you should have
in fact used a longer list which eventually would have included the
correct word.  So give yourself a probability of say, 1/24384 for that
word.  The average of your log probabilities is called the
``entropy.'' Entropy is usually measured in ``bits'' which mean base
2.  So use log base 2 for this step.
\item Compare your entropy to the LZ compression scheme.  You can do
this noting that the ZIP version is 59k bytes at Guttenberg.  What does your
total entropy look like?  (Number of words times number average
entropy per word.)
\item Now we want to make a prediction of the next word based on the
google n-gram data set.  First look up the previous word in the google
1-gram file.  For my example, it is ``important'' which occurs
119695314 times.  Now look up the actual word pair that occured in the
2-gram file.  For my example, it is ``important date'' which occured
35885 times.  So the probability is 35885/119695314, or about 1/3000.
So I did much better than google did.  (Yea, I cherry picked the quote
by using one I had memorized.)  How does google do on your 10 words?
What entropy would it give to the entire file?
\item (Bonus) Write a R script that will compute the log(probability)
of each word based on the google 2-gram data set.  What is the final
entropy?  Does it do better than LZ compression?
\end{enumerate}
\end{enumerate}
\item Write a one page story about some fact that you learned in your
exploration of the data above.  Make it self contained--so add any
figures and captions needed for your story.  (The one page target is
just that--a target.  Don't bother reformatting it until it fits
exactly on a page.)
\end{enumerate}

\end{document}
