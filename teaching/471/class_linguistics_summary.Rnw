\documentclass[14pt]{extarticle}
\renewcommand{\baselinestretch}{1.2}
\usepackage{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% defining some commands that I like:
%
%                       \dpf   \note    \DOC    \TESTING
%
\usepackage[usenames]{color}\definecolor{mypurple}{rgb}{.3,0,.5}
\newcommand{\dpf}[1]{\noindent{\textcolor{mypurple}{\{{\bf dpf:} \em #1\}}}}
\newcommand{\note}[1]{\noindent{\textcolor{red}{\{{\bf NOTE:} \em #1\}}}}
\SweaveOpts{prefix.string=.figures/linguistics}

\begin{document}


\title{Class: Lingustics summary}
\maketitle
(\href{class_linguistics_summary.pdf}{pdf version})

\section*{Admistrivia}

\begin{itemize}
\item Sathya will hold delayed office hours (starting at 5:30?)
\item I'll stay after class and answer quick questions
\item Oops (double banana): $\hat{p}_i =  \frac{(k+1)M_{k+1}/M}{M_k}$ if $N_i = k$
\end{itemize}

\section*{Large clouds of data}

\begin{itemize}
\item Projecting all words down to a small diminsional space
\item Allows looking at them in a nice graph
\item Makes pretty pictures
\end{itemize}

\subsection*{How to project?}

\begin{itemize}
\item PCA
\begin{itemize}
\item Look at words $\times$ documents/articles
\item Each word is a variable
\item Compute the PCA
\item Generates loadings on words
\item NYT: Sports words, business words, style words, etc
\end{itemize}
\item CCA
\begin{itemize}
\item Look at local words around target word
\item Find what is in common
\item What is the $Y$ that we can predict?
\end{itemize}
\item Show picture
\begin{itemize}
\item COLORS: Blue = nouns, red's = verbs, green = aj0, yellow = unk
\item on 10,000 words
\end{itemize}
\end{itemize}
\centerline{\includegraphics[width=4in]{pretty_4_grams_PH_10k_30-projection.pdf}}

\subsection*{What does it learn?}

\begin{itemize}
\item Looking at various things it can learn
\item \href{colors.R}{code to print CCA's}
\item Anoyance: the color code also uses a linux script
\item And it uses a large \href{pretty/pretty_3_grams_PHC_50k_30.csv}{data set}
\end{itemize}

\subsection*{Why we care?}
\begin{itemize}
\item In a prediction problem, using words requires fitting 1 million
parameters, one for each word.
\item I'll talk next time about how one might do this--but it is best
to avoid needing to do it
\item After our CCA reduction, we have 30 variables.  Use them all and
fit them all is now easy and accurate.
\end{itemize}

\subsection*{Random projections}
\begin{itemize}
\item Cool mathematics theory about random projections aren't as accurate.
\item JL theory (Johnson Lindenstrauss lemma)
\item But doesn't work for linear regression
\item CCA does about as well as knowing the truth (Theorem Sham and I proved)
\end{itemize}


\section*{Today's use of statistics}

\begin{itemize}
\item IBM has the current best ASR (automatic speech recogniziation) methods
\item Just came back from a talk which used this for S2S (speech to
speech) translation
\item State of the art: Align by ``union'' or align by ``intersection''
\item Today's cool idea: try the mean--it works wonders! (Talk at IRCS)
\item So the area is ripe for statistics
\end{itemize}


\section*{Applications}

\begin{itemize}
\item Named entity tagging (E.g. Find ``Microsoft'', ``MS'' ,''M\$'',
``$\mu$soft'', ``MS inc'', etc)
\begin{itemize}
\item Necessary for figuring out articles in business
\item Even in biology: is this a gene?  It has many names and even
different spellings
\end{itemize}
\item Word sense disambiguation
\begin{itemize}
\item Which form of ``fast'' do we mean in this context?
\item extreme form: ``bad'' = ``good'' in this context?
\end{itemize}
\item Prediction / compression
\begin{itemize}
\item HMM
\item Fast methods (Sham Kakade's new fast algorithm)
\item Faster methods (Regression? In progress by Bob Stine)
\end{itemize}
\end{itemize}



\end{document}
