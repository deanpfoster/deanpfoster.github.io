<html>
<header>
<title>  Statistical Data Mining</title>
</header>
<body>

<p>


<center><h1>Statistical Data Mining</h1></center>

<H2>Schedule (T Th 3:00-4:30 in G92)</h2>

<ul>
  <li> Sept 8: <a href="class_introduction.html">The modeling
spectrum</a>
  <ul>
   <li> read: <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">HTF</a> pp 415-420
  </ul>
   <p>
</ul></p><p>

<ul>
  <li> Sept 13: <a href="class_high_dimensions.html">A picture of high
dimensions</a>
  <ul>
   <li> No reading.  Start on the "new"  <a href="homework.html#hw1">first homework</a> assignment.
  </ul>
  <li> Sept 15: <a href="class_high_dimensions_2.html">Nearest
neighbors in high dimensions</a>
   <ul>
   <li>read: <a href="http://citeseer.ist.psu.edu/dasgupta99elementary.html">Johnson-Lindenstrauss
lemma</a>.
  <li>read: <a href="http://research.microsoft.com/~optas/">Database friendly
projections</a>

  <li> read: <a href="http://www.stat.berkeley.edu/users/peres/">Yuval
Peres</a>'s chapter on <a
href="peres_on_JL.pdf">Johnson-Lindestrauss</a> from his <a
href="http://stat-www.berkeley.edu/~peres/notes1.pdf">lecture
notes</a>.

   <li> You now know enough to complete <a href="homework.html#hw1">homework 1</a>.
   </ul>
 </ul></p><p>

 <ul>
  <li> Sept 20:<a href="class_stepwise_regression.html">Stepwise
regression</a>
   <ul>
    <li> start <a href="homework.html#hw2">homework 2</a> 
    <li> read: <a href="http://linneus20.ethz.ch:8080/2_3.html">Best
         basis problem</a>
    <li> read: William J. Welch, "Algorithmic Compuexity: Three
         NP-Hard problems in computational statistics," (<a
         href="welch_np_hard.pdf">.pdf</a>)
J. Statist. Comput. Simul. 1982.  (added 2014: There is more recent work on
NP completeness of varaible selection.  <a
href="db.lib.tsinghua.edu.cn/OpenPDF-SIAM/DATA/24040.pdf">Natarajan</a>
has one, <a href="http://arxiv.org/pdf/1402.1918.pdf">Michael Jordan</a> has a piece,
and I'm working on one.)
    <li> read: <a href="http://en.wikipedia.org/">Wikipedia</a>'s
        article on <a
        href="http://en.wikipedia.org/wiki/NP-complete">NP-complete</a>.
        Someone should add the Welch result to the list of NP-complete
        problems.  Do not get distracted by the page on <a
        href="http://en.wikipedia.org/wiki/Sudoku">sudoku</a>. 
   </ul>

  <li> Sept 22: <a href="class_bonferroni.html">Bonferroni</a>
   <ul>
    <li> read carefully the first 4 sections of <a
         href="../../research/risk_inflation.pdf">Risk inflation</a>. 
   </ul>

</ul></p><p>

<ul>
  <li> Sept 27: <a href="class_risk_inflation.html">Risk Inflation</a>
(<a href="homework.html#hw1">Homework 1 due</a>)
    <ul>
    <li> look over: Donoho and Johnstone (1994) <a
         href="http://www-stat.stanford.edu/people/faculty/johnstone/techreports.html">Ideal
         Denoising in an Orthonormal Basis Chosen from a Library of
         Bases</a> (<a href="http://www-stat.stanford.edu/people/faculty/johnstone/WEBLIST/1994/idealbasis.pdf">.pdf</a>) 
    </ul>

  <li> Sept 29: <a href="class_wavelets.html">Curve fitting and wavelets</a>
    <ul>
    <li> Read carefully: <a
         href="http://links.jstor.org/sici?sici=0006-3444%28199408%2981%3A3%3C425%3AISABWS%3E2.0.CO%3B2-C">Donoho
        and Johnstone's (1994) wavelet paper.</a>
    </ul>
</ul></p><p>

<ul>
  <li> Oct 4: Proper scoring rules
     <ul>
     <li> Read: <a href="schervish.pdf">General method for comparing
          probability assessors</a>, by Mark Schervish.
     </ul>
  <li> Oct 6: <a href="class_roc.html">Alternative scoring rules and
        calibration</a>, <a href="homework.html#hw2">Homework 2 due</a>
</ul></p><p>

<ul>
  <li> Oct 11: <a href="class_spam.html">Spam: Bag of words and Naive-Bayes</a>
    <ul>
    <li> <a href="http://opim.wharton.upenn.edu/home/wp/talks.html">Talk</a> in  
         OPIM at Noon in G50 (free food)
    <li> Read <a
href="http://www.nytimes.com/2005/10/09/business/09advi.html">NYT data
mining article</a> (<a href="nytimes_datamining.html">mirror</a>)
    <li> Read: Madigan's paper on <a href="madigan_spam.pdf">Naive
Bayes</a>
    <li> Read: as usual the wiki on <a
href="http://en.wikipedia.org/wiki/Bayesian_filtering">Bayes
filtering</a> and <a
href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive
Bayes</a>.
    <li> Read about the  <a href="good_turing.pdf">Good-turing
estimator</a> for rare event probabilities.
    </ul>
  <li> Oct 13: <a href="class_p-values.html">Tails</a>
    <ul>
    <li> Read: proof of <a
href="http://www.cs.ucsd.edu/~klevchen/techniques/chernoff.pdf">Chernoff</a>
used in class.
    <li> Read: Either Hal White's orginal sandwich estimator paper, or
GEE paper by Liang and Zeiger
    </ul>
</ul></p><p>
<ul>
  <li> Oct 17/18: (Fall break)
  <li> Oct 20: <a href="class_white.html">Sandwich estimator</a>
</ul>
</p><p>
<ul>
  <li> Oct 25: <a href="class_graph_like_data.html">graphs</a>
    <ul>
    <li><a
href="http://www.autonlab.org/tutorials/astar.html">discussion of A* algorithm</a>
    </ul> 
  <li> Oct 27: <a href="class_sfs.html">SFS: streaming feature selection</a>
    <ul>
    <li> Note: <a
href="http://www.cse.unsw.edu.au/~billw/cs9414/notes/ml/03ibl/03ibl.html">IB1</a>
is a nearest neighbor algorithm
    <li> read: <a href="aha95comparative.pdf">SFS</a> by Aha
    </ul> 

</ul></p><p>

<ul>
  <li> Nov 1: <a href="class_alpha_spending.html">Alpha spending</a>
    <ul>
    <li> <a href="homework.html#hw3">Homework 3 due</a> start on hw4
    <li> Read one page background on <a
href="http://www.cytel.com/Products/East/example_02.asp">alpha spending</a>.
    <li> Read: <a href="../../research/edc.pdf">Excess discovery count</a> (<a
href="../../research/edc.ps">.ps</a>) by Bob and me
    </ul>
  <li> Nov 3: <a href="class_edc.html">FDR and EDC</a>
    <ul>
    <li> Surf <a href="http://www.math.tau.ac.il/~ybenja/">Yoav</a>'s
page on <a
href="http://www.math.tau.ac.il/~ybenja/fdr/index.htm">FDR</a>.
     </ul>
</ul></p><p>

<ul>
  <li> Nov 8: Support vector machines (guest lecture by Jon)
  <li> Nov 10: <a href="class_rkhs.html">RKHS</a> 
     <ul>
     <li> <a href="homework.html#hw4">Homework 4 due</a>
     <li> <a href="http://omega.albany.edu:8008/ml/">nice source</a>
of information about support vectors and RKHS
     <li> In particular see <a
href="http://omega.albany.edu:8008/machine-learning-dir/notes-dir/ker1/ker1-l.html">RKHS</a> and
 <a
href="http://omega.albany.edu:8008/machine-learning-dir/notes-dir/reg1/reg1-l.html">regression
using rkhs</a>.
You might find it easier to read the pdf files rather than the html
file.
     <li> Read section 5.8 of Hastie, Tibshirani, Friedman that I handed out.
     </ul>
</ul></p><p>

<ul>
  <li> Nov 15: Support vector machines: part II (guest lecture by Jon)
  <li> Nov 17: <a href="class_rkhs_ls.html">Fitting RKHS using LS</a>
</ul></p><p>

<ul>
  <li> Nov 22: <a href="class_clustering.html">Clustering</a>
   <ul>
   <li> read handout: pages 412-413 and 461-464 of Hastie, Tibshirani,
Friedman.
   <li> Read <a
href="http://www.cs.huji.ac.il/~tishby/">Tali Tishby</a> and Eyal
Krupka's NIPS <a href="tali.pdf">paper</a>. 
   </ul> 
</ul></p><p>

<ul>
  <li> Nov 29:<a href="class_trees.html">Trees</a>
  <ul>
     <li> read handout: pages 266 -289 of Hastie, Tibshirani,
Friedman. 
  </ul>
  <li> Dec 1: <a href="class_information_theory.html">Information
theory</a>
  <ul>
     <li> read <a
href="http://www-stat.wharton.upenn.edu/~stine/">Bob</a>'s gentle
introduction to <a 
href="http://www-stat.wharton.upenn.edu/~stine/research/smr.pdf">information
theory</a>.
     <li> Feel free to talk to Bob, Adi or me about information theory.
     <li> A book on <a
href="http://www.inference.phy.cam.ac.uk/mackay/itila/">information
theory</a> that is better
than Harry Potter
      <li> For general information on <a
href="http://www.matf.bg.ac.yu/nastavno/vobul.html">information theory</a>.

   </ul>
</ul></p><p>

<ul>
  <li> Dec 6: <a href="class_worst.html">Alternative models of
data</a>
     <ul>
     <li> read Rick's and my review <a
href="../../research/no_regret.ps">paper</a>.
     <li> My first annals of stat paper was in this area.  Guess what?
It was on regression!  I've written several papers on this.  
     </ul>
  <li> Dec 8:  <a href="class_summary.html">Summary</a>
</ul></p><p>
<ul>
  <li> Dec 19: <a href="homework.html#hw5">Homework 5 due</a>
  <li> Dec 21: Late date for homework 5.  
</ul>

<h2>General information</h2>

Some estimate that there is now 4 exabytes of data being produced each
year.  This is a different world than that which Fisher pioneered.  He
developed a theory that can deal with a 2x2 contingency table which
might have a total of 4 bytes of data in it.  This 10<sup>18</sup>
increase in data is changing the world of statistics.  The goal if
this course is follow this change.
<p>
Exactly what data mining is depends on who you talk to.  For example,
<a href="http://www.cs.cmu.edu/~awm/">Andrew Moore</a> takes a very
wide view of  <a href="http://www.autonlab.org/tutorials/">data
mining.</a>  He includes lovely topics from economics (i.e. game
theory) to topics from classical AI (i.e. A* algorithm).  This will
contrast with the approach I will take.  I'll focus much more highly
on statistical regression.  
<p>
I've written a <a href="outline.html">crude outline</a> of what the
course will cover.

<h2>Prerequisites</h2>

This course is targeted at PhD students.  Some mathematical
sophistication will be assumed.  You will be expected to carefully
read research papers.  The primary statistics tool will be regression,
so at least a few weeks of background on that would be desirable.  If
you are unsure, send me an email and we can chat.

<hr>
<em>
<p align=right>
<!-- hhmts start -->
Last modified: Thu Sep 25 13:00:07 EDT 2014
<!-- hhmts end -->
</body>

</html>
