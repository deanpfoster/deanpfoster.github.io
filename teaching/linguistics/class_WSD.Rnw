\documentclass[14pt]{extarticle}
\renewcommand{\baselinestretch}{1.2}
\usepackage{hyperref}
\begin{document}
\SweaveOpts{prefix.string=.figures/notes}

\title{Class: WSD}
\maketitle

\section*{Porter stemmer}

Why is R close to red?
\begin{itemize}
\item called has stem call
\item fitted has stem fit(t)
\item based has stem bas-
\item Hence: Red has stem R!
\end{itemize}
Porter stemmer does this automatically.  Crude but still useful. 


\section*{Lemma's in linguistics}

A correct stem is called a lemma
\begin{itemize}
\item lemma ``sing(verb)'' contains:
\begin{itemize}
\item sing
\item sang
\item singing
\item sung
\item etc
\end{itemize}
\item lemma ``bird(noun)'' contains:
\begin{itemize}
\item birds
\item bird
\item but not birding
\end{itemize}
\item Problem: discover the lemma
\item Fairly easy in English, more interesting in Arabic (and more
profitable)
\item This provides one grouping of words--other groupings are possible 
\end{itemize}

\section*{Relationships between words}

Words can be related by many different semantic connectives:
\begin{itemize}
\item Homonym: spelled the same, mean different
\begin{itemize}
\item Polysemous: many meanings for one homonym
\item monosemous: only one meaning (List of such words are availiable)
\end{itemize}
\item metonymy: ``metaphore'' like usage, ``The White House said...''
\item synomym: two lemmas with same meaning: couch/sofa
\item antonym: opposites
\item Superordinate: superset
\item hyponym: subset
\item sounds:
\begin{itemize}
\item homophone (to, two, too)
\item homograph (bass fish, bass fiddle)
\end{itemize}
\end{itemize}

\section*{Gramatical check for two meanings}

Passing the zeugma test:
\begin{itemize}
\item I can bake bread
\item I can bake cakes
\item I can bake bread and cakes.
\end{itemize}
Flunking the zeugma test:
\begin{itemize}
\item I can serve time (in jail)
\item I can serve drinks
\item I can serve balls (say in tennis ball)
\item But I can't serve time, drinks, and balls.
\end{itemize}

\section*{Disambiguation}

\begin{itemize}
\item tuna, Salmon, bass
\item violin, cello, bass
\item Nice word since we don't have to say ``bass${}^1$'' and ``bass${}^2$''
\end{itemize}

\section*{As POS}
\begin{itemize}
\item Treate it as POS tagging
\item Simply a million tags
\item This being cruel to our computers: they will barf
\item Maybe possible bad with a combinatorial hidden state:
\begin{itemize}
\item have a hidden state which is a vector of properties
\item each evoloves independently
\item high diminsional hidden state
\item but few parameters to estimate
\end{itemize}
\end{itemize}

\section*{Databases}
\subsection*{WordNet}

\begin{itemize}
\item 100k nouns (1.23 meanings per noun)
\item 11k verbs  (2.16 meanings per verb)
\item 22k adjectives
\item 4k adverbs
\end{itemize}

alternatives: PropBank (provides roles) and FrameNet (provides slots)

\subsection*{line-hard-serve database}
\begin{itemize}
\item 4000 examples each of line (noun), hard (adj) and serve (verb)
\item Alternatives: ``interest'' dtabase (2300 examples)
\end{itemize}

\subsection*{Senseval}
\begin{itemize}
\item Three versions
\item 34 + 73 + 57 different words are disambiguated
\end{itemize}

\subsection*{Pseudowords}
\begin{itemize}
\item Make your own database
\item replace all occurances of either banana or door with a single
token: banana-door
\item Now disambiguate the merged word
\item Harder: merge cat-dog
\item Hardest: merge house-home
\end{itemize}

\subsection*{Parallel corpora}
\begin{itemize}
\item Use French-English traslations of government documents from Canada
\item If ``bank'' has two different words in french, they will be
translated differently
\item You don't even have to align them perfectly
\item Hexapala (6 languages for the bible)
\item Also ``Europarl'' for Dutch, German, Swedish, Greek, Italian, etc
\item Basically your very own Rosseta Stone (it had three languages)
\end{itemize}

\section{Lesk algorithm}

The simple Lesk algorithm: 
\begin{itemize}
\item Count co-occurances of words in each definition with words in
sentence.
\item Majority vote
\end{itemize}

The original Lesk algorithm:
\begin{itemize}
\item Think of words in definition as being neighbors.  Now use
neighbors of neighbors.
\end{itemize}

Improved Lesk algorithm:
\begin{itemize}
\item Weighted ``least squares''
\item Weighting called IDF: for inverse document frequency
\item 1 / number of documents word occurs in
\item Typically logged
\item Sometimes TF/IDF is used: this looks at the frequency of the
word itself also
\end{itemize}

Statistical answers:
\begin{itemize}
\item Many improvements possible
\item I'll let you think of some on your own.
\item So read: all of 19, and 20.1, 20.2,20.3, 20.4.  
\end{itemize}


\end{document}
