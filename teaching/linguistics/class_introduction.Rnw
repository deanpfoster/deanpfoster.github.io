\documentclass[14pt]{extarticle}
\renewcommand{\baselinestretch}{1.2}
\usepackage{hyperref}
\begin{document}
\SweaveOpts{prefix.string=.figures/notes}

\title{Class: Introduction}
\maketitle

\section{Admistrivia}

\begin{itemize}
\item Lots of homework: mostly R, some math.
\item Final project: Analysis and write up and presentation of some
lingustic data.  If you have a particular domain in mind, talk to me
soon and we can start connecting you with data.  I'll be leaving town
on the last day of class, so either have the project done by then, or
it will have to wait until I return on the first day of spring
semester.
\item Book: JM = {\em Speech and Language processing} by Jurafsky and Martin.
\item Readings will be mostly listed on the web (along with my
teaching notes).  I'll almost certainly forget to mention readings in
class.  For example, I'll probably not get around to saying, ``You
should read the chapters 1,2 and 4 this weekend.''
\end{itemize}

\section{Introduction: Example}

\begin{itemize}
\item Classic epidemology: count number of cases doctors report for
flu each week.
\begin{itemize}
\item Model speed of reporting (different for different areas)
\item Predict growth to make up for delay in data
\item Make forecast of current level of flu
\end{itemize}
\item Google Flu: 
\begin{itemize}
\item Take searches for ``flu'' as ground truth
\item Look for phrases that correlate with it: runny nose, etc
\item Count the number of searches for these terms
\item Don't bother with delay since you have daily/hourly searches
\end{itemize}
\end{itemize}

\section{Introduction: General cases}

\begin{itemize}
\item Medical: machine reading of reports.
\item Credit cards: Machine reading bills and predicting fraud / bankruptcy.
\item Biology: Machine reading of abstracts to find which genes
are discussed in the article.
\item Education: Machine reading of course evaluations.
\item Teaching: Automatic writing of questions for reading
compreshension.  OK, this maybe more statistical use in lingustics
rather than the other way around.  So let's turn to those.
\end{itemize}

\section{Statistics in lingustics}

\begin{itemize}
\item Best voice recognition are based on probabilistic models
\item Best POS taggers are statistical
\item Parsing use statistics now adays
\item Word disambiguation / word meaning are statistical
\item We are in the ``Rise of Machine learning'' era according to the
book. 
\end{itemize}

\section{Why now?}

\begin{itemize}
\item Chomsky didn't use data
\item In 70s/80s, people might train systems on 50k words.  Similar to
expecting a baby to talk at age, 5 hours.
\item Now we can process a billion words.  Close to a lifetime's
worth.  So we can start finding the same patterns that humans can
encrypt in language.
\end{itemize}

\section{The bright and beautiful future}

\begin{itemize}
\item In 1951 Turing wrote his piece on smart computers.  He said in
50 years we would have smart computers.  Hence the title of the Arthor
C Clark book: {\em2001: A space Odyssey.} ``I'm sorry Dave, I'm
afraid I can't do that.'' 
\item Understand text is the final problem in science: once it can be
solved, computers will start to be really smart.  (Suppose you had the
time to read the entire Wikipedia and understand the whole thing.
That is the starting point of any intellegent computer.)
\item Turing guess was a shot in the dark.  Now we have better
estimates of human memory: say about $10^{15}$ bits, or $10^{14}$
bytes, or 100 TeraBytes (Or TibiBytes for the purists).  You can buy
Terabyte disks now.  A terabyte of memory would be about one thousand
dollars (2009 estimate).
\item In 20 years, you will have about 100 Terabyte computers on your
desk.  In 25 years, your ``ipod'' will have that much memory.
\item So, peg the singularity at 2029 for a first guess.  
\end{itemize}

\end{document}
