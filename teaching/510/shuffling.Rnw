\documentclass[14pt]{extarticle}

\begin{document}
\title{433: Shuffling cards}
\author{Dean P Foster}
\maketitle


\section*{Administrivia}

\begin{itemize}

\item F92
\item No homework due this week
\item  Read the posted section.  Short, understandable but it will
require thinking.
\end{itemize}

\section*{Dynamical systems: The search for randomness}

Newton's billard table with one unknown particle leads to noise.
\begin{itemize}
\item Angle of strike is angle of direction
\item Assume zero friction
\item Balls keep going and going and going
\item How accurately can we forecast it?
\begin{itemize}
\item Clearly if I hold a magnet next to table (and balls contain
iron) then I can confuse a forecast.
\item I have less control if I hold a brick (all that matters is gravity)
\end{itemize}
\item Theory says: predictions are good for ever if you know everything
\item But what if you don't know the brick?
\item What if the brick is very far away? (Alpha centuria)
\item What if the brick is very small? (one atom)
\item Poll: How long? 
\begin{itemize}
\item Life of universe?
\item 1000 years
\item 1 year
\item 1 day
\item 1 minute
\end{itemize}
\item Closest answer: 1 minute (true answer about 2 minutes, but it
depends on how fast you hit the balls, etc)
\end{itemize}
{\bf Moral: Randomness is everywhere -- even in a Newtonian universe.}
Or A little randomness goes a long way.

\section*{How many times should you do a real shuffle?}

Problem: How many times should real people shuffle a deck?

\section*{Simple counting rules for three shuffling methods}

\subsection*{3 cut shuffle}

\begin{itemize}
\item How many positions are possible for a deck of cards?
\item Consider the 3-cut shufle
\begin{itemize}
\item Many positions assigned zero probability
\item Some assigned $1/52^2$ (or $1/{52 \choose 2}$ for the anal)
\item After doing $k$ such shuffles, how many zeros might there still
be? 
\item Each non-zero is larger than $52^{-2k}$.  So until $k \approx 52
\log_2(52)/2$ we still have several positions which have to be assigned
zero probability.
\item These counting problems are often easier done using ``bits.''
THen it is called information theory.
\end{itemize}
\end{itemize}

\subsection*{More interesting: shuffling a cube}
\begin{itemize}
\item How long to generate a perfectly shuffled cube by randomly
twistings sides?
\item Easy: $\infty$!
\item Why: it is impossible to flip an edge.
\end{itemize}

\subsection*{Riffle shuffle}

\begin{itemize}
\item Hard to describe.  But, we could label each card after the
shuffle as a ``top'' or a ``bottom.'' That would summarize what is
going on.
\item So, 52 bits.
\item (Or in probabilty land, $2^{-52}$.)
\item So it takes $\log_2(52)= 5.7$ such shuffles before we MIGHT have
eliminated all the zero probability densities.  
\end{itemize}

\subsection*{Cut shuffle}
\begin{itemize}
\item At each time, but the deck, put the bottom on the top
\item Number of ``bits'' of entropy stirred in each round is $\log_2
52$.
\item So it should take about $50 \log_2(52)$ such cut shuffles.
\item BUT, this clearly doesn't work.  
\item Why? We the Markov chain has many different comunication classes
that never speak to each other.
\end{itemize}

\subsection*{Card trick}

\begin{itemize}
\item Consider the following card trick:
\begin{itemize}
\item Have the mark: Pick a card, look at it, and put it back, and
riffle shuffle the deck 3 times.
\item Now look through the deck, There SHOULD be eight sequential rising
sequences with about 7 cards per sequence.
\item But, in fact there will likely be 9.
\item Do it on the black board with a 800 card deck
\item Probability of failure is chance of landing in the right spot:
about 1/100 for a 800 card deck.
\end{itemize}
\end{itemize}

\subsection*{Goal: want probability to match real probabilities}

\begin{itemize}
\item Tell story of random shuffling in bridge
\item Introduce new-age solitare Hearts and Clubs VS Spades and diamonds
\item In a new deck, Hearts and clubs wins on the first round
\item Makes big difference up to 7 or so shuffles
\end{itemize}

\subsection*{Definition}

\begin{itemize}
\item Fair game: 50/50 winner
\item Bad shuffle: one player wins
\item Value of bad shuffle is how much they win by
\end{itemize}


\section*{What is a shuffle? (riffle shuffle)}

\subsection*{1st model: set theory}
\begin{itemize}
\item Pick cut (Binomial)
\item Generate subset for each cut
\item Put cut in its place
\end{itemize}
\subsection*{2nd model: equally likely}
\begin{itemize}
\item Notice: this generates two rising sequential sequences
\item List all possible 2-rising sequential sequences
\item Each can be created by some cut-placement combination
\item Give them equal probability
\item Anoyance: What about the identity?
\end{itemize}
\subsection*{Inverse shuffle}
\begin{itemize}
\item work backwards!
\item Toss a coin for each outcome as t whether it should be from the
top or the bottom.
\end{itemize}


\subsection*{3rd model: Physics}
\begin{itemize}
\item Cut as usual
\item Draw one of the two top cards
\item Clearly if there aren't any left in one pile its probability is zero.
\item So let's make this continuous by making a draw proportional to
the size of the deck
\item Same as other two methods
\end{itemize}



\section*{Second day continue with the following}

\section*{Discuss betting: fair game}

How far is a shuffled deck from fair:
\begin{displaymath}
d(P,Q) = sup_{A: Q(A) = .5} 2 (P(A) - .5)
\end{displaymath}
We can also write this as:
\begin{displaymath}
d(P,Q) = sup_{A: Q(A) = .5} |P(A) - Q(A)|  + |P(A^c) - Q(A^c)|
\end{displaymath}
How do we find the set $A$?  For discrete densities we have:
\begin{displaymath}
||f - g|| \equiv \frac12 \sum_{\omega \in \Omega} |f(\omega) - g(\omega)|
\end{displaymath}
We can do an integral version:
\begin{displaymath}
||f - g|| \equiv \frac12 \int |f(x) - g(x)| dx
\end{displaymath}
Or a probability version:
\begin{displaymath}
||P - Q|| \equiv \sup_{A \in \cal F} |P(A) - Q(A)| + |P(A^c) - Q(A^c)|
\end{displaymath}
A bit of probability says this is:
\begin{displaymath}
||P - Q|| \equiv \sup_{A \in \cal F} 2|P(A) - Q(A)|
\end{displaymath}

\section*{Rising sequence set}

\begin{itemize}
\item We can bound $||P - Q||$ if we can exhibit a set $A$ which has
very different probabilites
\item Let $A$ be the Rising wins before falling
\begin{itemize}
\item First player is looking for the sequence 1,2,3,..
\item Second player is looking for 52,51,50,...
\item Perfect shuffle, each is equally likely
\item For 7 riffle shuffles, rising guy wins 80\% of the time.
\item So $||P_7 - P_\infty|| > .6$
\end{itemize}
\end{itemize}


\section*{Rate of convergence}

For a ``nice'' Markov chain (i.e. finite number of states, one
communication class, a periodic):
\begin{displaymath}
||P_n - P_\infty|| \le a e^{-bn}
\end{displaymath}
Proof: This definition plays well with the coupling argument.  Let $T$
 be the time when a chain in equilibrium couples with our Markov
 chain.  If $c_t = P(T > t)$ then 
\begin{displaymath}
||P_n - P_\infty|| \le c_t
\end{displaymath}
Since there is a small probability $\epsilon$ that lower bounds the
chance of two processes starting to couple at any time, we see that
$c_t \le (1 - \epsilon)^t$.

\section*{Old fashioned card sorter}
\begin{itemize}
\item I've programmed with punch cards!
\item What happens if you drop your deck of punched cards?  Yikes!
\item You want a card sorter then.  (And hopeuflly you had them numbered)
\item How do you build a physical device to sort punched cards?
\item Input flows into one of two output bins
\item Actually very fast!  To sort a million cards would only take 10
passes through the card sorter.
\end{itemize}

\section*{discuss labeling cards as you shuffle}
\begin{itemize}
\item Label cards as you shulffle them: top or bottom
\item A card then is given a sequence: ttbttbtbbtbt...
\item This gives the shuffling history
\item It allows unshuffling
\item If two cards have the same label, then we know the one with a
smaller number is on top of the other card
\item If two cards have different labels, either order of the two
cards is equally likely
\item Define $T$ to be the first time all cards are given unique
labels 
\end{itemize}

\section*{After time $T$}

\begin{itemize}
\item The labels are all equally likely
\item So if someone told us the list of labels that occur at time $T$
we would conditionally generate a completely random deck
\end{itemize}

\section*{What has to happen for $T$ to occur?}
\begin{itemize}
\item At time $t$ there are a total of $2^t$ possible labels (aka
birthday).
\item We want all 52 cards to have a unique label (aka unique
birthay).
\item Probability is about $52^2/2^t$ that there is a collision.
\item So this is an upper bound on the L-1 distance between the two
distributions. 
\end{itemize}

\section*{Lots of beautiful theory I'm skipping}

\begin{itemize}
\item The section I asked you to read has an exact calculation of the
L-1 distance
\item It introduces several cool ideas in combinotorics
\item It also introduces the multi-arm shuffle
\item You should be able to read and enjoy it now based on this
introduction.
\item This is one of the most famous and coolest results in modern
probability theory--so it is nice that you now understand it
\end{itemize}

\end{document}
